{"./":{"url":"./","title":"Introduction","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:04:48 "},"1.Linux基础/1.10数据库/10mysql数据恢复.html":{"url":"1.Linux基础/1.10数据库/10mysql数据恢复.html","title":"10mysql数据恢复","keywords":"","body":"创建数据库 create database ; 导入表结构 修改数据库字符集 SET character_set_client = utf8; SET character_set_connection = utf8; SET character_set_database = utf8; SET character_set_results = utf8; SET character_set_server = utf8; SET collation_connection = utf8_bin; SET collation_database = utf8_bin; SET collation_server = utf8_bin; Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/":{"url":"1.Linux基础/1.1常用shell/","title":"1.1常用shell","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/01查询文件内`tab`键.html":{"url":"1.Linux基础/1.1常用shell/01查询文件内`tab`键.html","title":"01查询文件内tab键","keywords":"","body":"查询文件内tab键 适用于yaml校验(制表符) grep $'\\t' 文件名 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/02计算shell运行时间.html":{"url":"1.Linux基础/1.1常用shell/02计算shell运行时间.html","title":"02计算shell运行时间","keywords":"","body":"计算shell运行时间 用date相减 startTime=`date +%Y%m%d-%H:%M:%S` startTime_s=`date +%s` endTime=`date +%Y%m%d-%H:%M:%S` endTime_s=`date +%s` sumTime=$[ $endTime_s - $startTime_s ] echo \"$startTime ---> $endTime\" \"Total:$sumTime seconds\" time time sh xxx.sh # 会返回3个时间数据 # real 该命令的总耗时, 包括user和sys及io等待, 时间片切换等待等等 # user 该命令在用户模式下的CPU耗时,也就是内核外的CPU耗时,不含IO等待这些时间 # sys 该命令在内核中的CPU耗时,不含IO,时间片切换耗时. Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/04关闭selinux.html":{"url":"1.Linux基础/1.1常用shell/04关闭selinux.html","title":"04关闭selinux","keywords":"","body":"关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/05开启tcp端口监听.html":{"url":"1.Linux基础/1.1常用shell/05开启tcp端口监听.html","title":"05开启tcp端口监听","keywords":"","body":"开启tcp端口监听 （测试网络连通性） python -m SimpleHTTPServer 9099 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/06文件切割.html":{"url":"1.Linux基础/1.1常用shell/06文件切割.html","title":"06文件切割","keywords":"","body":"文件切割 split -d -b 100m enterprise.log enterprise- Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/07磁盘占用异常排查.html":{"url":"1.Linux基础/1.1常用shell/07磁盘占用异常排查.html","title":"07磁盘占用异常排查","keywords":"","body":"磁盘占用异常排查 #查找 du -m --max-depth=1 |sort -gr lsof |grep delete #删除 lsof |grep delete|awk '{print $2}'|xargs -n1 kill -9 清理文件句柄 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/08查看tcp连接状态.html":{"url":"1.Linux基础/1.1常用shell/08查看tcp连接状态.html","title":"08查看tcp连接状态","keywords":"","body":"查看tcp连接状态 netstat -na|awk '/^tcp/ {++S[$NF]} END {for(i in S) print i,S[i]}' Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/09打包iso.html":{"url":"1.Linux基础/1.1常用shell/09打包iso.html","title":"09打包iso","keywords":"","body":"打包iso mkisofs -o ./package.iso -J -R -A -V -v package Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/10创建大文件.html":{"url":"1.Linux基础/1.1常用shell/10创建大文件.html","title":"10创建大文件","keywords":"","body":"创建大文件 创建 fallocate -l 10G test4 撤销 fallocate -d test4 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/11磁盘监控脚本.html":{"url":"1.Linux基础/1.1常用shell/11磁盘监控脚本.html","title":"11磁盘监控脚本","keywords":"","body":"磁盘监控脚本 系统版本：CentOS7 脚本/usr/bin/disk-monitor.sh内容如下： #!/bin/bash LOCAL_HOST=192.168.1.3 RECEIVE_LIST=\"aaa@xxx.com\" CC_LIST=\"bbb@xxx.com,ccc@xxx.com,ddd@xxx.com\" MOUNT_NODE_COUNT=`df -h|wc -l` echo \"挂载节点数为：${MOUNT_NODE_COUNT}\" while [[ ${MOUNT_NODE_COUNT} -ne 1 ]]; do FILE_SYSTEM=`df -h |sed -n \"$MOUNT_NODE_COUNT p\"|awk '{print $1}'` MOUNT_NODE=`df -h |sed -n \"$MOUNT_NODE_COUNT p\"|awk '{print $6}'` PART_FREE_SPACE=`df -h |sed -n \"$MOUNT_NODE_COUNT p\"|awk '{print $4}'` UTILIZATION_RATE=`df -h |sed -n \"$MOUNT_NODE_COUNT p\"|awk '{print $5}'` UTILIZATION_RATE_VALUE=`echo ${UTILIZATION_RATE}|sed 's/.$//'` # echo \"文件系统：`echo ${FILE_SYSTEM}`，挂在节点：`echo \"$MOUNT_NODE\"`，分区磁盘使用率为：`echo ${UTILIZATION_RATE}`, 剩余磁盘空间：`echo ${PART_FREE_SPACE}`\" if [[ ${UTILIZATION_RATE_VALUE} -gt 95 ]]; then MAIL_CONTENT=\"[当前地址]：${LOCAL_HOST} [文件系统]：`echo ${FILE_SYSTEM}` [挂在节点]：`echo \"$MOUNT_NODE\"` [分区磁盘使用率]：`echo ${UTILIZATION_RATE}` 已达告警阈值，请及时清理！！！\" echo ${MAIL_CONTENT} echo \"${MAIL_CONTENT}\" | mail -s \"磁盘剩余空间告警\" -c ${CC_LIST} ${RECEIVE_LIST} &> /dev/null fi let MOUNT_NODE_COUNT-- done 安装mailx yum install -y mailx 配置mailx，/etc/mail.rc追加以下内容 set from=aaa@xxx.com set smtp=smtp.xxx.com:587 set smtp-auth-user=aaa set smtp-auth-password=****** set smtp-auth=login set smtp-use-starttls set ssl-verify=ignore set nss-config-dir=/etc/pki/nssdb/ 配置定时任务 cat >> /etc/crontab Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/12ssl生成脚本.html":{"url":"1.Linux基础/1.1常用shell/12ssl生成脚本.html","title":"12ssl生成脚本","keywords":"","body":"ssl生成脚本 #!/bin/bash # 域名 export domain=www.example.com # IP地址（可选） export address=192.168.1.11 # 国家 export contryName=CN # 省/州/邦 export stateName=Liaoning # 地方/城市名 export locationName=Shenyang # 组织/公司名称 export organizationName=example # 组织/公司部门名称 export sectionName=develop echo \"Getting Certificate Authority...\" openssl genrsa -out ca.key 4096 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=$contryName/ST=$stateName/L=$locationName/O=$organizationNaem/OU=$sectionName/CN=$domain\" \\ -key ca.key \\ -out ca.crt echo \"Create your own Private Key...\" openssl genrsa -out $domain.key 4096 echo \"Generate a Certificate Signing Request...\" openssl req -sha512 -new \\ -subj \"/C=$contryName/ST=$stateName/L=$locationName/O=$organizationNaem/OU=$sectionName/CN=$domain\" \\ -key $domain.key \\ -out $domain.csr echo \"Generate the certificate of your registry host...\" cat > v3.ext Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/13批量修改文件格式.html":{"url":"1.Linux基础/1.1常用shell/13批量修改文件格式.html","title":"13批量修改文件格式","keywords":"","body":" 批量修改为unix for file in `find hack/lib -name *.sh` do vi +':w ++ff=unix' +':q' ${file} done Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/14查看内存占用前n.html":{"url":"1.Linux基础/1.1常用shell/14查看内存占用前n.html","title":"14查看内存占用前n","keywords":"","body":" 查看内存占用前3进程 $ ps aux --sort -rss |grep -v PID | head -3 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/15nginx日志解析.html":{"url":"1.Linux基础/1.1常用shell/15nginx日志解析.html","title":"15nginx日志解析","keywords":"","body":" 提取nginx访问前x的IP地址 第一个sort进行ip地址排序，sort -nr对数量排序 $ cat /var/log/nginx/access.log|awk '{print $1}'|sort|uniq -c|sort -nr 396 10.30.2.169 67 192.168.129.64 34 192.168.129.176 6 10.9.48.129 提取nginx日志中状态码数量 $ cat /var/log/nginx/access.log|awk '{print $9}'|sort|uniq -c|sort -rn 529 200 6 302 5 401 4 502 4 400 4 304 2 499 1 201 1 101 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/20系统类信息获取.html":{"url":"1.Linux基础/1.1常用shell/20系统类信息获取.html","title":"20系统类信息获取","keywords":"","body":"查看硬件信息 yum install -y lshw lshw -short -class disk -class storage -class volume Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/99获取系统信息.html":{"url":"1.Linux基础/1.1常用shell/99获取系统信息.html","title":"99获取系统信息","keywords":"","body":"查看系统UUID $ cat /sys/class/dmi/id/product_uuid ff594d56-e05f-a672-39de-806201d083e4 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.1常用shell/防火墙相关.html":{"url":"1.Linux基础/1.1常用shell/防火墙相关.html","title":"防火墙相关","keywords":"","body":"防火墙开放端口 el6 #开放端口（7777） iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport 7777 -j ACCEPT #保存 /etc/rc.d/init.d/iptables save #重载 service iptables restart el7 firewall-cmd --zone=public --add-port=7777/tcp --permanent #重新载入 firewall-cmd --reload 删除所有规则 iptables -Z iptables -t nat -F iptables -t nat -X iptables -t nat -P PREROUTING ACCEPT iptables -t nat -P POSTROUTING ACCEPT iptables -t nat -P OUTPUT ACCEPT iptables -t mangle -F iptables -t mangle -X iptables -t mangle -P PREROUTING ACCEPT iptables -t mangle -P INPUT ACCEPT iptables -t mangle -P FORWARD ACCEPT iptables -t mangle -P OUTPUT ACCEPT iptables -t mangle -P POSTROUTING ACCEPT iptables -F iptables -X iptables -P FORWARD ACCEPT iptables -P INPUT ACCEPT iptables -P OUTPUT ACCEPT iptables -t raw -F iptables -t raw -X iptables -t raw -P PREROUTING ACCEPT iptables -t raw -P OUTPUT ACCEPT Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.2包管理器/apt/":{"url":"1.Linux基础/1.2包管理器/apt/","title":"apt","keywords":"","body":"配置清华源 清理已有源 sudo rm -f /etc/apt/sources.list.d/* 添加阿里云 20.x sudo tee /etc/apt/sources.list 更新 sudo apt-get update -y Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.2包管理器/choco/":{"url":"1.Linux基础/1.2包管理器/choco/","title":"choco","keywords":"","body":"choco 管理员运行cmd，执行 @powershell -NoProfile -ExecutionPolicy Bypass -Command \"iex ((new-object net.webclient).DownloadString('https://chocolatey.org/install.ps1'))\" && SET PATH= %PATH %; %ALLUSERSPROFILE %\\chocolatey\\bin Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.2包管理器/gem/":{"url":"1.Linux基础/1.2包管理器/gem/","title":"gem","keywords":"","body":"gem源配置 查看默认源 gem sources -l 配置代理 修改文件/usr/bin/gem begin args += ['--http-proxy','http://x.x.x.x:port'] Gem::GemRunner.new.run args rescue Gem::SystemExitException => e exit e.exit_code end 修改默认源 gem sources -r https://rubygems.org/ -a https://gems.ruby-china.com/ bundle config mirror.https://rubygems.org https://gems.ruby-china.com Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.2包管理器/helm/":{"url":"1.Linux基础/1.2包管理器/helm/","title":"helm","keywords":"","body":"helm Kubernetes的软件包管理工具 版本说明 Helm 2 是 C/S 架构，主要分为客户端 helm 和服务端 Tiller; Helm 3 中移除了 Tiller, 版本相关的数据直接存储在了 Kubernetes 中 Helm 组件及相关术语 helm Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。 Chart Helm 的软件包，采用 TAR 格式。类似于 APT 的 DEB 包或者 YUM 的 RPM 包，其包含了一组定义 Kubernetes 资源相关的 YAML 文件。 Repoistory Helm 的软件仓库，Repository 本质上是一个 Web 服务器，该服务器保存了一系列的 Chart 软件包以供用户下载，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。 Release 使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release。可以理解为 Helm 使用 Chart 包部署的一个应用实例 helm安装 下载helm release压缩包 release版本 解压，添加到PATH 创建应用 初始化 [root@node3 cloud]# helm create redis Creating redis 应用目录结构 [root@node3 cloud]# tree redis/ redis/ ├── charts ├── Chart.yaml ├── templates │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── hpa.yaml │ ├── ingress.yaml │ ├── NOTES.txt │ ├── serviceaccount.yaml │ ├── service.yaml │ └── tests │ └── test-connection.yaml └── values.yaml Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.2包管理器/pip/":{"url":"1.Linux基础/1.2包管理器/pip/","title":"pip","keywords":"","body":"配置pip 配置源 mkdir ~/.pip cat >> ~/.pip/pip.conf 下载包（只下载不安装） pip download -d -r requirement.txt requirement.txt格式内容 scipy numpy jupyter ipython easydict Cython h5py numpy mahotas requests bs4 lxml pillow redis torch torchvision paramiko pycrypto uliengineering matplotlib keras==2.1.5 web.py==0.40.dev0 scikit-image==0.15.0 lmdb pandas opencv-contrib-python==4.0.0.21 tensorflow-gpu==1.8 安装（离线导出）包 pip3 install --no-index --find-links=./pip -r requirement.txt Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.2包管理器/yum/A-配置本地源.html":{"url":"1.Linux基础/1.2包管理器/yum/A-配置本地源.html","title":"A-配置本地源","keywords":"","body":"本地yum源配置 适用于 主机不可以直连外网、且不可以通过代理访问外网 1、查看操作系统 cat /etc/system-release 2、获取系统安装镜像 1、获取方式一：找系统运维管理员提供，推荐 2、获取方式二：自己下载，不推荐，文件大小一般4G左右。下载版本>=当前系统版本，建议下载最新小版本（如操作系统为7.4.1708，则下载7.4.1708以上） 方便下载，推荐以下两个版本（CentOS6、CentOS7）： CentOS-6.10-x86_64-bin-DVD1.iso CentOS-7-x86_64-DVD-2009.iso 3、上传挂载 注意路径、文件名需要替换，以下命令相当于将CentOS-7-x86_64-DVD-2009.iso，解压iso文件至/media mount -o loop ~/CentOS-7-x86_64-DVD-2009.iso /media 4、卸载、拷贝、删除 这一步主要为了重启后不用重新挂载，当然多主机情况建议利用nginx等配置内网源server，以便其他服务器可以配置。 优势：减少无用的冗余数据占用存储空间。 mkdir -p /yum && cp -r /media/* /yum/ umout /media 5、删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 6、新建yum repo文件 tee /etc/yum.repos.d/c7.repo 7、测试 yum clean all && yum makecache yum install -y telnet vim 8.更新 更新系统&软件（可选） yum update -y 9.删除因更新产生的配置文件 新产生的配置文件需联网使用，内网环境无法访问，需要删除， rm -f /etc/yum.repos.d/CentOS-*.repo Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.2包管理器/yum/B-阿里源.html":{"url":"1.Linux基础/1.2包管理器/yum/B-阿里源.html","title":"B-阿里源","keywords":"","body":"阿里yum源 1、配置DNS解析 echo \"nameserver 114.114.114.114\" >> /etc/resolv.conf 2、删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 3、下载阿里yum源文件 CentOS 6 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo CentOS 7 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.2包管理器/yum/C-软件及依赖导出.html":{"url":"1.Linux基础/1.2包管理器/yum/C-软件及依赖导出.html","title":"C-软件及依赖导出","keywords":"","body":"导出依赖与使用 导出（yum源可用） yum install yum-plugin-downloadonly -y yum install --downloadonly --downloaddir=./gcc gcc 生成repo依赖关系 yum install -y createrepo createrepo ./gcc 压缩 tar zcvf gcc.tar.gz gcc 使用（yum源不可用） tar zxvf gcc.tar.gz -C / cat > /etc/yum.repos.d/gcc.repo Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.2包管理器/yum/D-构建rpm包.html":{"url":"1.Linux基础/1.2包管理器/yum/D-构建rpm包.html","title":"D-构建rpm包","keywords":"","body":"rpm制作 安装rpmbuild yum install rpm-build -y 创建目录 mkdir ~/rpmbuild/{BUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS} 编写~/rpmbuild/SPECS/nginx.spec 样例 %define realname nginx %define orgnize neu %define realver 1.18.0 %define srcext tar.gz %define opensslVersion openssl-1.0.2r # Common info Name: %{realname} Version: %{realver} Release: %{orgnize}%{?dist} Summary:Nginx is a web server software License:GPL URL:http://nginx.org Source0: %{realname}-%{realver}%{?extraver}.%{srcext} Source1: %{opensslVersion}.tar.gz Source2: headers-more-nginx-module-master.tar.gz Source3: naxsi-0.56.tar.gz Source4: nginx_upstream_check_module-master.tar.gz Source5: ngx-fancyindex-master.tar.gz Source6: ngx_cache_purge-2.3.tar.gz Source11: nginx.logrotate #Source12: nginx.conf Source13: conf Source14: nginx Source21: pcre-8.44.tar.gz Source22: zlib-1.2.11.tar.gz Source23: LuaJIT-2.0.5.tar.gz Source24: lua-nginx-module-0.10.13.tar.gz Source25: ngx_devel_kit-0.3.0.tar.gz #Patch: nginx-memset_zero.patch # Install-time parameters Provides: httpd http_daemon webserver %{?suse_version:suse_help_viewer} Requires: logrotate #BuildRequires: gcc zlib-devel pcre-devel %description nginx [engine x] is an HTTP and reverse proxy server %post chkconfig nginx on sed -i \"/* soft nofile 655350/d\" /etc/security/limits.conf echo \"* soft nofile 655350\" >> /etc/security/limits.conf sed -i \"/* hard nofile 655350/d\" /etc/security/limits.conf echo \"* hard nofile 655350\" >> /etc/security/limits.conf sed -i \"/* soft nproc 65535/d\" /etc/security/limits.conf echo \"* soft nproc 65535\" >> /etc/security/limits.conf sed -i \"/* hard nproc 65535/d\" /etc/security/limits.conf echo \"* hard nproc 65535\" >> /etc/security/limits.conf #sed -i '/\\/etc\\/logrotate.d\\/nginx/d' /etc/crontab #echo \"0 0 * * * root bash /usr/sbin/logrotate -f /etc/logrotate.d/nginx\" >> /etc/crontab mkdir -p ~/.vim cp -r -v /opt/nginx/vim ~/.vim/ cat > ~/.vim/filetype.vim c && %__mv -f c CHANGES.ru %__install -d %{buildroot}~/.vim %__install -D -m755 %{S:11} %{buildroot}%{_sysconfdir}/logrotate.d/%{name} %__install -D -m755 %{S:14} %{buildroot}%{_sysconfdir}/init.d/%{name} %__cp -r -v %{_builddir}/%{realname}-%{realver}%{?extraver}/lj2 %{buildroot}/opt/nginx/ %__cp -r -v %{_builddir}/%{realname}-%{realver}%{?extraver}/contrib/vim %{buildroot}/opt/nginx/ %__cp -r -v %{S:13}/* %{buildroot}/opt/nginx/conf/ %clean [ \"%{buildroot}\" != \"/\" ] && rm -rf %{buildroot} %files %config(noreplace) %{_sysconfdir}/logrotate.d/%{name} %config(noreplace) %{_sysconfdir}/init.d/%{name} %doc /opt/nginx/* %changelog 构建 rpmbuild -ba ~/rpmbuild/SPECS/nginx.spec Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.2包管理器/yum/yum执行命令卡住解决方案.html":{"url":"1.Linux基础/1.2包管理器/yum/yum执行命令卡住解决方案.html","title":"yum执行命令卡住解决方案","keywords":"","body":"ps aux | awk '/yum/{system(\"kill-9\"$2)}' #清除rpm库文件 rm -f /var/lib/rpm/__db* #重新构建 rpm --rebuilddb #清除yum缓存 yum clean all && rm -rf /var/cache/yum #重新缓存 yum makecache Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/":{"url":"1.Linux基础/1.3系统设置/","title":"1.3系统设置","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/01时钟服务器配置.html":{"url":"1.Linux基础/1.3系统设置/01时钟服务器配置.html","title":"01时钟服务器配置","keywords":"","body":"时钟服务器配置 安装ntpdate yum install -y ntp 配置定时任务，至少保证5分钟同步一次 */5 * * * * ntpdate ntp-server 硬件时间 查看硬件时间 $ hwclock -r 将硬件时间同步为当前系统时间 $ hwclock --systohc 手动同步 timedatectl set-ntp true Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/02时区配置.html":{"url":"1.Linux基础/1.3系统设置/02时区配置.html","title":"02时区配置","keywords":"","body":"配置为上海时区 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/03关闭防火墙.html":{"url":"1.Linux基础/1.3系统设置/03关闭防火墙.html","title":"03关闭防火墙","keywords":"","body":"关闭防火墙 systemctl disable firewalld --now Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/04关闭selinux.html":{"url":"1.Linux基础/1.3系统设置/04关闭selinux.html","title":"04关闭selinux","keywords":"","body":"关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/05调整文件描述符等.html":{"url":"1.Linux基础/1.3系统设置/05调整文件描述符等.html","title":"05调整文件描述符等","keywords":"","body":"调整文件描述符等 cat >> /etc/pam.d/login /etc/security/limits.conf cat >> /etc/security/limits.conf /etc/security/limits.d/20-nproc.conf cat >> /etc/security/limits.d/20-nproc.conf /proc/sys/fs/file-max Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/06配置yum本地源.html":{"url":"1.Linux基础/1.3系统设置/06配置yum本地源.html","title":"06配置yum本地源","keywords":"","body":"配置yum本地源 rm -rf /etc/yum.repos.d/* mount -o loop CentOS-7-x86_64-DVD-2009.iso /media mkdir -p /yum cp -r /media/* /yum umount /media rm -f CentOS-7-x86_64-DVD-2009.iso 配置文件 cat > /etc/yum.repos.d/c7.repo Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/07配置sudo用户.html":{"url":"1.Linux基础/1.3系统设置/07配置sudo用户.html","title":"07配置sudo用户","keywords":"","body":"配置sudo用户 强密码生成 或利用以下指令生成 pwmake 128 初始化用户，配置sudo权限 useradd -m ddd && echo \"m&t+arz4SEvWq5)QG\" | passwd --stdin ddd echo \"ddd ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/08配置互信.html":{"url":"1.Linux基础/1.3系统设置/08配置互信.html","title":"08配置互信","keywords":"","body":"配置互信 配置root用户 ssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa cat .ssh/id_rsa.pub > ~/.ssh/authorized_keys chmod -R 600 ~/.ssh Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/09关闭图形化.html":{"url":"1.Linux基础/1.3系统设置/09关闭图形化.html","title":"09关闭图形化","keywords":"","body":"关闭图形化 centos7 systemctl set-default multi-user.target init 3 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/10配置hostname.html":{"url":"1.Linux基础/1.3系统设置/10配置hostname.html","title":"10配置hostname","keywords":"","body":"配置hostname 方法一 cat >> /etc/sysconfig/network /proc/sys/kernel/hostname 方法二 cat >> /etc/sysconfig/network 方法三 cat >> /etc/sysconfig/network 方法四 hostnamectl --static set-hostname master Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.3系统设置/11文件打开数.html":{"url":"1.Linux基础/1.3系统设置/11文件打开数.html","title":"11文件打开数","keywords":"","body":"ulimit PAM模块 ssh 开启 pam 模块后应修改以下文件 /etc/pam.d/su /etc/pam.d/sshd /etc/pam.d/login /etc/pam.d/cron 添加如下内容 session required pam_limits.so 重启生效 systemctl restart sshd 修改/etc/security/limits.conf文件，用户列不能用*号，否则root登录后执行ulimit -n 值不对 root soft nofile 1048567 root hard nofile 1048567 root soft nproc 65535 root hard nproc 65535 NOTE: group and wildcard limits are not applied to the root user. To set a limit for the root user, this field must contain the literal username root. Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.4软件更新/":{"url":"1.Linux基础/1.4软件更新/","title":"1.4软件更新","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.4软件更新/01升级内核.html":{"url":"1.Linux基础/1.4软件更新/01升级内核.html","title":"01升级内核","keywords":"","body":"el7在线升级稳定版内核 导入public key,添加扩展源 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org yum install https://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm -y 安装最新稳定版 yum -y --enablerepo=elrepo-kernel install kernel-lt.x86_64 kernel-lt-devel.x86_64 删除旧版本工具包 yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 -y 安装新版本工具包 yum --disablerepo=\\* --enablerepo=elrepo-kernel install -y kernel-lt-tools.x86_64 查看内核列表 awk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg 重建内核 grub2-mkconfig -o /boot/grub2/grub.cfg 配置新版内核 sed -i \"s/GRUB_DEFAULT=saved/GRUB_DEFAULT=0/g\" /etc/default/grub 重启 reboot 删除旧版本内核 oldkernel=`rpm -qa|grep kernel-[0-9]` && yum remove -y $oldkernel el7在线升级主线版内核 导入public key,添加扩展源 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org yum install https://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm -y 安装最新主线版 yum -y --enablerepo=elrepo-kernel install kernel-ml.x86_64 kernel-ml-devel.x86_64 删除旧版本工具包 rpm -qa|grep kernel-3|xargs -n1 rpm -e rpm -e kernel-tools-libs-* 安装新版本工具包 yum --disablerepo=\\* --enablerepo=elrepo-kernel install -y kernel-ml-tools.x86_64 查看内核列表 awk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg 重建内核 grub2-mkconfig -o /boot/grub2/grub.cfg 配置新版内核 grub2-set-default 0 系统盘非raid模式直接重启 reboot trouble shooting pstore: unknown compression: deflate 启动异常 修改引导配置 vim /etc/default/grub 在GRUB_CMDLINE_LINUX最后添加mgag200.modeset=0 配置样例 GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=\"$(sed 's, release .*$,,g' /etc/system-release)\" GRUB_DEFAULT=saved GRUB_DISABLE_SUBMENU=true GRUB_TERMINAL_OUTPUT=\"console\" GRUB_CMDLINE_LINUX=\"crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet mgag200.modeset=0\" GRUB_DISABLE_RECOVERY=\"true\" 重新生成引导文件 grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.4软件更新/02升级gcc.html":{"url":"1.Linux基础/1.4软件更新/02升级gcc.html","title":"02升级gcc","keywords":"","body":"升级gcc 升级至5.4 下载介质 gcc-5.4.0.tar.gz gmp-4.3.2.tar.gz mpc-1.0.1.tar.gz mpfr-2.4.2.tar.gz 安装依赖 tar zxvf gmp-4.3.2.tar.gz cd gmp-4.3.2 ./configure --prefix=/usr/local/gmp-4.3.2 \\ && make -j $(nproc) && make install && cd - tar zxvf mpfr-2.4.2.tar.gz cd mpfr-2.4.2 ./configure --prefix=/usr/local/mpfr-2.4.2 \\ --with-gmp=/usr/local/gmp-4.3.2 \\ && make -j $(nproc) && make install && cd - tar zxvfv mpc-1.0.1.tar.gz cd mpc-1.0.1 ./configure --prefix=/usr/local/mpc-1.0.1 \\ --with-gmp=/usr/local/gmp-4.3.2 --with-mpfr=/usr/local/mpfr-2.4.2 \\ && make -j $(nproc) && make install && cd - 配置环境变量 echo \"export LD_LIBRARY_PATH=\\$LD_LIBRARY_PATH:/usr/local/gmp-4.3.2/lib:/usr/local/mpc-1.0.1/lib:/usr/local/mpfr-2.4.2/lib\" >> /etc/profile . /etc/profile 编译gcc tar -xzvf gcc-5.4.0.tar.gz && mkdir gcc-5.4.0/gcc-build && cd gcc-5.4.0/gcc-build \\ && ../configure --prefix=/usr/local/gcc-5.4.0 --enable-threads=posix \\ --disable-checking --disable-multilib --enable-languages=c,c++ \\ --with-gmp=/usr/local/gmp-4.3.2 --with-mpfr=/usr/local/mpfr-2.4.2 \\ --with-mpc=/usr/local/mpc-1.0.1 && make -j $(nproc) && make install && cd - 备份更新 mkdir -p /usr/local/bakup/gcc mv /usr/bin/{gcc,g++} /usr/local/bakup/gcc/ cp /usr/local/gcc-5.4.0/bin/gcc /usr/bin/gcc cp /usr/local/gcc-5.4.0/bin/g++ /usr/bin/g++ Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.4软件更新/03升级openssl.html":{"url":"1.Linux基础/1.4软件更新/03升级openssl.html","title":"03升级openssl","keywords":"","body":"openssl 1.下载最新稳定版 openssl release地址 2.安装必要依赖 yum install -y wget gcc perl 3.解压编译 tar zxvf openssl-OpenSSL_*.tar.gz cd openssl-OpenSSL* ./config shared --openssldir=/usr/local/openssl --prefix=/usr/local/openssl make -j $(nproc) && make install sed -i '/\\/usr\\/local\\/openssl\\/lib/d' /etc/ld.so.conf echo \"/usr/local/openssl/lib\" >> /etc/ld.so.conf ldconfig -v \\mv /usr/bin/openssl /usr/bin/openssl.old ln -s /usr/local/openssl/bin/openssl /usr/bin/openssl cd - openssl version Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.4软件更新/04升级openssh.html":{"url":"1.Linux基础/1.4软件更新/04升级openssh.html","title":"04升级openssh","keywords":"","body":"升级openssh 1.下载openssh包 下载站点 2.开启telnet(防止失败) yum install -y telnet-server telnet xinetd systemctl restart telnet.socket systemctl restart xinetd echo 'pts/0' >>/etc/securetty echo 'pts/1' >>/etc/securetty systemctl restart telnet.socket 3.安装 备份旧ssh配置文件 mv /etc/ssh/ /etc/ssh-bak 编译安装 yum install -y pam-devel zlib-devel tar zxvf openssh-*.tar.gz cd openssh* ./configure --prefix=/usr --sysconfdir=/etc/ssh --with-ssl-dir=/usr/local/openssl --with-md5-passwords make -j $(nproc) && make install 复制启动脚本： \\cp contrib/redhat/sshd.init /etc/init.d/sshd \\chkconfig sshd on 验证版本信息： ssh -V 配置 cat > /etc/ssh/sshd_config 调整service,重启ssh服务 sed -i \"s;Type=notify;#Type=notify;g\" /usr/lib/systemd/system/sshd.service systemctl daemon-reload && systemctl restart sshd 查看ssh服务是否健康 journalctl -xef -u sshd 启动时，如果报sshd: no hostkeys available — exiting错误，执行以下步骤修复 ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key chmod 600 /etc/ssh/* 重启ssh systemctl restart sshd 成功后关闭telnet systemctl disable telnet.socket --now systemctl disable xinetd --now Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.5驱动/":{"url":"1.Linux基础/1.5驱动/","title":"1.5驱动","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.5驱动/GPU驱动.html":{"url":"1.Linux基础/1.5驱动/GPU驱动.html","title":"GPU驱动","keywords":"","body":"卸载nouveau驱动 加入黑名单 $ echo \"blacklist nouveau\" >> /etc/modprobe.d/blacklist.conf 备份initramfs mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak 重建initramfs $ dracut -v /boot/initramfs-$(uname -r).img $(uname -r) --force 重启确认是否nouveau已被禁用 $ reboot $ lsmod | grep nouveau 安装Gpu驱动 1.查看Gpu版本 $ yum install -y pciutils $ lspci | grep NVIDIA 执行结果如下 3b:00.0 3D controller: NVIDIA Corporation GP102GL [Tesla P40] (rev a1) d8:00.0 3D controller: NVIDIA Corporation GP102GL [Tesla P40] (rev a1) 说明版本为Tesla P40 2.下载驱动 nvidia驱动 需要根据cuda版本选择，笔者这里cuda版本如下: $ nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Sun_Jul_28_19:07:16_PDT_2019 Cuda compilation tools, release 10.1, V10.1.243 下载地址如下： https://cn.download.nvidia.com/tesla/418.226.00/NVIDIA-Linux-x86_64-418.226.00.run 下载cuda https://developer.nvidia.com/cuda-toolkit-archive 笔者需要下载列表： https://developer.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda_10.1.105_418.39_linux.run 文件确认 此时文件列表： NVIDIA-Linux-x86_64-418.226.00.run: GPU驱动文件 cuda_10.1.105_418.39_linux.run: cuda文件 安装依赖 $ yum -y install kernel-devel kernel-doc kernel-headers gcc gcc-c++ 安装驱动 $ chmod +x NVIDIA-Linux-x86_64-418.226.00.run $ ./NVIDIA-Linux-x86_64-418.226.00.run --kernel-source-path=/usr/src/kernels/`uname -r` 安装过程：按提示键入交互 安装完毕后，测试显卡驱动 [root@localhost tmp]# nvidia-smi Thu Aug 11 02:48:19 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418.226.00 Driver Version: 418.226.00 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P40 Off | 00000000:3B:00.0 Off | 0 | | N/A 27C P0 49W / 250W | 0MiB / 22919MiB | 0% Default | +-------------------------------+----------------------+----------------------+ | 1 Tesla P40 Off | 00000000:D8:00.0 Off | 0 | | N/A 25C P0 49W / 250W | 0MiB / 22919MiB | 2% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 安装cuda $ chmod +x cuda_10.1.105_418.39_linux.run $ ./cuda_10.1.105_418.39_linux.run --silent 加入PATH $ echo \"export PATH=\\$PATH:/usr/local/cuda-10.1/bin\" >> /etc/profile $ . /etc/profile 查看cuda版本 $ nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Fri_Feb__8_19:08:17_PST_2019 Cuda compilation tools, release 10.1, V10.1.105 测试 $ conda create -n yourEnv python=3.6 numpy pandas $ conda activate yourEnv $ conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.1 -c pytorch 测试cuda是否可用 $ python >>> import torch >>> print(torch.cuda.is_available()) True Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.6虚拟化/01vsphere使用.html":{"url":"1.Linux基础/1.6虚拟化/01vsphere使用.html","title":"01vsphere使用","keywords":"","body":"磁盘扩容 1.编辑虚拟机调整硬盘空间，重启虚拟机 2.查看分区后扩容后的大小 fdisk -l 已经扩到了100G（扩容前30G） 3.新建分区并设置分区为LVM格式 fdisk /dev/sda 最后键入w写入 4.创建物理卷，并加入到卷组 partprobe pvcreate /dev/sda3 扩展vg卷组大小 vgs vgextend /dev/centos /dev/sda3 vgs 5.使用lvextend命令来扩容lv逻辑卷空间大小 lvs lvextend -L +69.9G /dev/centos/root 查看文件系统 df -lhT 重新加载逻辑卷的大小 xfs类型文件系统执行 xfs_growfs /dev/centos/root ext2、ext3、ext4类型文件系统执行 resize2fs /dev/centos/root 6.查看磁盘大小 df -h Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.6虚拟化/02kvm.html":{"url":"1.Linux基础/1.6虚拟化/02kvm.html","title":"02kvm","keywords":"","body":" Table of Contents generated with DocToc 安装kvm KVM的web管理界面 安装vnc 安装kvm 关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 安装依赖 yum install kvm kmod-kvm qemu -y 判断是否载入kvm模块 /sbin/lsmod | grep kvm 拷贝指令 cp /usr/libexec/qemu-kvm /usr/bin/ KVM的web管理界面 Wok Wok基于cherrypy的web框架，可以通过一些插件来进行扩展，例如：虚拟化管理、主机管理、系统管理。它可以在任何支持HTML5的网页浏览器中运行。 Kimchi Kimchi是一个基于HTML5的KVM管理工具，是Wok的一个插件（使用Kimchi前一定要先安装了wok），通过Kimchi可以更方便的管理KVM。 项目地址 安装work wok下载链接 yum install -y wok-2.5.0-0.el7.centos.noarch.rpm 安装kimchi kimchi下载链接 yum install -y kimchi-2.5.0-0.el7.centos.noarch.rpm 启动wok systemctl daemon-reload systemctl start wokd 开放8001端口 firewall-cmd --zone=public --add-port=8001/tcp --permanent #重新载入 firewall-cmd --reload 访问 安装vnc 安装拷贝文件 yum install tigervnc-server -y cp /lib/systemd/system/vncserver@.service /etc/systemd/system/vncserver.service 修改配置 vim /etc/systemd/system/vncserver.service 设置密码 重载启动 systemctl daemon-reload Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.6虚拟化/03openstack.html":{"url":"1.Linux基础/1.6虚拟化/03openstack.html","title":"03openstack","keywords":"","body":" Table of Contents generated with DocToc 控制节点 计算节点 参考文章 节点信息 控制节点：132.232.81.250 计算节点：139.155.29.65 控制节点 0.修改hostname cat >> /etc/sysconfig/network > /etc/hosts 1.配置yum源 rm -f /etc/yum.repos.d/*.repo curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 更新 cat > /etc/yum.repos.d/cloud.repo 2.安装openstack客户端 yum install python-openstackclient -y yum install openstack-selinux -y 3.mariadb数据库的安装 参考安装配置 OpenStack使用数据库来存储，支持大部分数据库MariaDB或、MySQL或者PostgreSQL，数据库运行于控制节点 yum install mariadb mariadb-server python2-PyMySQL -y 启动 systemctl enable mariadb.service --now 初始化 mysql_secure_installation 根据提示依次输入 Enter current password for root (enter for none): # 输入数据库超级管理员root的密码(注意不是系统root的密码)，第一次进入还没有设置密码则直接回车 Set root password? [Y/n] # 设置密码，y New password: # 新密码 Re-enter new password: # 再次输入密码 Remove anonymous users? [Y/n] # 移除匿名用户， y Disallow root login remotely? [Y/n] # 拒绝root远程登录，n，不管y/n，都会拒绝root远程登录 Remove test database and access to it? [Y/n] # 删除test数据库，y：删除。n：不删除，数据库中会有一个test数据库，一般不需要 Reload privilege tables now? [Y/n] # 重新加载权限表，y。或者重启服务也许 4.安装rabbitmq 安装 yum install rabbitmq-server -y 启动 systemctl enable rabbitmq-server --now 初始化 rabbitmqctl add_user openstack RABBIT_PASS rabbitmqctl set_permissions openstack \".*\" \".*\" \".*\" 5.安装Memcached yum install memcached python-memcached -y 调整配置 sed -i \"s#127.0.0.1#0.0.0.0#g\" /etc/sysconfig/memcached 启动 systemctl enable memcached.service --now 6.安装keystone服务 创建数据库用户 CREATE DATABASE keystone; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \\ IDENTIFIED BY 'KEYSTONE_DBPASS'; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \\ IDENTIFIED BY 'KEYSTONE_DBPASS'; 安装keystone相关软件包 yum install openstack-keystone httpd mod_wsgi -y 配置keystone cat > /etc/keystone/keystone.conf 同步数据 su -s /bin/sh -c \"keystone-manage db_sync\" keystone 初始化fernet keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone 配置httpd echo \"ServerName controller\" >>/etc/httpd/conf/httpd.conf echo 'Listen 5000 Listen 35357 WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP} WSGIProcessGroup keystone-public WSGIScriptAlias / /usr/bin/keystone-wsgi-public WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On ErrorLogFormat \"%{cu}t %M\" ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined Require all granted WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP} WSGIProcessGroup keystone-admin WSGIScriptAlias / /usr/bin/keystone-wsgi-admin WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On ErrorLogFormat \"%{cu}t %M\" ErrorLog /var/log/httpd/keystone-error.log CustomLog /var/log/httpd/keystone-access.log combined Require all granted ' >/etc/httpd/conf.d/wsgi-keystone.conf 启动httpd systemctl start httpd systemctl enable httpd 初始化keystone export OS_TOKEN=ADMIN_TOKEN export OS_URL=http://controller:35357/v3 export OS_IDENTITY_API_VERSION=3 openstack service create --name keystone --description \"OpenStack Identity\" identity openstack endpoint create --region RegionOne identity public http://controller:5000/v3 openstack endpoint create --region RegionOne identity internal http://controller:5000/v3 openstack endpoint create --region RegionOne identity admin http://controller:35357/v3 # 创建域,项目,用户,角色 openstack domain create --description \"Default Domain\" default openstack project create --domain default --description \"Admin Project\" admin openstack user create --domain default --password ADMIN_PASS admin openstack role create admin openstack role add --project admin --user admin admin openstack project create --domain default \\ --description \"Service Project\" service unset OS_TOKEN OS_URL ###把他加入开机自启，不然下次启动会无法访问 export OS_PROJECT_DOMAIN_NAME=default export OS_USER_DOMAIN_NAME=default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=ADMIN_PASS export OS_AUTH_URL=http://controller:35357/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 验证keystone服务是否正常 7.安装glance镜像服务 mysql中创库授权 CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \\ IDENTIFIED BY 'GLANCE_DBPASS'; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \\ IDENTIFIED BY 'GLANCE_DBPASS'; 在keystone创建系统账号,并关联角色 openstack user create --domain default --password GLANCE_PASS glance openstack role add --project service --user glance admin 在keystone上创建服务名称,注册api openstack service create --name glance --description \"OpenStack Image\" image openstack endpoint create --region RegionOne image public http://controller:9292 openstack endpoint create --region RegionOne image internal http://controller:9292 openstack endpoint create --region RegionOne image admin http://controller:9292 安装相关软件包 yum install openstack-glance openstack-utils -y 修改配置文件 openstack-config --set /etc/glance/glance-api.conf database connection mysql+pymysql://glance:GLANCE_DBPASS@controller/glance openstack-config --set /etc/glance/glance-api.conf glance_store stores file,http openstack-config --set /etc/glance/glance-api.conf glance_store default_store file openstack-config --set /etc/glance/glance-api.conf glance_store filesystem_store_datadir /var/lib/glance/images/ openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_type password openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name default openstack-config --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name default openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name service openstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glance openstack-config --set /etc/glance/glance-api.conf keystone_authtoken password GLANCE_PASS openstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystone #cat glance-registry.conf >/etc/glance/glance-registry.conf openstack-config --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:GLANCE_DBPASS@controller/glance openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_type password openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name default openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name default openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name service openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glance openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password GLANCE_PASS openstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystone 同步数据(创表) su -s /bin/sh -c \"glance-manage db_sync\" glance 启动服务 systemctl enable openstack-glance-api.service openstack-glance-registry.service systemctl start openstack-glance-api.service openstack-glance-registry.service 验证。上传cirros-0.3.4-x86_64-disk.img到当前目录 openstack image create \"cirros\" --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public 检查上传结果 openstack image list 8.安装nova计算服务控制端 mysql中创库授权 CREATE DATABASE nova_api; CREATE DATABASE nova; GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' \\ IDENTIFIED BY 'NOVA_DBPASS'; GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' \\ IDENTIFIED BY 'NOVA_DBPASS'; GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \\ IDENTIFIED BY 'NOVA_DBPASS'; GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \\ IDENTIFIED BY 'NOVA_DBPASS'; 在keystone创建系统账号,并关联角色 openstack user create --domain default --password NOVA_PASS nova openstack role add --project service --user nova admin 在keystone上创建服务名称,注册api openstack service create --name nova \\ --description \"OpenStack Compute\" compute openstack endpoint create --region RegionOne \\ compute public http://controller:8774/v2.1/%\\(tenant_id\\)s openstack endpoint create --region RegionOne \\ compute internal http://controller:8774/v2.1/%\\(tenant_id\\)s openstack endpoint create --region RegionOne \\ compute admin http://controller:8774/v2.1/%\\(tenant_id\\)s 安装相关软件包 yum install -y openstack-nova-api openstack-nova-placement-api \\ openstack-nova-conductor openstack-nova-console \\ openstack-nova-novncproxy openstack-nova-scheduler 修改配置文件 cp /etc/nova/nova.conf{,.bak} grep -Ev '^$|#' /etc/nova/nova.conf.bak >/etc/nova/nova.conf openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata openstack-config --set /etc/nova/nova.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 10.0.0.11 openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron True openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver openstack-config --set /etc/nova/nova.conf api_database connection mysql+pymysql://nova:NOVA_DBPASS@controller/nova_api openstack-config --set /etc/nova/nova.conf database connection mysql+pymysql://nova:NOVA_DBPASS@controller/nova openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova openstack-config --set /etc/nova/nova.conf keystone_authtoken password NOVA_PASS openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_host controller openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstack openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password RABBIT_PASS openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu openstack-config --set /etc/nova/nova.conf libvirt cpu_mode none openstack-config --set /etc/nova/nova.conf vnc enabled True openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0 openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address '$my_ip' openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://controller:6080/vnc_auto.html openstack-config --set /etc/nova/nova.conf neutron url http://controller:9696 openstack-config --set /etc/nova/nova.conf neutron auth_url http://controller:35357 openstack-config --set /etc/nova/nova.conf neutron auth_type password openstack-config --set /etc/nova/nova.conf neutron project_domain_name default openstack-config --set /etc/nova/nova.conf neutron user_domain_name default openstack-config --set /etc/nova/nova.conf neutron region_name RegionOne openstack-config --set /etc/nova/nova.conf neutron project_name service openstack-config --set /etc/nova/nova.conf neutron username neutron openstack-config --set /etc/nova/nova.conf neutron password NEUTRON_PASS openstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy True openstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret METADATA_SECRET 修改myip为实际IP sed -i \"s#my_ip = 10.0.0.11#my_ip = 172.27.0.13#g\" /etc/nova/nova.conf 配置placement sed -i '/\\[placement\\]/d' /etc/nova/nova.conf cat >> /etc/nova/nova.conf 修改/etc/httpd/conf.d/00-nova-placement-api.conf,在之间添加如下代码 = 2.4> Require all granted Order allow,deny Allow from all 同步nova-api数据库 su -s /bin/sh -c \"nova-manage api_db sync\" nova 注册cell0数据库 su -s /bin/sh -c \"nova-manage cell_v2 map_cell0\" nova 创建cell1的cell su -s /bin/sh -c \"nova-manage cell_v2 create_cell --name=cell1 --verbose\" nova 同步nova数据库 su -s /bin/sh -c \"nova-manage db sync\" nova 验证cell0和cell1的注册是否正确 nova-manage cell_v2 list_cells 启动服务 systemctl enable openstack-nova-api.service \\ openstack-nova-consoleauth.service openstack-nova-scheduler.service \\ openstack-nova-conductor.service openstack-nova-novncproxy.service systemctl restart openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service nova服务注册 openstack service create --name nova --description \"OpenStack Compute\" compute openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1 openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1 openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1 openstack service create --name placement --description \"Placement API\" placement openstack endpoint create --region RegionOne placement public http://controller:8778 openstack endpoint create --region RegionOne placement internal http://controller:8778 openstack endpoint create --region RegionOne placement admin http://controller:8778 验证控制节点服务 openstack host list 计算节点 1.配置yum源 rm -f /etc/yum.repos.d/*.repo curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 更新 cat > /etc/yum.repos.d/cloud.repo /etc/yum.repos.d/CentOS-Virt.repo 2.安装openstack客户端 yum install python-openstackclient -y yum install openstack-selinux -y 3.修改hostname，配置/etc/hosts文件 cat >> /etc/sysconfig/network > /etc/hosts 4.计算节点安装nova计算服务agent端 安装软件 yum install openstack-nova-compute openstack-utils -y 配置 cp /etc/nova/nova.conf{,.bak} grep '^[a-Z\\[]' /etc/nova/nova.conf.bak >/etc/nova/nova.conf openstack-config --set /etc/nova/nova.conf DEFAULT transport_url rabbit://openstack:RABBIT_PASS@controller openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 10.0.0.31 openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron True openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver openstack-config --set /etc/nova/nova.conf glance api_servers http://controller:9292 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://controller:35357 openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers controller:11211 openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name default openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova openstack-config --set /etc/nova/nova.conf keystone_authtoken password NOVA_PASS openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_host controller openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstack openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password RABBIT_PASS openstack-config --set /etc/nova/nova.conf vnc enabled True openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0 openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address '$my_ip' openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://controller:6080/vnc_auto.html 调整my_ip sed -i \"s#10.0.0.31#172.27.0.8#g\" /etc/nova/nova.conf 启动 systemctl start libvirtd systemctl enable libvirtd systemctl start openstack-nova-compute systemctl enable openstack-nova-compute 控制节点验证 openstack compute service list 计算节点加入控制节点（控制节点执行） su -s /bin/sh -c \"nova-manage cell_v2 discover_hosts --verbose\" nova Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.6虚拟化/04Ovirt.html":{"url":"1.Linux基础/1.6虚拟化/04Ovirt.html","title":"04Ovirt","keywords":"","body":" Table of Contents generated with DocToc 磁盘阵列 oVirt安装 添加存储 添加主机 添加存储域 创建iso域 上传镜像 新增虚机 磁盘阵列 先安装配置raid10,挂载路径/data 根据磁盘数量创建阵列、推荐raid5 oVirt安装 配置阿里yum源 关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 安装 ovirt-release42.rpm yum install -y ovirt-release42.rpm 修改ovirt-4.2.repo、ovirt-4.2-dependencies.repo #baseurl调整为baseurl mirrorlist调整为#mirrorlist gpgcheck=1调整为gpgcheck=0 sed -i 's#gpgcheck=1#gpgcheck=0#g' /etc/yum.repos.d/*.repo yum -y update --nogpgcheck yum install firewalld -y yum install -y ovirt-engine --nogpgcheck 启动 systemctl start ovirt-engine systemctl enable ovirt-engine 配置 确保80没被占用,或修改httpd服务端口 参考地址 需要开启防火墙，不然配置报错 engine-setup #默认配置 #engine-setup --accept-defaults 修改配置 echo \"SSO_CALLBACK_PREFIX_CHECK=false\" > /etc/ovirt-engine/engine.conf.d/99-sso.conf 重启 systemctl restart ovirt-engine 开放443端口 firewall-cmd --zone=public --add-port=443/tcp --permanent #重新载入 firewall-cmd --reload 登陆 添加存储 计算 => 数据中心 => 新建 添加主机 关闭新建ssh连接确认 sed -i \"s;# StrictHostKeyChecking ask;StrictHostKeyChecking no;g\" /etc/ssh/ssh_config systemctl restart sshd 关闭yum公钥检测 默认使用的时候没问题，通过ovirt使用提示校验公钥失败 sed -i 's#gpgcheck=1#gpgcheck=0#g' /etc/yum.repos.d/*.repo 新增主机 计算 => 主机 => 新建 点击确定前，确保目标主机yum可用，且关闭公钥检测 查看部署日志 tail -200f /var/log/ovirt-engine/engine.log 添加存储域 存储 => 存储域 => 新建 创建iso域 安装配置nfs yum install -y nfs-utils rpcbind systemctl daemon-reload systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server mkdir -p /images echo \"/images 192.168.0.0/16(rw)\">>/etc/exports exportfs -a chmod 777 -R /images 详细教程 创建iso域 上传镜像 ovirt-iso-uploader -v --iso-domain=images upload /root/CentOS-7-x86_64-Minimal-1810.iso 新增虚机 计算 -> 虚拟机 -> 新建 新增磁盘 添加网络 配置内存cpu、时区 配置引导 安装图形化界面客户端 windows下载地址 启动、打开控制台 打开文件、安装操作系统 关机虚机、点击右上角... 创建模板 填写模板信息 从模板机创建虚拟机 计算 -> 模板 -> 创建虚拟机 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.6虚拟化/05iDRAC.html":{"url":"1.Linux基础/1.6虚拟化/05iDRAC.html","title":"05iDRAC","keywords":"","body":" Table of Contents generated with DocToc 安装java 安装java 1.下载虚拟控制台连接文件 2.widows环境下下载安装java1.7 jre1.7下载地址 3.调整java设置 控制面板->程序->Java Java->查看->勾选取消高版本->确定 高级->勾选调试三项->应用确定 4.设置引导 登录iDRAC控制台,配置如下 5.运行jnlp文件 鼠标右键步骤1下载的文件 -> 打开方式 -> 选择jre1.7的javaws.exe文件 允许使用过期版本 控制台日志输出 iDRAC虚拟控制台 6.安装操作系统 添加虚拟介质 添加映射本地iso 执行温引导 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.6虚拟化/06migration.html":{"url":"1.Linux基础/1.6虚拟化/06migration.html","title":"06migration","keywords":"","body":"安装openstack单节点 配置阿里yum镜像源 rm -f /etc/yum.repos.d/* 在线 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 离线：手动下载后上传至/etc/yum.repos.d/ Centos-7.repo epel-7.repo 关闭防火墙 systemctl disable firewalld --now 关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 关闭NetworkManager服务 systemctl disable NetworkManager --now 安装依赖 yum update -y yum install python3-devel libffi-devel gcc openssl-devel python3-libselinux python3-pip ansible -y 配置pip源与代理 proxy=http://xxx.xxx.xxx.xxx:8080替换为实际代理，如不需代理删除该配置项 mkdir ~/.pip cat >> ~/.pip/pip.conf 安装kolla-ansible pip3 install -U pip pip3 install kolla-ansible 安装docker yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum -y install docker-ce systemctl enable docker --now 配置镜像加速 yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum -y install docker-ce systemctl enable docker --now 拷贝配置 mkdir -p /etc/kolla cp -r /usr/local/share/kolla-ansible/etc_examples/kolla/* /etc/kolla cp /usr/local/share/kolla-ansible/ansible/inventory/* /etc/kolla 修改hostname添加解析 hostnamectl set-hostname kolla echo '192.168.1.6 kolla' >> /etc/hosts 调整配置 生成/etc/kolla/passwords.yml配置项密码 /usr/local/bin/kolla-genpwd 修改keystone_admin_password值 vi /etc/kolla/passwords.yml 找到keystone_admin_password，替换值为Admin@123 调整网络配置 em1为宿主机网卡名称 sed -i \"s;#network_interface: \"eth0\";network_interface: \"em1\";g\" /etc/kolla/globals.yml sed -i \"s;#enable_haproxy: \\\"yes\\\";enable_haproxy: \\\"no\\\";g\" /etc/kolla/globals.yml 192.168.1.6为宿主机IP sed -i \"s;#kolla_internal_vip_address: \\\"10.10.10.254\\\";kolla_internal_vip_address: \\\"192.168.1.6\\\";g\" /etc/kolla/globals.yml 克隆 git clone https://opendev.org/openstack/openstack-ansible 新增yum源 openstack-train.repo cat > /etc/yum.repos.d/openstack-train.repo kvm-common.repo cat > /etc/yum.repos.d/kvm-common.repo 配置yum代理 适用于主机通过代理访问互联网场景 以下变量注意替换 username: 代理用户名 password: 代理用户密码 proxy_host: 代理IP地址 proxy_port: 代理端口 echo \"proxy=http://username:password@proxy_host:proxy_port\" >> /etc/yum.conf 重建yum缓存 yum clean all yum makecache 安装OpenStack-packstack软件包 yum -y install openstack-packstack 回退leatherman版本 yum downgrade leatherman -y 生成默认配置 packstack --gen-answer-file=~/openstack.txt 修改~/openstack.txt配置 修改内容如下 sed -i \"s#CONFIG_SWIFT_INSTALL=y#CONFIG_SWIFT_INSTALL=n#g\" ~/openstack.txt sed -i \"s#CONFIG_AODH_INSTALL=y#CONFIG_AODH_INSTALL=n#g\" ~/openstack.txt sed -i \"s#CONFIG_NEUTRON_ML2_TYPE_DRIVERS=geneve,flat#CONFIG_NEUTRON_ML2_TYPE_DRIVERS=vxlan,flat#g\" ~/openstack.txt sed -i \"s#CONFIG_NEUTRON_ML2_TENANT_NETWORK_TYPES=geneve#CONFIG_NEUTRON_ML2_TENANT_NETWORK_TYPES=vxlan#g\" ~/openstack.txt sed -i \"s#CONFIG_NEUTRON_ML2_MECHANISM_DRIVERS=ovn#CONFIG_NEUTRON_ML2_MECHANISM_DRIVERS=openvswitch#g\" ~/openstack.txt 手动修改项 CONFIG_COMPUTE_HOSTS=192.168.19 #计算节点ip地址 CONFIG_NEUTRON_ML2_FLAT_NETWORKS=physnet1 #flat网络这边要设置物理网卡名字 CONFIG_NEUTRON_L2_AGENT=openvswitch #L2网络的代理模式,也可选择linuxbridge CONFIG_NEUTRON_OVS_BRIDGE_MAPPINGS=physnet1:br-ex #这边要设置物理网卡的名字 CONFIG_NEUTRON_OVS_BRIDGE_IFACES=br-ex:eth0 #这边br-ex:eth0是网络节点的nat网卡，到时候安装完毕之后IP地址会飘到这个上 更改主机密码（123456需要替换） sed -i -r 's/(.+_PW)=.+/\\1=123456/' openstack.txt 备份配置 cp openstack.txt openstack.txt.bak 安装 packstack --answer-file=~/openstack.txt 如出现如下错误 ... Applying Puppet manifests [ ERROR ] ... 执行以下语句 sed -i \"s;#baseurl;baseurl;g\" /etc/yum.repos.d/*.repo sed -i \"s;mirrorlist=;#mirrorlist=;g\" /etc/yum.repos.d/*.repo rm -f /etc/yum.repos.d/CentOS-* rm -f * rm -rf /var/tmp/packstack/ packstack --allinone 错误二 ... Error: (pymysql.err.OperationalError) (1045, u\"Access denied for user 'nova'@'192.168.1.6' (using password: YES)\") (Background on this error at: http://sqlalche.me/e/e3q8) ... 执行以下命令 su -s /bin/sh -c \"nova-manage cell_v2 create_cell --name=cell1 --verbose\" nova 获取admin登录口令 [root@localhost ~]# cat keystonerc_admin|grep OS_PASSWORD export OS_PASSWORD='ab2529c81120445a' Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.6虚拟化/07磁盘分区相关.html":{"url":"1.Linux基础/1.6虚拟化/07磁盘分区相关.html","title":"07磁盘分区相关","keywords":"","body":"突破2T上限 修改分区表类型 parted /dev/sdb mklabel gpt 分区 fdisk /dev/sdb 格式化 mkfs.ext4 /dev/sdb1 挂载 mount /dev/sdb1 /data1 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.6虚拟化/08安装vcenter.html":{"url":"1.Linux基础/1.6虚拟化/08安装vcenter.html","title":"08安装vcenter","keywords":"","body":"安装vcenter 仅作学习交流之用 前置依赖：windows 虚拟机 + Exsi 系统 准备 vcenter 镜像 基于 windows 进行桌面安装 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.6虚拟化/exsi.html":{"url":"1.Linux基础/1.6虚拟化/exsi.html","title":"exsi","keywords":"","body":"设置主机名 esxcfg-advcfg -s 新的主机名称 /Misc/hostname Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.6虚拟化/openstack.html":{"url":"1.Linux基础/1.6虚拟化/openstack.html","title":"openstack","keywords":"","body":"单机 https://blog.51cto.com/qchenz/4964475 编辑/usr/lib/systemd/system/mariadb.service文件，在文件[Service]下添加 LimitNOFILE=65535 LimitNPROC=65535 保存后，执行下面命令，使配置生效 systemctl daemon-reload systemctl restart mariadb.service CONFIG_MARIADB_PW=2a76a0b9f1c941d0 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/":{"url":"1.Linux基础/1.7存储/ceph/","title":"ceph","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/01简述.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/01简述.html","title":"01简述","keywords":"","body":"说明 ceph相关于nfs性能较高，且能提供块存储、文件系统、对象存储网关三种类型存储。 ceph集成部署较为简单、且成本较低(相对于nas、SAN) ceph可利用现有服务器资源（裸金属服务器），而不用额外购买存储设备。构建高性能、高可用的统一存储系统，供公司内部适用 主流云计算系统（Openstack等）一般选取ceph作为后端存储，说明其优异性 ceph开源，相比商用软件有天然优势 Ceph解析 Ceph是一个提供对象存储、块存储和文件存储的统一存储系统。 Ceph是高度可靠、易于管理和免费的。Ceph的强大功能可以改变企业IT基础设施和管理大量数据的能力。 Ceph提供了超强的可伸缩性--数以千计的客户端访问pb到eb的数据. Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/02相关术语.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/02相关术语.html","title":"02相关术语","keywords":"","body":"Ceph术语 本术语表中的术语旨在解释说明Ceph现有的技术术语。 cephx：Ceph认证协议，操作类似Kerberos，但不存在单点故障情况 Ceph平台：所有Ceph软件 Ceph堆栈：Ceph的两种或多种组件的集合 集群映射：包括MON映射、OSD映射、PG映射、MDS映射和CRUSH映射 Ceph对象存储：由Ceph存储集群和Ceph对象网关组成 RGW：Ceph的S3/Swift网关组件 RBD：Ceph块存储组件 Ceph块存储：用于与librbd、管理程序（如QEMU或Xen）和管理程序抽象层（如libvirt）结合使用 Ceph文件系统：Ceph的POSIX文件系统组件 OSD：物理或逻辑存储单元(如LUN)。有时，Ceph用户使用术语OSD来指代Ceph OSD守护进程，尽管正确的术语是Ceph OSD Ceph OSD：Ceph OSD守护进程，与逻辑盘(OSD)交互。 OSD id：定义OSD的整数。它由监视器生成，作为创建新OSD的一部分 OSD fsid：这是一个唯一标识符，用于进一步提高OSD的唯一性，它可以在OSD路径中的osd_fsid文件中找到。这个fsid术语可以与uuid互换使用 OSD uuid：与OSD fsid一样，这是OSD唯一标识符，可以与OSD fsid互换使用 bluestore：OSD BlueStore是OSD守护程序（kraken和更新版本）的新后端。与filestore不同，它直接将对象存储在Ceph块设备上，而不需要任何文件系统接口 filestore：OSD守护进程的后端，需要日志记录、将文件写入文件系统 MON：Ceph监控软件 MGR：Ceph管理器软件，收集集群所有状态 MDS：Ceph元数据服务 Ceph Client：可以访问Ceph存储集群的Ceph组件的集合。组件包括Ceph对象网关、Ceph块设备、Ceph文件系统以及它们相应的库、内核模块、用户空间文件系统 Ceph Kernel Modules：内核模块的集合，可以用来与Ceph系统交互(例如，Ceph.ko, rbd.ko)。 Ceph Client Libraries：可用于与Ceph系统组件交互的库集合 Ceph Release：任何明显编号的Ceph版本 Ceph Point Release：任何只包含错误或安全修正的特别发行版 Ceph Interim Release：Ceph的版本尚未通过质量保证测试，但可能包含新特性 Ceph Release Candidate：Ceph的一个主要版本，已经经历了最初的质量保证测试，并且已经为beta测试做好了准备 Ceph Stable Release：Ceph的主要版本，其中所有来自前一个临时版本的特性都成功地通过了质量保证测试 Teuthology：在Ceph上执行脚本测试的软件集合 CRUSH：可伸缩散列下的受控复制(Controlled Replication Under Scalable Hashing)。这是Ceph用来计算对象存储位置的算法 CRUSH rule：适用于特定池的压缩数据放置规则 Pools：池是用于存储对象的逻辑分区 systemd oneshot：一种systemd类型，其中一个命令定义在ExecStart中，它将在完成时退出(它不打算后台化) LVM tags：用于LVM卷和组的可扩展元数据。它用于存储关于设备及其与osd的关系的特定于`c Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/03存储流程解析.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/03存储流程解析.html","title":"03存储流程解析","keywords":"","body":"Ceph存储集群 Ceph提供了一个基于RADOS(一种可扩展的、可靠的pb级存储集群存储服务)的无限可扩展的Ceph存储集群. Ceph存储集群由两种类型的守护进程组成: Ceph Monitor（mon） Ceph OSD Daemon（OSD） 其中Ceph Monitor维护集群映射的主副本。多节点Ceph Monitor确保了Ceph Monitor守护进程失败时的高可用性。 Ceph客户端从Ceph Monitor获取集群信息 ceph osd守护进程检查自己的状态和其他OSD的状态，并向Ceph Monitor上报。 Ceph客户端和每个ceph osd守护进程使用CRUSH算法高效地计算数据位置信息，而不必依赖于中央查找表。 Ceph的高级特性包括通过librados提供到Ceph存储集群的本地接口，以及建立在librados之上的许多服务接口。 Ceph数据存储流程 Ceph存储集群从Ceph客户端接收数据——无论是通过一个Ceph块设备、Ceph对象存储、Ceph文件系统还是使用librados创建的自定义实现——它将数据作为对象存储。 每个对象都对应于文件系统中的一个文件，文件系统存储在对象存储设备上。ceph osd守护进程处理存储磁盘的读写操作。 ceph osd守护进程将所有数据作为对象存储在一个平面命名空间中(没有目录层次结构)。 对象具有标识符、二进制数据和由一组名称/值对组成的元数据。语义完全由Ceph客户端决定。 例如，CephFS使用元数据存储文件属性，如文件所有者、创建日期、最后修改日期等。其中，对象ID全局唯一。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/04高可用性.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/04高可用性.html","title":"04高可用性","keywords":"","body":"Ceph的可伸缩性和高可用性 在传统的架构中，客户端与一个集中的组件(例如，网关、代理、API、facade等)通信，该组件充当一个进入复杂子系统的单一入口点。 这对性能和可伸缩性都施加了限制，同时引入了单点故障(例如，如果集中式组件宕机，整个系统也宕机） Ceph消除了集中式网关，使客户端可以直接与ceph osd守护进程交互。ceph osd守护进程在其他Ceph节点上创建对象副本，以确保数据的安全性和高可用性。 Ceph还使用一组mon来确保高可用性。为了消除集中化，Ceph使用了一种称为CRUSH的算法 mon高可用 在Ceph客户机能够读写数据之前，它们必须访问Ceph mon以获取集群映射的最新副本。Ceph存储集群可以使用单个mon进行操作;然而，这引入了单点故障(即，如果监视器出现故障，Ceph客户机就无法读写数据)。 为了增加可靠性和容错性，Ceph支持mon集群。在一个监视器集群中，延迟和其他故障可能导致一个或多个监视器落后于集群的当前状态。由于这个原因，Ceph必须在关于集群状态的各个监视器实例之间达成一致。Ceph总是使用大多数监视器(例如，1、2:3、3:5、4:6等)和Paxos算法来在监视器之间建立关于集群当前状态的共识 即部署多点ceph mon 规避单点故障 身份认证高可用性 为了识别用户并防止中间人攻击，Ceph提供了cephx身份验证系统来验证用户和守护进程。（cephx协议不处理传输中的数据加密(例如，SSL/TLS)或静止时的加密。） Cephx使用共享密钥进行身份验证，这意味着客户端和监控集群都拥有客户端密钥的副本。身份验证协议是这样的，双方能够向对方证明他们有一个密钥的副本，而不实际暴露它。这提供了相互的身份验证，这意味着集群确定用户拥有密钥，用户也确定集群拥有密钥的副本 Ceph的一个关键的可伸缩性特性是避免对Ceph对象存储的集中接口，这意味着Ceph客户端必须能够直接与OSD交互。为了保护数据，Ceph提供了其cephx身份验证系统，该系统对操作Ceph客户端的用户进行身份验证。cephx协议的操作方式与Kerberos类似 要使用cephx，管理员必须首先设置用户。在下面的图表中client.admin从命令行调用ceph auth get-or-create-key来生成用户名和密钥。 Ceph的auth子系统生成用户名和密钥，将一个副本存储在监视器中，并将用户的密钥传输回client.admin。 这意味着客户端和监视器共享一个密钥来使用cephx. 为了使用监视器进行身份验证，客户端将用户名传递给监视器，监视器生成一个会话密钥并使用与用户名相关联的密钥对其进行加密。 然后，监视器将加密的票据传输回客户端。随后，客户机使用共享密钥解密，以检索会话密钥。会话密钥标识当前会话的用户。 然后客户端使用会话密钥签名的用户请求票据。监视器生成一个票据，用用户的密钥对其加密，并将其传回客户机。 客户端解密票据，并使用它对整个集群中的OSDs和元数据服务器的请求进行签名 cephx协议对客户端机器和Ceph服务器之间的通信进行身份验证。 在初始身份验证之后，客户机和服务器之间发送的每个消息都使用票据进行签名， 监视器、OSD和元数据服务器可以使用它们的共享秘密来验证该票据 这种身份验证提供的保护在Ceph客户端和Ceph服务器主机之间。身份验证没有扩展到Ceph客户端之外。 如果用户从远程主机访问Ceph客户端，Ceph身份验证不应用于用户的主机和客户端主机之间的连接 分级缓存 缓存层为Ceph客户端提供了更好的I/O性能，用于存储在备份存储层中的数据子集。 分级缓存包括创建一个作为缓存层的相对快速/昂贵的存储设备(例如，固态驱动器)池，以及一个配置为纠错码或相对较慢/便宜的设备作为经济存储层的后备池。 Ceph objecter处理放置对象的位置，而分层代理确定何时将对象从缓存刷新到后备存储层。因此，缓存层和后备存储层对Ceph客户端是完全透明的 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/05存储类型.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/05存储类型.html","title":"05存储类型","keywords":"","body":"Ceph存储类型 Ceph客户端包括许多服务接口: 块设备：Ceph块设备(又称RBD)服务提供可调整大小、精简配置的块设备，并提供快照和克隆。Ceph跨集群划分块设备以获得高性能。Ceph既支持内核对象(KO)，也支持直接使用librbd的QEMU管理程序——避免了虚拟化系统的内核对象开销 对象存储：Ceph对象存储服务(简称RGW)提供RESTful api，兼容Amazon S3和OpenStack Swift接口 文件系统：Ceph文件系统(cepphfs)服务提供一个兼容POSIX的文件系统，可以挂载，也可以作为用户空间中的文件系统(FUSE)使用。 1.Ceph块存储 块是字节序列（例如，512字节的数据块）。基于块的存储接口是使用旋转介质（如硬盘、CD、软盘，甚至传统的9磁道磁带）存储数据的最常用方法。 块设备接口的普遍性使得虚拟块设备成为与Ceph这样的海量数据存储系统交互的理想候选设备 Ceph块设备是精简配置的，可调整大小，并在Ceph集群中的多个OSD上存储数据条带化。 Ceph块设备利用RADOS功能，如快照、复制和一致性。 Ceph的RADOS块设备（RBD）使用内核模块或librbd库与OSD交互 Ceph的block设备以无限的可扩展性向内核模块或kvm（如QEMU）以及为基于云的计算系统（如OpenStack和CloudStack）提供高性能存储， 这些系统依赖libvirt和QEMU与Ceph block设备集成。您可以使用同一集群同时操作Ceph-RADOS网关、CephFS文件系统和Ceph-block设备。 Ceph块设备在Ceph存储集群中的多个对象上划分块设备映像，每个对象映射到一个放置组并分布，放置组分布在整个集群中不同的ceph osd守护进程上。 精简配置的可快照Ceph块设备是虚拟化和云计算的一个有吸引力的选择。 在虚拟机场景中，人们通常在QEMU/KVM中部署带有rbd网络存储驱动程序的Ceph块设备，其中服务端使用librbd向客户端提供块设备服务。 许多云计算栈使用libvirt与管理程序集成。您可以通过QEMU和libvirt使用瘦配置的Ceph块设备来支持OpenStack和CloudStack以及其他解决方案。 2.Ceph文件系统 Ceph文件系统(cepphfs)提供了posix兼容的文件系统作为一种服务，它是在基于对象的Ceph存储集群之上分层的。 cepfs文件映射到Ceph存储集群中存储的对象。Ceph客户端将cepfs文件系统挂载为内核对象或用户空间中的文件系统(FUSE) Ceph文件系统服务包括部署在Ceph存储集群中的Ceph元数据服务器(MDS)。 MDS的目的是将所有文件系统元数据(目录、文件所有权、访问模式等)存储在高可用性Ceph元数据服务器中，元数据驻留在内存中。 MDS(称为Ceph - MDS的守护进程)存在的原因是，简单的文件系统操作，如列出目录或更改目录(ls、cd)，会给ceph osd守护进程带来不必要的负担。 因此，将元数据从数据中分离出来意味着Ceph文件系统可以提供高性能服务，而不会对Ceph存储集群造成负担。 cepfs将元数据与数据进行分离，元数据存储在MDS中，文件数据存储在Ceph存储集群中的一个或多个对象中。 Ceph文件系统旨在与POSIX兼容。为了实现高可用性或可伸缩性，ceph-mds可以作为单个进程运行，也可以将其分发到多个物理机器。 高可用：额外的ceph-mds实例可以是备用的，随时准备接管任何失效的active ceph-mds的职责。这很容易，因为包括日志在内的所有数据都存储在RADOS上。该转换由ceph-mon自动触发 可扩展：多个ceph mds实例可以处于活动状态，它们将目录树拆分为子树（以及单个繁忙目录的碎片），从而有效地平衡所有活动服务器之间的负载 3.Ceph 对象存储 Ceph对象存储守护进程radosgw是一个FastCGI服务，它提供了一个RESTful的HTTP API来存储对象和元数据。 它以自己的数据格式在Ceph存储集群之上分层，并维护自己的用户数据库、身份验证和访问控制。 RADOS网关采用统一的命名空间，既可以使用OpenStack swift接口，也可以使用Amazon s3接口。 例如，一个应用使用s3兼容的API写入数据，另一个应用使用swift兼容的API读取数据 S3/Swift对象和存储集群对象对比： Ceph的Object Storage使用Object这个术语来描述它存储的数据。 S3和Swift对象与Ceph写入Ceph存储集群的对象不同。 Ceph对象存储对象映射到Ceph存储集群对象。 S3和Swift对象不一定与存储集群中存储的对象以1:1的方式对应。 S3或Swift对象有可能映射到多个Ceph对象。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/06crush介绍.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/06crush介绍.html","title":"06crush介绍","keywords":"","body":"CRUSH算法介绍 Ceph客户端和ceph osd守护进程都使用CRUSH算法来有效地计算对象的位置信息，而不是依赖于一个中心查找表。 与以前的方法相比，CRUSH提供了更好的数据管理机制，可以实现大规模的数据管理。 CRUSH使用智能数据复制来确保弹性，这更适合于超大规模存储。 集群映射 Ceph依赖于Ceph客户端和ceph osd守护进程了解集群拓扑，其中包括5个映射，统称为“集群映射”： Monitor映射: 包含集群fsid、每个监视器的位置、名称、地址和端口、映射创建的时间，以及它最后一次修改时间。 要查看监视器映射，执行ceph mon dump。 [root@ceph01 ~]# ceph mon dump dumped monmap epoch 2 epoch 2 fsid b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 last_changed 2021-02-22 14:36:08.199609 created 2021-02-22 14:27:26.357269 min_mon_release 14 (nautilus) 0: [v2:192.168.1.69:3300/0,v1:192.168.1.69:6789/0] mon.ceph01 1: [v2:192.168.1.70:3300/0,v1:192.168.1.70:6789/0] mon.ceph02 2: [v2:192.168.1.71:3300/0,v1:192.168.1.71:6789/0] mon.ceph03 OSD映射:包含集群的fsid，映射创建和最后修改的时间，池的列表，副本大小，PG号，OSD的列表和状态(如up, in)。 执行ceph osd dump命令，查看OSD映射 [root@ceph01 ~]# ceph osd dump epoch 1 fsid b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 created 2021-02-22 14:27:48.482130 modified 2021-02-22 14:27:48.482130 flags sortbitwise,recovery_deletes,purged_snapdirs,pglog_hardlimit crush_version 1 full_ratio 0.95 backfillfull_ratio 0.9 nearfull_ratio 0.85 require_min_compat_client jewel min_compat_client jewel require_OSD_release nautilus max_OSD 0 PG Map：包含PG版本、它的时间戳、最后一个OSD Map epoch、完整比率，以及每个放置组的详细信息，例如PG ID、Up Set、Acting Set、PG的状态（例如active+clean），以及每个池的数据使用统计信息 CRUSH Map:包含一个存储设备列表，故障域层次结构(例如，设备、主机、机架、行、房间等)，以及存储数据时遍历层次结构的规则。执行ceph osd getcrushmap -o {filename};然后，通过执行crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename}来反编译它。 使用cat查看反编译后的映射。 MDS Map:包含当前MDS Map的epoch、Map创建的时间和最后一次修改的时间。它还包含存储元数据的池、元数据服务器的列表，以及哪些元数据服务器已经启动和运行. 每个映射维护其操作状态更改的迭代历史。Ceph监视器维护集群映射的主副本，包括集群成员、状态、变更和Ceph存储集群的总体运行状况 crush class 说明 从luminous版本ceph新增了一个功能crush class，这个功能又可以称为磁盘智能分组。因为这个功能就是根据磁盘类型自动的进行属性的关联，然后进行分类。无需手动修改crushmap，极大的减少了人为的操作。 ceph中的每个OSD设备都可以选择一个class类型与之关联，默认情况下，在创建OSD的时候会自动识别设备类型，然后设置该设备为相应的类。通常有三种class类型：hdd，ssd，nvme。 查看集群OSD crush tree(ceph01节点执行) [root@ceph01 ~]# ceph osd crush tree --show-shadow ID CLASS WEIGHT TYPE NAME -2 ssd 25.76233 root default~ssd -4 ssd 9.75183 host ceph01~ssd 0 ssd 0.87329 OSD.0 1 ssd 0.87329 OSD.1 2 ssd 0.87329 OSD.2 3 ssd 0.87329 OSD.3 4 ssd 0.87329 OSD.4 5 ssd 0.87329 OSD.5 6 ssd 0.87329 OSD.6 7 ssd 0.90970 OSD.7 8 ssd 0.90970 OSD.8 9 ssd 0.90970 OSD.9 10 ssd 0.90970 OSD.10 -6 ssd 8.00525 host ceph02~ssd 11 ssd 0.87329 OSD.11 12 ssd 0.87329 OSD.12 13 ssd 0.87329 OSD.13 14 ssd 0.87329 OSD.14 15 ssd 0.87329 OSD.15 16 ssd 0.90970 OSD.16 17 ssd 0.90970 OSD.17 18 ssd 0.90970 OSD.18 19 ssd 0.90970 OSD.19 -8 ssd 8.00525 host ceph03~ssd 20 ssd 0.87329 OSD.20 21 ssd 0.87329 OSD.21 22 ssd 0.87329 OSD.22 23 ssd 0.87329 OSD.23 24 ssd 0.87329 OSD.24 25 ssd 0.90970 OSD.25 26 ssd 0.90970 OSD.26 27 ssd 0.90970 OSD.27 28 ssd 0.90970 OSD.28 修改nvme类型class ceph osd crush rm-device-class 7 ceph osd crush set-device-class nvme OSD.7 ceph osd crush rm-device-class 8 ceph osd crush set-device-class nvme OSD.8 ceph osd crush rm-device-class 9 ceph osd crush set-device-class nvme OSD.9 ceph osd crush rm-device-class 10 ceph osd crush set-device-class nvme OSD.10 ceph osd crush rm-device-class 16 ceph osd crush set-device-class nvme OSD.16 ceph osd crush rm-device-class 17 ceph osd crush set-device-class nvme OSD.17 ceph osd crush rm-device-class 18 ceph osd crush set-device-class nvme OSD.18 ceph osd crush rm-device-class 19 ceph osd crush set-device-class nvme OSD.19 ceph osd crush rm-device-class 25 ceph osd crush set-device-class nvme OSD.25 ceph osd crush rm-device-class 26 ceph osd crush set-device-class nvme OSD.26 ceph osd crush rm-device-class 27 ceph osd crush set-device-class nvme OSD.27 ceph osd crush rm-device-class 28 ceph osd crush set-device-class nvme OSD.28 查看集群OSD crush tree [root@ceph01 ~]# ceph osd crush tree --show-shadow ID CLASS WEIGHT TYPE NAME -12 nvme 10.91638 root default~nvme -9 nvme 3.63879 host ceph01~nvme 7 nvme 0.90970 OSD.7 8 nvme 0.90970 OSD.8 9 nvme 0.90970 OSD.9 10 nvme 0.90970 OSD.10 -10 nvme 3.63879 host ceph02~nvme 16 nvme 0.90970 OSD.16 17 nvme 0.90970 OSD.17 18 nvme 0.90970 OSD.18 19 nvme 0.90970 OSD.19 -11 nvme 3.63879 host ceph03~nvme 25 nvme 0.90970 OSD.25 26 nvme 0.90970 OSD.26 27 nvme 0.90970 OSD.27 28 nvme 0.90970 OSD.28 -2 ssd 14.84595 root default~ssd -4 ssd 6.11304 host ceph01~ssd 0 ssd 0.87329 OSD.0 1 ssd 0.87329 OSD.1 2 ssd 0.87329 OSD.2 3 ssd 0.87329 OSD.3 4 ssd 0.87329 OSD.4 5 ssd 0.87329 OSD.5 6 ssd 0.87329 OSD.6 -6 ssd 4.36646 host ceph02~ssd 11 ssd 0.87329 OSD.11 12 ssd 0.87329 OSD.12 13 ssd 0.87329 OSD.13 14 ssd 0.87329 OSD.14 15 ssd 0.87329 OSD.15 -8 ssd 4.36646 host ceph03~ssd 20 ssd 0.87329 OSD.20 21 ssd 0.87329 OSD.21 22 ssd 0.87329 OSD.22 23 ssd 0.87329 OSD.23 24 ssd 0.87329 OSD.24 crush pool使用 创建crush rule # ceph osd crush rule create-replicated # 创建`class` 为`SSD`的rule ceph osd crush rule create-replicated SSD_rule default host ssd 创建资源池 #ceph osd pool create {pool_name} {pg_num} [{pgp_num}] ceph osd pool create ssd-pool 256 256 查看ssd-poolcrush rule [root@ceph01 ~]# ceph osd pool get ssd-pool crush_rule crush_rule: replicated_rule 设置ssd-poolcrush rule为SSD_rule #ceph osd pool set crush_rule ceph osd pool set ssd-pool crush_rule SSD_rule 查看ssd-poolcrush rule [root@ceph01 ~]# ceph osd pool get ssd-pool crush_rule crush_rule: SSD_rule 设置ssd-pool配额 #osd pool set-quota max_objects|max_bytes 设置最大对象数10 ceph osd pool set-quota ssd-pool max_objects 10 设置存储大小为1G ceph osd pool set-quota ssd-pool max_bytes 1G 扩容ssd-pool配额 #osd pool set-quota max_objects|max_bytes 设置最大对象数20 ceph osd pool set-quota ssd-pool max_objects 20 设置存储大小为2G ceph osd pool set-quota ssd-pool max_bytes 2G 修改ssd-pool为ssd-demo-pool ceph osd pool rename ssd-pool ssd-demo-pool 最佳实践：创建不同类型存储池（基于HDD、SSD，提供不同场景使用） Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/07ceph协议.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/07ceph协议.html","title":"07ceph协议","keywords":"","body":"Ceph协议 Ceph客户端使用原生协议与Ceph存储集群进行交互，Ceph将这个功能打包到librados库中。 下图描述了基本架构： Ceph协议与librados 现代应用程序需要一个具有异步通信功能的简单对象存储接口。 Ceph存储集群提供了一个具有异步通信能力的简单对象存储接口。 该接口提供了对整个集群中的对象的直接、并行访问。如： 池操作 快照和写时复制克隆 读/写对象-创建或删除-整个对象或字节范围-追加或截断 创建/设置/获取/删除XATTRs 创建/设置/获取/删除键值对 复合运算与双ack语义 对象类 对象观测通知 客户端可以持久性观测一个对象，并保持与主OSD的会话处于打开状态。 客户端可以向所有观察者发送通知消息和有效负载，并在观察者收到通知时接收通知。 这使客户端能够将任何对象用作同步/通信通道 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/08数据分段.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/08数据分段.html","title":"08数据分段","keywords":"","body":"数据分段 存储设备有吞吐量限制，这会影响性能和可伸缩性。因此，存储系统通常支持跨多个存储设备分段存储顺序信息，以提高吞吐量和性能。 RAID是最常见的数据分条形式。与Ceph的条带化最相似的RAID类型是raid0，或“条带卷”。Ceph的分条提供了RAID 0分条的吞吐量，n-way RAID镜像的可靠性和更快的恢复。 Ceph存储集群中Ceph存储的对象没有条带化。Ceph对象存储、Ceph块设备和Ceph文件系统在多个Ceph存储集群对象上条带化它们的数据。 通过librados直接写入Ceph存储集群的Ceph客户端必须为自己执行条带化（和并行I/O）以获得这些好处。 最简单的Ceph条带格式涉及1个对象的条带计数。Ceph客户端将条带单元写入Ceph存储群集对象，直到该对象达到最大容量，然后为额外的数据条带创建另一个对象。 最简单的条带化形式对于小块设备图像、S3或Swift对象和cepfs文件就足够了。 然而，这种简单的形式并没有最大限度地利用Ceph在放置组之间分发数据的能力，因此并没有极大地提高性能。下图描述了条带化的最简单形式： 如果预期会有较大的镜像、较大的S3或Swift对象（例如，视频）或较大的cepfs目录，那么通过在对象集中的多个对象上对客户端数据进行条带化，会有相当大的读/写性能改进。 当客户端并行地将条带单元写入相应的对象时，会显著提升写入性能。由于对象被映射到不同的放置组并进一步映射到不同的OSD，因此每次写入都以最大的写入速度并行进行。 对单个磁盘的写入将受到磁头移动（例如，每次寻道6ms）和该设备带宽（例如，100MB/s）的限制。 通过将写操作扩展到多个对象（映射到不同的放置组和OSD），Ceph可以减少每个驱动器的寻道次数，并结合多个驱动器的吞吐量，以实现更快的写（或读）速度。 在下面的图中，客户端数据在由4个对象组成的对象集(下图中的对象集1)上进行分条， 其中第一个分条单元是对象0中的分条单元0，第四个分条单元是对象3中的分条单元3。 写入第四个分条后，客户端确定对象集是否已满。 如果对象集未满，客户端将开始再次向第一个对象(下图中的对象0)写入条带。 如果对象集已满，客户端将创建一个新的对象集(下图中的对象集2)， 并开始写入新对象集(下图中的对象4)中的第一个对象中的第一个条带(条带单元16)。 决定Ceph条带化数据的方式的三个重要变量: 对象大小：Ceph存储集群中的对象具有最大可配置大小（例如，2MB、4MB等）。对象大小应该足够大以容纳多个条带单位，并且应该是条带单位的倍数 条带宽度：条带具有可配置的单元大小（例如64kb）。Ceph客户端将要写入对象的数据划分为大小相等的条带单元，但最后一个条带单元除外。条带宽度应该是对象大小的一小部分，这样一个对象可以包含许多条纹单位。 条带计数：Ceph客户端在由条带计数确定的一系列对象上写入条带单元序列。这一系列对象被称为对象集。Ceph客户端写入对象集中的最后一个对象后，它返回到对象集中的第一个对象 一旦Ceph客户端将数据分条到条带单元并将条带单元映射到对象，Ceph的CRUSH算法将对象映射到放置组，并将放置组映射到ceph osd守护进程，然后将对象作为文件存储在存储磁盘上 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/09集群动态管理.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/09集群动态管理.html","title":"09集群动态管理","keywords":"","body":"动态集群管理 每个池（pool）都有许多放置组（PG），CRUSH动态地将PGs映射到OSDs。当Ceph客户端存储对象时，CRUSH将每个对象映射到放置组 将对象映射到放置组将在ceph osd守护进程和Ceph客户端之间创建一个间接层。 Ceph存储集群必须能够增长(或收缩)，并重新平衡动态存储对象的位置。 如果Ceph客户端“知道”哪个ceph osd守护进程有哪个对象，那么Ceph客户端和ceph osd守护进程之间就会产生一个紧密耦合。 相反，CRUSH算法将每个对象映射到一个放置组，然后将每个放置组映射到一个或多个ceph osd守护进程。 这一间接层允许Ceph在新的ceph osd守护进程和底层OSD设备上线时动态重新平衡。下图描述了CRUSH如何将对象映射到放置组，以及将放置组映射到OSD。 有了集群映射的副本和CRUSH算法，客户端就可以准确地计算出在读写特定对象时应该使用哪个OSD。 计算放置组ID 当Ceph客户端绑定到Ceph mon时，它将检索集群映射的最新副本。通过集群映射，客户获取集群中的所有mon、OSD和mds信息。但是，它对对象位置一无所知。 计算过程 。这很简单:Ceph将数据存储在命名池中(例如，“liverpool”)。 当客户端想要存储一个命名对象(例如，“john”、“paul”、“george”、“ringo”等)时，它使用对象名、哈希码、池中的PGs数量和池名计算放置组。Ceph客户端使用以下步骤计算PG id。 1、客户端输入对象ID和pool 2、Ceph获取对象ID并对其进行哈希运算 3、Ceph计算pg数的哈希模,（例如，58）获取PG ID 4、Ceph获取给定池名的池ID（例如，“liverpool”=4） 5、Ceph将池ID前置到PG ID(例如，4.58)。 重新平衡 当你将ceph osd守护进程添加到Ceph存储集群时，集群映射会随着新的OSD更新。再次计算PG id，这将更改集群映射。 下面的图描述了重新平衡的过程(虽然很粗略，因为在大型集群中影响更小)， 其中一些(但不是所有)pg从现有OSD (OSD 1和OSD 2)迁移到新的OSD (OSD 3)。即使在重新平衡时，崩溃也是稳定的。 许多放置组保持原来的配置，每个OSD增加了一些容量，因此在重新平衡完成后，新OSD上不会出现负载峰值。 数据一致性 作为维护数据一致性和清洁度的一部分，ceph osd还可以清理放置组中的对象。 也就是说，ceph osd可以将一个放置组中的对象元数据与其存储在其他OSD中的放置组中的副本进行比较。 清理（通常每天执行）捕获OSD错误或文件系统错误。 OSD还可以通过逐位比较对象中的数据来执行更深入的清理。深度清理（通常每周执行一次）会在磁盘上发现在轻度清理时不明显的坏扇区。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/10对比raid.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/10对比raid.html","title":"10对比raid","keywords":"","body":"对比raid RAID(Redundant Array of Independent Disks)即独立冗余磁盘阵列，是一种把多块独立的硬盘（物理硬盘）按不同的方式组合起来形成一个硬盘组（逻辑硬盘），让用户认为只有一个单个超大硬盘，从而提供比单个硬盘更高的存储性能和提供数据备份技术 RAID 漫长的重建过程，而且在重建过程中，不能有第二块盘损坏，否则会引发更大的问题； 备用盘增加TCO ，作为备用盘，当没有硬盘故障时，就会一直闲置的 不能保证两块盘同时故障后，数据的可靠性 在重建结束前，客户端无法获取到足够的IO资源 无法避免网络、服务器硬件、操作系统、电源等故障 Ceph 为了保证可靠性，采用了数据复制的方式，这意味着不再需要RAID，也就克服了RAID存在的诸多问题 Ceph 数据存储原则：一个Pool 有若干PG，每个PG 包含若干对象，一个对象只能存储在一个PG中，而Ceph 默认一个PG 包含三个OSD，每个OSD都可看做一块硬盘。 因此，一个对象存储在Ceph中时，就被保存了三份。当一个磁盘故障时，还剩下2个PG，系统就会从另外两个PG中复制数据到其他磁盘上。这个是由crush算法决定 磁盘复制属性值可以通过管理员进行调整 磁盘存储上使用了加权机制，所以磁盘大小不一致也不会出现问题 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/01核心概念/11对比SAN、NAS、DAS.html":{"url":"1.Linux基础/1.7存储/ceph/01核心概念/11对比SAN、NAS、DAS.html","title":"11对比SAN、NAS、DAS","keywords":"","body":"对比SAN、NAS、DAS DAS Direct Attached Storage，即直连附加存储，第一代存储系统，通过SCSI总线扩展至一个外部的存储，磁带整列，作为服务器扩展的一部分 NAS Network Attached Storage，即网络附加存储，通过网络协议如NFS远程获取后端文件服务器共享的存储空间，将文件存储单独分离出来 SAN Storage Area Network，即存储区域网络，分为IP-SAN和FC-SAN，即通过TCP/IP协议和FC(Fiber Channel)光纤协议连接到存储服务器 Ceph Ceph在一个统一的存储系统中同时提供了对象存储、块存储和文件存储，即Ceph是一个统一存储，能够将企业企业中的三种存储需求统一汇总到一个存储系统中，并提供分布式、横向扩展，高度可靠性的存储系统 主要区别如下： DAS直连存储服务器使用SCSI或FC协议连接到存储阵列、通过SCSI总线和FC光纤协议类型进行数据传输； 例如一块有空间大小的裸磁盘：/dev/sdb。DAS存储虽然组网简单、成本低廉但是可扩展性有限、无法多主机实现共享、目前已经很少使用 NAS网络存储服务器使用TCP网络协议连接至文件共享存储、常见的有NFS、CIFS协议等；通过网络的方式映射存储中的一个目录到目标主机，如/data。 NAS网络存储使用简单，通过IP协议实现互相访问，多台主机可以同时共享同一个存储。但是NAS网络存储的性能有限，可靠性不是很高。 SAN存储区域网络服务器使用一个存储区域网络IP或FC连接到存储阵列、常见的SAN协议类型有IP-SAN和FC-SAN。SAN存储区域网络的性能非常好、可扩展性强；但是成本特别高、尤其是FC存储网络：因为需要用到HBA卡、FC交换机和支持FC接口的存储 存储结构/性能对比 DAS NAS FC-SAN IP-SAN Ceph 成本 低 较低 高 较高 高 数据传输速度 快 慢 极快 较快 快 扩展性 无扩展性 较低 易于扩展 最易扩展 易于扩展 服务器访问存储方式 块 文件 块 块 对象、文件、块 服务器系统性能开销 低 较低 低 较高 低 安全性 高 低 高 低 高 是否集中管理存储 否 是 是 是 否 备份效率 低 较低 高 较高 高 网络传输协议 无 TCP/IP FC TCP/IP 私有协议(TCP) Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/02集成部署/01硬件需求.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/01硬件需求.html","title":"01硬件需求","keywords":"","body":"硬件需求 Ceph被设计成在普通硬件上运行，这使得构建和维护pb级数据集群在经济上可行,在规划集群硬件时，需要平衡许多考虑因素，包括故障域和潜在的性能问题。 硬件规划应该包括在多个主机上分布Ceph守护进程和其他使用Ceph的进程。官方建议在为特定类型的守护进程配置的主机上运行特定类型的Ceph守护进程。 即ceph集群与客户端应为不同宿主机，具体硬件需求参考如下： CPU Ceph元数据服务器动态地重新分配它们的负载，这是CPU密集型的。因此，元数据服务器应该具有强大的处理能力(例如，四核或更好的cpu)。ceph osds运行RADOS服务，使用CRUSH计算数据位置，复制数据，并维护它们自己的集群映射副本。 因此，OSD应该具有合理的处理能力(例如，双核处理器)。监视器只是维护集群映射的主副本，因此它们不是CPU密集型的。 您还必须考虑主机除了运行Ceph守护进程外，是否还将运行cpu密集型进程。 例如，如果您的主机将运行计算虚拟机(例如OpenStack Nova)，您将需要确保这些其他进程为Ceph守护进程留出足够的处理能力。 建议在不同的主机上运行额外的cpu密集型进程 样例集群CPU配置： Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 72 On-line CPU(s) list: 0-71 Thread(s) per core: 2 Core(s) per socket: 18 Socket(s): 2 NUMA node(s): 2 Vendor ID: GenuineIntel CPU family: 6 Model: 85 Model name: Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz Stepping: 7 CPU MHz: 999.914 CPU max MHz: 3900.0000 CPU min MHz: 1000.0000 BogoMIPS: 5200.00 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 1024K L3 cache: 25344K NUMA node0 CPU(s): 0-17,36-53 NUMA node1 CPU(s): 18-35,54-71 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 invpcid_single intel_ppin intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear spec_ctrl intel_stibp flush_l1d arch_capabilities 内存 内存越多越好 CEPH-MON&CEPH-MGR（监控、管理节点）: 监视器和管理器守护进程的内存使用情况通常随集群的大小而变化。对于小型集群， 一般1-2GB就足够了。对于大型集群，您应该提供更多(5-10GB)。 您可能还需要考虑调整mon_OSD_cache_size或rocksdb_cache_size等设置 CEPH-MDS元数据节点 : 元数据守护进程的内存利用率取决于它的缓存被配置为消耗多少内存。对于大多数系统，官方建议至少使用1GB。具体大小可调整mds_cache_memory OSDS(CEPH-OSD)存储节点：默认情况下，使用BlueStore后端的OSD需要3-5GB RAM。当使用BlueStore时， 可以通过配置选项OSD_memory_target来调整OSD的内存消耗。当使用遗留的FileStore后端时， 操作系统页面缓存用于缓存数据，所以通常不需要进行调优，并且OSD的内存消耗通常与系统中每个守护进程的PGs数量有关。 样例集群内存配置： total used free shared buff/cache available Mem: 187G 8.8G 164G 4.0G 13G 173G Swap: 0B 0B 0B 存储 请仔细规划您的数据存储配置。在规划数据存储时，需要考虑大量的成本和性能折衷。 同时进行OS操作，以及多个守护进程对单个驱动器同时进行读和写操作的请求会显著降低性能。 注意： 因为Ceph在发送ACK之前必须把所有的数据都写到日志中(至少对于XFS来说是这样)，让日志和OSD的性能平衡是非常重要的! 硬盘驱动器 OSD应该有足够的硬盘驱动器空间来存放对象数据。官方建议硬盘驱动器的最小大小为1TB。 考虑较大磁盘的每GB成本优势。官方建议将硬盘驱动器的价格除以千兆字节数，得出每千兆字节的成本，因为较大的驱动器可能对每千兆字节的成本有很大的影响。 例如，价格为$75.00的1TB硬盘的成本为每GB$0.07(即$75 / 1024 = 0.0732)。 相比之下，价格为150美元的3TB硬盘的成本为每GB 0.05美元(即150美元/ 3072 = 0.0488)。 在前面的示例中，使用1TB的磁盘通常会使每GB的成本增加40%——从而大大降低集群的成本效率。 此外，存储驱动器容量越大，每个ceph osd守护进程需要的内存就越多，尤其是在重新平衡、回填和恢复期间。 一般的经验法则是1TB的存储空间需要1GB的RAM 存储驱动器受到寻道时间、访问时间、读和写时间以及总吞吐量的限制。 这些物理限制会影响整个系统的性能——尤其是在恢复过程中。 官方建议为操作系统和软件使用专用的驱动器，为主机上运行的每个ceph osd守护进程使用一个驱动器(物理硬盘)。 大多数慢OSD问题是由于在同一个驱动器上运行一个操作系统、多个OSD和/或多个日志引起的。 由于在小型集群上故障排除性能问题的成本可能会超过额外磁盘驱动器的成本，因此可以通过避免过度使用OSD存储驱动器来加速集群设计规划 您可以在每个硬盘驱动器上运行多个ceph osd进程，但这可能会导致资源争用，并降低总体吞吐量。 您可以将日志和对象数据存储在同一个驱动器上，但这可能会增加向客户端记录写操作和ACK所需的时间。 Ceph必须先向日志写入数据，然后才能对写入数据进行验证 总结为：Ceph最佳实践规定，您应该在不同的驱动器上运行操作系统、OSD数据和OSD日志 固态硬盘 提高性能的一个机会是使用固态驱动器(SSD)来减少随机访问时间和读取延迟，同时加速吞吐量。 与硬盘驱动器相比，SSD每GB的成本通常超过10倍，但SSD的访问时间通常至少比硬盘驱动器快100倍 SSD没有可移动的机械部件，因此它们不必受到与硬盘驱动器相同类型的限制。不过SSD确实有很大的局限性。 在评估SSD时，考虑顺序读写的性能是很重要的。 当存储多个OSD的多个日志时，顺序写吞吐量为400MB/s的SSD可能比顺序写吞吐量为120MB/s的SSD性能更好 由于SSD没有可移动的机械部件，所以在Ceph中不需要大量存储空间的区域使用SSD是有意义的。 相对便宜的固态硬盘可能会吸引你的经济意识。谨慎使用。 当选择与Ceph一起使用的SSD时，可接受的IOPS是不够的。对于日志和SSD有几个重要的性能考虑因素: 写密集型语义:日志记录涉及写密集型语义，因此您应该确保选择部署的SSD在写入数据时的性能等于或优于硬盘驱动器。 廉价的SSD可能会在加速访问时间的同时引入写延迟，因为有时高性能硬盘驱动器的写速度可以与市场上一些更经济的SSD一样快甚至更快! 顺序写:当您在一个SSD上存储多个日志时，您还必须考虑SSD的顺序写限制，因为它们可能会同时处理多个OSD日志的写请求。 分区对齐:SSD性能的一个常见问题是，人们喜欢将驱动器分区作为最佳实践，但他们常常忽略了使用SSD进行正确的分区对齐，这可能导致SSD传输数据的速度慢得多。确保SSD分区对齐 虽然SSD存储对象的成本非常高，但是通过将OSD的日志存储在SSD上，将OSD的对象数据存储在单独的硬盘驱动器上，可以显著提高OSD的性能。 OSD日志配置默认为/var/lib/ceph/OSD/$cluster-$id/journal。您可以将此路径挂载到SSD或SSD分区上，使其与对象数据不只是同一个磁盘上的文件 Ceph加速CephFS文件系统性能的一种方法是将CephFS元数据的存储与CephFS文件内容的存储隔离。 Ceph为cepfs元数据提供了一个默认的元数据池。您永远不必为CephFS元数据创建一个池，但是您可以为仅指向主机的SSD存储介质的CephFS元数据池创建一个CRUSH map层次结构。 重要提示: 官方建议探索SSD的使用以提高性能。但是，在对SSD进行重大投资之前，官方强烈建议检查SSD的性能指标，并在测试配置中测试SSD，以评估性能 控制器 磁盘控制器对写吞吐量也有很大的影响。仔细考虑磁盘控制器的选择，以确保它们不会造成性能瓶颈。 注意事项 你可以在每个主机上运行多个OSD，但是你应该确保OSD硬盘的总吞吐量不超过客户端读写数据所需的网络带宽。 您还应该考虑集群在每个主机上存储的总体数据的百分比。如果某个主机上的百分比很大，并且该主机发生故障，那么它可能会导致一些问题，比如超过了完整的比例，这将导致Ceph停止操作，作为防止数据丢失的安全预防措施。 当您在每个主机上运行多个OSD时，还需要确保内核是最新的。请参阅OS推荐，了解glibc和syncfs(2)方面的注意事项，以确保在每个主机上运行多个OSD时，您的硬件能够像预期的那样执行 拥有大量OSD的主机(例如> 20)可能会产生大量线程，特别是在恢复和平衡过程中。许多Linux内核默认的最大线程数相对较小(例如，32k)。如果在拥有大量OSD的主机上启动OSD时遇到问题，请考虑设置kernel。将pid_max设置为更高的线程数。理论最大值是4,194,303线程。 例如，您可以将以下内容添加到/etc/sysctl.conf文件中: kernel.pid_max = 4194303 样例集群存储配置： Disk /dev/nvme0n1: 1000.2 GB, 1000204886016 bytes, 1953525168 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/nvme1n1: 1000.2 GB, 1000204886016 bytes, 1953525168 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/nvme2n1: 1000.2 GB, 1000204886016 bytes, 1953525168 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/nvme3n1: 1000.2 GB, 1000204886016 bytes, 1953525168 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/sdb: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sda: 480.1 GB, 480103981056 bytes, 937703088 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk label type: gpt Disk identifier: EDC26861-DACE-4831-848D-4FA0C5F642D7 # Start End Size Type Name 1 2048 2099199 1G EFI System EFI System Partition 2 2099200 4196351 1G Microsoft basic 3 4196352 937701375 445.1G Linux LVM Disk /dev/sde: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/sdd: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/sdg: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/sdf: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sdh: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk label type: gpt Disk identifier: 5F151167-14E6-4826-BBEB-55280AC27EEC # Start End Size Type Name 1 10487808 1875384974 889.3G ceph osd ceph data 2 2048 10487807 5G Ceph Journal ceph journal Disk /dev/sdc: 960.2 GB, 960197124096 bytes, 1875385008 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk /dev/mapper/centos-root: 478.0 GB, 477953523712 bytes, 933502976 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes 网络 官方说明如下: 每个主机至少有两个1Gbps的网络接口控制器(nic)。由于大多数普通硬盘驱动器的吞吐量大约为100MB/秒，您的网卡应该能够处理主机上OSD磁盘的流量 建议至少使用两个网卡来考虑公共(前端)网络和集群(后端)网络。集群网络(最好不连接到外部网络)处理数据复制的额外负载 通过1Gbps的网络复制1TB的数据需要3个小时，3TB(典型的驱动器配置)需要9个小时。相比之下，在10Gbps的网络中，复制时间将分别为20分钟和1小时。 在pb级集群中，OSD磁盘故障应该是一种预期，而不是异常。系统管理员希望PGs尽可能快地从降级状态恢复到active + clean状态，同时考虑到价格/性能权衡。 此外，一些部署工具(如戴尔的Crowbar)可以部署5个不同的网络，但使用vlan使硬件和网络电缆更易于管理。使用802.1q协议的vlan需要支持vlan的网卡和交换机。增加的硬件费用可能会被网络设置和维护的操作成本节省所抵消 当使用vlan处理集群与计算栈(如OpenStack、CloudStack等)之间的虚拟机流量时，也值得考虑使用10G以太网。 每个网络的机架顶部路由器也需要能够与具有更快吞吐量的脊柱路由器进行通信。40Gbps到100Gbps 您的服务器硬件应该有一个底板管理控制器(BMC)。管理和部署工具也可能广泛地使用bmc， 因此考虑使用带外网络进行管理的成本/收益权衡。管理程序SSH访问、VM镜像上传、操作系统镜像安装、管理套接字等都可能给网络带来巨大的负载。 运行三个网络可能看起来有点小题大做，但每个流量路径都代表一个潜在的容量、吞吐量和/或性能瓶颈，在部署大规模数据集群之前，您应该仔细考虑这些问题 简言之： 建议三个以上网络接口： ceph集群网络接口 对外网络接口 管理接口 样例集群网口配置： [root@ceph01 ~]# ethtool ens4f0 Settings for ens4f0: Supported ports: [ FIBRE ] Supported link modes: 10000baseT/Full Supported pause frame use: Symmetric Supports auto-negotiation: No Supported FEC modes: Not reported Advertised link modes: 10000baseT/Full Advertised pause frame use: Symmetric Advertised auto-negotiation: No Advertised FEC modes: Not reported Speed: 10000Mb/s Duplex: Full Port: FIBRE PHYAD: 0 Transceiver: internal Auto-negotiation: off Supports Wake-on: d Wake-on: d Current message level: 0x00000007 (7) drv probe link Link detected: yes Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/02集成部署/02硬件配置建议.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/02硬件配置建议.html","title":"02硬件配置建议","keywords":"","body":"故障域是指阻止访问一个或多个OSDs的任何故障。这可能是主机上已停止的守护进程;硬盘故障、操作系统崩溃、网卡故障、电源故障、网络中断、电源中断，等等。 在规划硬件需求时，您必须平衡降低成本的诱惑，即把太多的责任放在太少的故障域中，以及隔离每个潜在故障域所增加的成本 硬件配置建议 最小配置建议 Process Criteria Minimum Recommended ceph-OSD Processor 1x 64-bit AMD-64 1x 32-bit ARM dual-core or better RAM ~1GB for 1TB of storage per daemon Volume Storage 1x storage drive per daemon Journal 1x SSD partition per daemon (optional) Network 2x 1GB Ethernet NICs ceph-mon Processor 1x 64-bit AMD-64 1x 32-bit ARM dual-core or better RAM 1 GB per daemon Disk Space 10 GB per daemon Network 2x 1GB Ethernet NICs ceph-mds Processor 1x 64-bit AMD-64 quad-core 1x 32-bit ARM quad-core RAM 1 GB minimum per daemon Disk Space 1 MB per daemon Network 2x 1GB Ethernet NICs 生产环境建议 Configuration Criteria Minimum Recommended Dell PE R510 Processor 2x 64-bit quad-core Xeon CPUs RAM 16 GB Volume Storage 8x 2TB drives. 1 OS, 7 Storage Client Network 2x 1GB Ethernet NICs OSD Network 2x 1GB Ethernet NICs Mgmt. Network 2x 1GB Ethernet NICs Dell PE R515 Processor 1x hex-core Opteron CPU RAM 16 GB Volume Storage 12x 3TB drives. Storage OS Storage 1x 500GB drive. Operating System. Client Network 2x 1GB Ethernet NICs OSD Network 2x 1GB Ethernet NICs Mgmt. Network 2x 1GB Ethernet NICs Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/02集成部署/03操作系统建议.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/03操作系统建议.html","title":"03操作系统建议","keywords":"","body":"操作系统建议 内核 Ceph客户端内核: 4.14.x 4.9.x 平台 Distro Release Code Name Kernel Notes Testing CentOS 7 N/A linux-3.10.0 3 B, I, C Debian 8.0 Jessie linux-3.16.0 1, 2 B, I Debian 9.0 Stretch linux-4.9 1, 2 B, I Fedora 22 N/A linux-3.14.0 B, I RHEL 7 Maipo linux-3.10.0 B, I Ubuntu 14.04 Trusty Tahr linux-3.13.0 B, I, C Ubuntu 16.04 Xenial Xerus linux-4.4.0 3 B, I, C Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/02集成部署/04环境初始化.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/04环境初始化.html","title":"04环境初始化","keywords":"","body":"环境说明 节点信息 节点名称 节点IP 节点属性 ceph01 192.168.1.69 admin,deploy,mon ceph02 192.168.1.70 单元格 ceph03 192.168.1.70 单元格 环境初始化 配置yum 所有ceph节点，包含客户端节点 1、删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 2、创建yum源文件 online curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 下载以下文件上传至/etc/yum.repos.d/ Centos-7.repo epel-7.repo 离线环境参考ceph本地源 搭建本地源 3、配置ceph镜像源仓库 cat > /etc/yum.repos.d/ceph.repo 4、配置yum代理 适用于主机通过代理访问互联网场景 以下变量注意替换 username: 代理用户名 password: 代理用户密码 proxy_host: 代理IP地址 proxy_port: 代理端口 echo \"proxy=http://username:password@proxy_host:proxy_port\" >> /etc/yum.conf 配置时钟同步 1.配置dns 该dns用以解析时钟服务地址，互联网下应为114.114.114.114 echo \"nameserver x.x.x.x\" >> /etc/resolv.conf 2.安装ntp yum install -y ntp 3.同步 时钟服务地址据实际情况调整 ntpdate time.wl.com echo \"*/5 * * * * root ntpdate time.wl.com\" >> /etc/crontab 4.调整时区 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 升级内核 1.导入kernel源 elrepo-release-7.0-4.el7.elrepo.noarch.rpm rpm -ivh elrepo-release-7.0-4.el7.elrepo.noarch.rpm -y 2.安装最新主线版 yum -y --enablerepo=elrepo-kernel install kernel-ml.x86_64 kernel-ml-devel.x86_64 3.删除旧版本工具包 rpm -qa|grep kernel-3|xargs -n1 yum remove -y 4.安装新版本工具包 yum --disablerepo=\\* --enablerepo=elrepo-kernel install -y kernel-ml-tools.x86_64 5.查看内核列表 awk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg 6.重建内核 grub2-mkconfig -o /boot/grub2/grub.cfg 7.配置新版内核 grub2-set-default 0 8.重启 reboot -f Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/02集成部署/05安装ceph核心组件.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/05安装ceph核心组件.html","title":"05安装ceph核心组件","keywords":"","body":"安装ceph组件 ceph版本为14.2.16 nautilus 1.创建ceph目录(deploy节点执行) mkdir -p /etc/ceph 2.配置主机互信(deploy节点执行) ssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa ssh-copy-id ceph01 ssh-copy-id ceph02 ssh-copy-id ceph03 3.安装ceph(所有节点执行) yum install -y ceph ceph-deploy 4.初始化mon节点(deploy节点执行) ceph-deploy new ceph01 ceph02 ceph03 5.初始化mon(deploy节点执行) ceph-deploy mon create-initial 6.修改集群文件(deploy节点执行) cd /etc/ceph/ echo \"public_network=192.168.1.0/24\" >> /etc/ceph/ceph.conf ceph-deploy --overwrite-conf config push ceph01 ceph02 ceph03 7.配置admin节点 cd /etc/ceph/ ceph-deploy admin ceph01 ceph02 ceph03 8.查看集群状态 [root@ceph01 ~]# ceph -s cluster: id: b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 health: HEALTH_WARN mon ceph03 is low on available space services: mon: 3 daemons, quorum ceph01,ceph02,ceph03 (age 31m) mgr: no daemons active OSD: 0 OSDs: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: 9.安装命令补全 yum -y install bash-completion Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/02集成部署/06添加硬盘至集群内.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/06添加硬盘至集群内.html","title":"06添加硬盘至集群内","keywords":"","body":"创建osd 所有存储节点执行相同操作 1.列出节点磁盘信息 ceph-deploy disk list ceph01 ceph-deploy disk list ceph02 ceph-deploy disk list ceph03 输出如下 ... [ceph01][INFO ] Disk /dev/sda: 480.1 GB, 480103981056 bytes, 937703088 sectors [ceph01][INFO ] Disk /dev/sdb: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sdf: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sdd: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sde: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sdc: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sdg: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph01][INFO ] Disk /dev/sdh: 960.2 GB, 960197124096 bytes, 1875385008 sectors ... ... [ceph02][INFO ] Disk /dev/sda: 480.1 GB, 480103981056 bytes, 937703088 sectors [ceph02][INFO ] Disk /dev/sdb: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sdf: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sdd: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sde: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sdc: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sdg: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph02][INFO ] Disk /dev/sdh: 960.2 GB, 960197124096 bytes, 1875385008 sectors ... ... [ceph03][INFO ] Disk /dev/sda: 480.1 GB, 480103981056 bytes, 937703088 sectors [ceph03][INFO ] Disk /dev/sdb: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sdf: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sdd: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sde: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sdc: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sdg: 960.2 GB, 960197124096 bytes, 1875385008 sectors [ceph03][INFO ] Disk /dev/sdh: 960.2 GB, 960197124096 bytes, 1875385008 sectors ... 其中/dev/sda-h为SSD类型磁盘，且/dev/sda为系统盘 2.查看磁盘挂载 lsblk 输出如下 NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:16 0 894.3G 0 disk ├─sda2 8:34 0 1G 0 part /boot ├─sda3 8:35 0 445.1G 0 part │ ├─centos-swap 253:1 0 16G 0 lvm │ └─centos-root 253:0 0 429.1G 0 lvm / └─sda1 8:33 0 1G 0 part /boot/efi sr0 11:0 1 4.2G 0 rom sdb 8:0 0 894.3G 0 disk sdc 8:32 0 894.3G 0 disk sdd 8:0 0 894.3G 0 disk sde 8:64 0 894.3G 0 disk sdf 8:64 0 894.3G 0 disk sdg 8:96 0 894.3G 0 disk sdh 8:112 0 894.3G 0 disk .... 3.格式化磁盘 mkfs.ext4 /dev/sdb mkfs.ext4 /dev/sdc mkfs.ext4 /dev/sdd mkfs.ext4 /dev/sde mkfs.ext4 /dev/sdf mkfs.ext4 /dev/sdg mkfs.ext4 /dev/sdh 4.擦净节点磁盘 cd /etc/ceph/ for i in {b..h};do ceph-deploy disk zap ceph01 /dev/sd$i done cd /etc/ceph/ for i in {b..h};do ceph-deploy disk zap ceph02 /dev/sd$i done cd /etc/ceph/ for i in {b..h};do ceph-deploy disk zap ceph03 /dev/sd$i done 5.创建osd节点 cd /etc/ceph for i in {b..h};do ceph-deploy osd create --data /dev/sd$i ceph01 done cd /etc/ceph for i in {b..h};do ceph-deploy osd create --data /dev/sd$i ceph02 done cd /etc/ceph for i in {b..h};do ceph-deploy osd create --data /dev/sd$i ceph03 done Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/02集成部署/07安装dashboard.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/07安装dashboard.html","title":"07安装dashboard","keywords":"","body":"安装dashboard rpm 1.安装 ceph-deploy mgr create ceph01 2.启用Dashboard ceph mgr module enable dashboard 3.启用必要模块 ceph mgr module enable pg_autoscaler 4.用户、密码、权限 # 创建用户 #ceph dashboard ac-user-create administrator ceph dashboard ac-user-create admin Ceph-12345 administrator 5.创建自签证书 ceph dashboard create-self-signed-cert 6.查看Dashboard地址 [root@ceph01 ~]# ceph mgr services { \"dashboard\": \"https://ceph01:8443/\" } 7.登录访问 8.修改端口(可选) 确认配置 [root@ceph01 ~]# ceph config-key ls [ \"config-history/1/\", \"config-history/2/\", \"config-history/2/+global/osd_pool_default_pg_autoscale_mode\", \"config/global/osd_pool_default_pg_autoscale_mode\", \"mgr/dashboard/accessdb_v1\", \"mgr/dashboard/crt\", \"mgr/dashboard/jwt_secret\", \"mgr/dashboard/key\" ] 修改端口 ceph config set mgr mgr/dashboard/ssl_server_port 7000 使变更的配置生效 ceph mgr module disable dashboard ceph mgr module enable dashboard 9.配置访问前缀(可选) ceph config set mgr mgr/dashboard/url_prefix /ceph-ui 重启mgr systemctl restart ceph-mgr@ceph01 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/02集成部署/111ceph本地源搭建.html":{"url":"1.Linux基础/1.7存储/ceph/02集成部署/111ceph本地源搭建.html","title":"111ceph本地源搭建","keywords":"","body":"ceph本地源 适用无法直连或通过代理连接互联网镜像源 联网主机：用于导出ceph依赖,操作系统为CentOS7 离线主机：实际部署ceph应用的主机(多节点实例) 依赖导出（联网主机） rm -f /etc/yum.repos.d/*.repo curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo yum update -y yum install yum-plugin-downloadonly -y yum install --downloadonly --downloaddir=./ceph ceph ceph-common ceph-deploy 生成repo依赖关系（联网主机） yum install -y createrepo createrepo ./ceph 压缩（联网主机） tar zcvf ceph.tar.gz ceph 配置使用（离线主机） tar zxvf ceph.tar.gz -C / cat > /etc/yum.repos.d/ceph.repo Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/03存储使用/块设备类型存储使用.html":{"url":"1.Linux基础/1.7存储/ceph/03存储使用/块设备类型存储使用.html","title":"块设备类型存储使用","keywords":"","body":"块设备使用 ceph管理节点 1.创建池 [root@ceph01 ~]# ceph osd pool create rbd-demo-pool 64 64 pool 'rbd-demo-pool' created 2.设置配额 [root@ceph01 ~]# ceph osd pool set-quota rbd-demo-pool max_bytes 1G set-quota max_bytes = 1073741824 for pool rbd-demo-pool 3.关联应用 [root@ceph01 ~]# ceph osd pool application enable rbd-demo-pool rbd enabled application 'rbd' on pool 'rbd-demo-pool' 4.初始化 [root@ceph01 ~]# rbd pool init rbd-demo-pool 5.创建rbd用户 ceph auth get-or-create client.qemu mon 'profile rbd' osd 'profile rbd pool=rbd-demo-pool' mgr 'profile rbd pool=rbd-demo-pool' -o /etc/ceph/ceph.client.qemu.keyring 6.创建rbd映像 在将块设备添加到节点之前，必须先在Ceph存储集群中为其创建映像。要创建块设备映像，请执行以下操作： rbd create --size 1G rbd-demo-pool/rbd-demo-image 7.查看块设备映像 # rbd ls {poolname} [root@ceph01 ~]# rbd ls rbd-demo-pool rbd-demo-image 8.查看块设备映像信息 [root@ceph01 ~]# rbd info rbd-demo-pool/rbd-demo-image rbd image 'rbd-demo-image': size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: 3d92a06e59b5 block_name_prefix: rbd_data.3d92a06e59b5 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Fri Mar 19 15:45:58 2021 access_timestamp: Fri Mar 19 15:45:58 2021 modify_timestamp: Fri Mar 19 15:45:58 2021 客户端 1.删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 2.创建yum源文件（客户端） online curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo offline 下载以下文件上传至/etc/yum.repos.d/ Centos-7.repo epel-7.repo 3.配置ceph镜像源仓库 cat > /etc/yum.repos.d/ceph.repo 4.配置yum代理 适用于主机通过代理访问互联网场景 以下变量注意替换 username: 代理用户名 password: 代理用户密码 proxy_host: 代理IP地址 proxy_port: 代理端口 echo \"proxy=http://username:password@proxy_host:proxy_port\" >> /etc/yum.conf 5.安装ceph-common yum install -y ceph-common 6.拷贝配置文件 mkdir -p /etc/ceph 7.客户端创建挂载目录 mkdir -p /ceph chmod 777 /ceph 8.从服务端scp以下文件至客户端/etc/ceph下 /etc/ceph/ceph.client.qemu.keyring /etc/ceph/ceph.conf scp /etc/ceph/{ceph.conf,ceph.client.admin.keyring} ip:/etc/ceph/ 9.映射块设备 [root@localhost ~]# rbd map rbd-demo-pool/rbd-demo-image --name client.qemu /dev/rbd0 [root@localhost ~]# echo \"rbd-demo-pool/rbd-demo-image id=qemu,keyring=/etc/ceph/ceph.client.qemu.keyring\" >> /etc/ceph/rbdmap 10.格式化块设备 mkfs.ext4 -q /dev/rbd0 11.挂载使用 mount /dev/rbd0 /ceph 12.查看挂载 lsblk 13.查看块设备映射 [root@localhost ~]# rbd device list id pool namespace image snap device 0 rbd-demo-pool rbd-demo-image - /dev/rbd0 14.修改fstab，设置开机挂载 echo \"/dev/rbd0 /ceph ext4 defaults,noatime,_netdev 0 0\" >> /etc/fstab 15.配置开机自启动 vim /etc/init.d/rbdmap 填充以下内容 #!/bin/bash #chkconfig: 2345 80 60 #description: start/stop rbdmap ### BEGIN INIT INFO # Provides: rbdmap # Required-Start: $network # Required-Stop: $network # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Ceph RBD Mapping # Description: Ceph RBD Mapping ### END INIT INFO DESC=\"RBD Mapping\" RBDMAPFILE=\"/etc/ceph/rbdmap\" . /lib/lsb/init-functions #. /etc/redhat-lsb/lsb_log_message，加入此行后不正长 do_map() { if [ ! -f \"$RBDMAPFILE\" ]; then echo \"$DESC : No $RBDMAPFILE found.\" exit 0 fi echo \"Starting $DESC\" # Read /etc/rbdtab to create non-existant mapping newrbd= RET=0 while read DEV PARAMS; do case \"$DEV\" in \"\"|\\#*) continue ;; */*) ;; *) DEV=rbd/$DEV ;; esac OIFS=$IFS IFS=',' for PARAM in ${PARAMS[@]}; do CMDPARAMS=\"$CMDPARAMS --$(echo $PARAM | tr '=' ' ')\" done IFS=$OIFS if [ ! -b /dev/rbd/$DEV ]; then echo $DEV rbd map $DEV $CMDPARAMS [ $? -ne \"0\" ] && RET=1 newrbd=\"yes\" fi done 16.赋权 yum install redhat-lsb -y chmod +x /etc/init.d/rbdmap service rbdmap start chkconfig rbdmap on 适用场景 可以当成本地盘来用： 虚机存储 开发数据库存储 存储日志 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/03存储使用/文件系统类型存储使用.html":{"url":"1.Linux基础/1.7存储/ceph/03存储使用/文件系统类型存储使用.html","title":"文件系统类型存储使用","keywords":"","body":"ceph文件系统使用 服务端 1.安装mds（ceph节点安装，建议3个mds） ceph-deploy mds create ceph01 ceph02 ceph03 2.创建cephfs存储池与元数据池 ceph osd pool create cephfs_data 64 ceph osd pool create cephfs_metadata 64 3.创建文件系统 ceph fs new cephfs cephfs_metadata cephfs_data 4.关联应用 [root@ceph01 ~]# ceph osd pool application enable cephfs_data cephfs enabled application 'cephfs' on pool 'cephfs_data' 5.设置配额 ceph osd pool set-quota cephfs_data max_bytes 100G 6.创建用户 [root@ceph01 ~]# ceph auth get-or-create client.cephfs mon 'allow r' mds 'allow r, allow rw path=/' osd 'allow rw pool=cephfs_data' [client.cephfs] key = AQCHWlRg46I6EBAAg+xBZnFsqOYIGluPd5h1QA== 客户端 内核需4.x 1.删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 2.创建yum源文件（客户端） online curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo offline 下载以下文件上传至/etc/yum.repos.d/ Centos-7.repo epel-7.repo 3.配置ceph镜像源仓库 cat > /etc/yum.repos.d/ceph.repo 4.配置yum代理 适用于主机通过代理访问互联网场景 以下变量注意替换 username: 代理用户名 password: 代理用户密码 proxy_host: 代理IP地址 proxy_port: 代理端口 echo \"proxy=http://username:password@proxy_host:proxy_port\" >> /etc/yum.conf 5.安装ceph-common yum install -y ceph-common 6.创建目录 配置目录 mkdir -p /etc/ceph 挂载cephfs目录 mkdir -p /cephfs 7.服务端创建认证文件 服务端执行以下命令获取cephfs用户认证信息 [root@ceph01 ~]# ceph auth get client.cephfs exported keyring for client.cephfs [client.cephfs] key = AQCHWlRg46I6EBAAg+xBZnFsqOYIGluPd5h1QA== caps mds = \"allow r, allow rw path=/\" caps mon = \"allow r\" caps osd = \"allow rw pool=cephfs_data\" 8.客户端创建认证文件 cat /etc/ceph/cephfs.key AQCHWlRg46I6EBAAg+xBZnFsqOYIGluPd5h1QA== EOF 9.客户端挂载文件系统 mount -t ceph 192.168.1.69:6789,192.168.1.70:6789,192.168.1.71:6789:/ /cephfs -o name=cephfs,secretfile=/etc/ceph/cephfs.key 10.创建测试文件 touch /cephfs/123 11.配置开机挂载 cat > /etc/fstab 192.168.1.69:6789,192.168.1.70:6789,192.168.1.71:6789:/ /cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfs.key,noatime,_netdev 0 2 EOF 12.重启主机验证 reboot 适用场景 文件共享 网站文件、代码存储 数据备份 日志存储 数据分析 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/04对接k8s/01csi简介.html":{"url":"1.Linux基础/1.7存储/ceph/04对接k8s/01csi简介.html","title":"01csi简介","keywords":"","body":"csi简介 Kubernetes从1.9版本开始引入容器存储接口Container Storage Interface（CSI）机制，用于在Kubernetes和外部存储系统之间建立一套标准的存储管理接口，通过该接口为容器提供存储服务。 csi设计背景 Kubernetes通过PV、PVC、Storageclass已经提供了一种强大的基于插件的存储管理机制， 但是各种存储插件提供的存储服务都是基于一种被称为in-true（树内）的方式提供的， 这要求存储插件的代码必须被放进Kubernetes的主干代码库中才能被Kubernetes调用， 属于紧耦合的开发模式。这种in-tree方式会带来一些问题： 存储插件的代码需要与Kubernetes的代码放在同一代码库中，并与Kubernetes的二进制文件共同发布 存储插件代码的开发者必须遵循Kubernetes的代码开发规范 存储插件代码的开发者必须遵循Kubernetes的发布流程，包括添加对Kubernetes存储系统的支持和错误修复 Kubernetes社区需要对存储插件的代码进行维护，包括审核、测试等工作 存储插件代码中的问题可能会影响Kubernetes组件的运行，并且很难排查问题 存储插件代码与Kubernetes的核心组件（kubelet和kubecontroller-manager）享有相同的系统特权权限，可能存在可靠性和安全性问题。 部署第三方驱动的可执行文件仍然需要宿主机的root权限，存在安全隐患 存储插件在执行mount、attach这些操作时，通常需要在宿主机上安装一些第三方工具包和依赖库， 使得部署过程更加复杂，例如部署Ceph时需要安装rbd库，部署GlusterFS时需要安装mount.glusterfs库，等等 基于以上这些问题和考虑，Kubernetes逐步推出与容器对接的存储接口标准，存储提供方只需要基于标准接口进行存储插件的实现，就能使用Kubernetes的原生存储机制为容器提供存储服务。这套标准被称为CSI（容器存储接口）。 在CSI成为Kubernetes的存储供应标准之后，存储提供方的代码就能和Kubernetes代码彻底解耦，部署也与Kubernetes核心组件分离，显然，存储插件的开发由提供方自行维护，就能为Kubernetes用户提供更多的存储功能，也更加安全可靠。 基于CSI的存储插件机制也被称为out-of-tree（树外）的服务提供方式，是未来Kubernetes第三方存储插件的标准方案。 csi架构 KubernetesCSI存储插件的关键组件和推荐的容器化部署架构： CSI Controller CSI Controller的主要功能是提供存储服务视角对存储资源和存储卷进行管理和操作。 在Kubernetes中建议将其部署为单实例Pod，可以使用StatefulSet或Deployment控制器进行部署，设置副本数量为1，保证为一种存储插件只运行一个控制器实例。 在这个Pod内部署两个容器： 与Master（kube-controller-manager）通信的辅助sidecar容器。在sidecar容器内又可以包含external-attacher和external-provisioner两个容器，它们的功能分别如下: external-attacher：监控VolumeAttachment资源对象的变更，触发针对CSI端点的ControllerPublish和ControllerUnpublish操作。 external-provisioner：监控PersistentVolumeClaim资源对象的变更，触发针对CSI端点的CreateVolume和DeleteVolume操作。 CSI Driver存储驱动容器，由第三方存储提供商提供，需要实现上述接口。 这两个容器通过本地Socket（Unix DomainSocket，UDS），并使用gPRC协议进行通信。 sidecar容器通过Socket调用CSI Driver容器的CSI接口，CSI Driver容器负责具体的存储卷操作。 CSI Node CSI Node的主要功能是对主机（Node）上的Volume进行管理和操作。在Kubernetes中建议将其部署为DaemonSet，在每个Node上都运行一个Pod。 在这个Pod中部署以下两个容器： 与kubelet通信的辅助sidecar容器node-driver-registrar，主要功能是将存储驱动注册到kubelet中 CSI Driver存储驱动容器，由第三方存储提供商提供，主要功能是接收kubelet的调用，需要实现一系列与Node相关的CSI接口，例如NodePublishVolume接口（用于将Volume挂载到容器内的目标路径）、NodeUnpublishVolume接口（用于从容器中卸载Volume），等等。 node-driver-registrar容器与kubelet通过Node主机的一个hostPath目录下的unixsocket进行通信。CSI Driver容器与kubelet通过Node主机的另一个hostPath目录下的unixsocket进行通信，同时需要将kubelet的工作目录（默认为/var/lib/kubelet）挂载给CSIDriver容器，用于为Pod进行Volume的管理操作（包括mount、umount等）。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/04对接k8s/02块设备类型存储使用.html":{"url":"1.Linux基础/1.7存储/ceph/04对接k8s/02块设备类型存储使用.html","title":"02块设备类型存储使用","keywords":"","body":"k8s接入ceph块存储 使用ceph-csi 实现 ceph服务端 1.创建一个池，用以为k8s提供块存储服务 [root@ceph01 ~]# ceph osd pool create rbd-k8s-pool 256 256 SSD_rule pool 'rbd-k8s-pool' created 2.设置配额 [root@ceph01 ~]# ceph osd pool set-quota rbd-k8s-pool max_bytes 100G set-quota max_bytes = 107374182400 for pool rbd-k8s-pool 3.关联应用 [root@ceph01 ~]# ceph osd pool application enable rbd-k8s-pool rbd enabled application 'rbd' on pool 'rbd-k8s-pool' 4.初始化 rbd pool init rbd-k8s-pool 5.创建用户 [root@ceph01 ~]# ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=rbd-k8s-pool' mgr 'profile rbd pool=rbd-k8s-pool' [client.kubernetes] key = AQCS6kFg0NRDIBAAorr8r5Oxiz1eYH61VvLVYA== k8s节点 主节点执行以下步骤 1.下载配置文件 ceph-csi-3.2.0.zip 2.上传配置文件解压 unzip ceph-csi-3.2.0.zip 3.创建一个命名空间，用于管理ceph-csi kubectl create ns ceph-csi 4.更改ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-config-map.yaml 首先获取集群信息(ceph管理节点执行) b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294为集群ID mon节点地址：192.168.1.69:6789,192.168.1.70:6789,192.168.1.71:6789 [root@ceph01 ~]# ceph mon dump dumped monmap epoch 2 epoch 2 fsid b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 last_changed 2021-02-22 14:36:08.199609 created 2021-02-22 14:27:26.357269 min_mon_release 14 (nautilus) 0: [v2:192.168.1.69:3300/0,v1:192.168.1.69:6789/0] mon.ceph01 1: [v2:192.168.1.70:3300/0,v1:192.168.1.70:6789/0] mon.ceph02 2: [v2:192.168.1.71:3300/0,v1:192.168.1.71:6789/0] mon.ceph03 更改csi-config-map.yaml内容如下： vim ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-config-map.yaml 内容参考如下 --- apiVersion: v1 kind: ConfigMap data: config.json: |- [ { \"clusterID\": \"b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294\", \"monitors\": [ \"192.168.1.69:6789\", \"192.168.1.70:6789\", \"192.168.1.71:6789\" ] } ] metadata: name: ceph-csi-config 5.创建csi-config-map kubectl -n ceph-csi apply -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-config-map.yaml 6.创建csi-rbd-secret 创建 cat ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbd-secret.yaml apiVersion: v1 kind: Secret metadata: name: csi-rbd-secret namespace: ceph-csi stringData: userID: kubernetes userKey: AQCS6kFg0NRDIBAAorr8r5Oxiz1eYH61VvLVYA== EOF 其中：AQCS6kFg0NRDIBAAorr8r5Oxiz1eYH61VvLVYA==可通过在ceph服务端执行ceph auth get client.kubernetes获取 发布 kubectl apply -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbd-secret.yaml 7.配置清单中的namespace改成ceph-csi sed -i \"s/namespace: default/namespace: ceph-csi/g\" $(grep -rl \"namespace: default\" ./ceph-csi-3.2.0/deploy/rbd/kubernetes) sed -i -e \"/^kind: ServiceAccount/{N;N;a\\ namespace: ceph-csi # 输入到这里的时候需要按一下回车键，在下一行继续输入 }\" $(egrep -rl \"^kind: ServiceAccount\" ./ceph-csi-3.2.0/deploy/rbd/kubernetes) 8.创建ServiceAccount和RBAC ClusterRole/ClusterRoleBinding资源对象 kubectl create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml kubectl create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml 9.创建PodSecurityPolicy kubectl create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-provisioner-psp.yaml kubectl create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-nodeplugin-psp.yaml 10.调整csi-rbdplugin-provisioner.yaml和csi-rbdplugin.yaml 将csi-rbdplugin.yaml中的kms部分配置注释掉 # vim ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbdplugin.yaml ... - name: ceph-csi-encryption-kms-config mountPath: /etc/ceph-csi-encryption-kms-config/ ... ... - name: ceph-csi-encryption-kms-config configMap: name: ceph-csi-encryption-kms-config ... 11.将csi-rbdplugin-provisioner.yaml中的kms部分配置注释掉 # vim ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml ... - name: ceph-csi-encryption-kms-config mountPath: /etc/ceph-csi-encryption-kms-config/ ... ... - name: ceph-csi-encryption-kms-config configMap: name: ceph-csi-encryption-kms-config ... 12.将csi-rbdplugin.yaml中的image部分调整为可访问镜像地址 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 quay.io/cephcsi/cephcsi:v3.2.0 13.将csi-rbdplugin-provisioner.yaml中的image部分调整为可访问镜像地址 quay.io/cephcsi/cephcsi:v3.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-attacher:v3.0.2 k8s.gcr.io/sig-storage/csi-resizer:v1.0.1 14.发布csi-rbdplugin-provisioner.yaml和csi-rbdplugin.yaml kubectl -n ceph-csi create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml kubectl -n ceph-csi create -f ceph-csi-3.2.0/deploy/rbd/kubernetes/csi-rbdplugin.yaml 15.查看运行状态 [root@ceph01 ~]# kubectl get pod -n ceph-csi NAME READY STATUS RESTARTS AGE csi-rbdplugin-ddc42 3/3 Running 0 76s csi-rbdplugin-fwwfv 3/3 Running 0 76s csi-rbdplugin-provisioner-76959bd74d-gwd9k 7/7 Running 0 5h32m csi-rbdplugin-provisioner-76959bd74d-nb574 7/7 Running 0 5h32m 16.创建StorageClass b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294为ceph集群ID注意替换 生成配置文件 cat ceph-csi-3.2.0/deploy/rbd/kubernetes/storageclass.yaml --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-csi-rbd-sc provisioner: rbd.csi.ceph.com parameters: clusterID: b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 pool: rbd-k8s-pool imageFeatures: layering csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi csi.storage.k8s.io/fstype: ext4 reclaimPolicy: Delete allowVolumeExpansion: true mountOptions: - discard EOF 创建 kubectl apply -f ceph-csi-3.2.0/deploy/rbd/kubernetes/storageclass.yaml 配置为默认storage class kubectl patch storageclass ceph-csi-rbd-sc -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' 查看storage class [root@ceph01 ~]# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ceph-csi-rbd-sc (default) rbd.csi.ceph.com Delete Immediate true 117s 17.创建pvc验证可用性 生成配置 cat ceph-csi-3.2.0/deploy/rbd/kubernetes/pvc-demo.yaml --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: ceph-pvc-demo namespace: default spec: storageClassName: ceph-csi-rbd-sc accessModes: - ReadWriteOnce resources: requests: storage: 1Gi EOF 创建 kubectl apply -f ceph-csi-3.2.0/deploy/rbd/kubernetes/pvc-demo.yaml 查看 [root@ceph01 ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ceph-pvc-demo Bound pvc-7b7d4d8a-c4f4-40b6-9372-661ece7c385e 1Gi RWO ceph-csi-rbd-sc 13s 18.pvc扩容 生成配置 cat ceph-csi-3.2.0/deploy/rbd/kubernetes/nginx-demo.yaml apiVersion: v1 kind: Pod metadata: name: testpv labels: role: web-frontend spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: ceph-pvc-demo mountPath: \"/usr/share/nginx/html\" volumes: - name: ceph-pvc-demo persistentVolumeClaim: claimName: ceph-pvc-demo EOF 发布 kubectl apply -f ceph-csi-3.2.0/deploy/rbd/kubernetes/nginx-demo.yaml 查看pvc [root@ceph01 ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ceph-pvc-demo Bound pvc-360f8c5b-0f82-4f17-957b-a7eb5cf93f7e 1Gi RWO ceph-csi-rbd-sc 2m50s 编辑修改pvc kubectl edit pvc ceph-pvc-demo 修改以下内容，storage: 1Gi调整为storage: 10Gi ... spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi ... 重启 kubectl get pod testpv -o yaml | kubectl replace --force -f - 再次查看pvc [root@ceph01 ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ceph-pvc-demo Bound pvc-360f8c5b-0f82-4f17-957b-a7eb5cf93f7e 10Gi RWO ceph-csi-rbd-sc 9m26s Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/04对接k8s/03文件系统类型存储使用.html":{"url":"1.Linux基础/1.7存储/ceph/04对接k8s/03文件系统类型存储使用.html","title":"03文件系统类型存储使用","keywords":"","body":"k8s接入ceph文件系统 ceph服务端 1.安装mds ceph-deploy mds create ceph01 ceph02 ceph03 2.创建cephfs存储池与元数据池，用以为k8s提供文件系统服务 ceph osd pool create cephfs_data 64 ceph osd pool create cephfs_metadata 64 3.创建文件系统 ceph fs new k8s-cephfs cephfs_metadata cephfs_data 4.关联应用 [root@ceph01 ~]# ceph osd pool application enable cephfs_data cephfs enabled application 'cephfs' on pool 'cephfs_data' 5.设置配额 ceph osd pool set-quota cephfs_data max_bytes 100G 6.创建用户 [root@ceph01 kubernetes]# ceph auth get-or-create client.cephfs mon 'allow r' mds 'allow r, allow rw path=/' osd 'allow rw pool=cephfs_data' [client.cephfs] key = AQCoW0dgQk4qGhAAwayKv70OSyyWB3XpZ1JLYQ== k8s节点 1.下载配置文件 ceph-csi-3.2.0.zip 2.上传配置文件解压 unzip ceph-csi-3.2.0.zip 3.创建一个命名空间，用于管理ceph-csi kubectl create ns ceph-csi 4.更改ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-config-map.yaml 获取集群信息(ceph管理节点执行) [root@ceph01 ~]# ceph mon dump dumped monmap epoch 2 epoch 2 fsid b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294 last_changed 2021-02-22 14:36:08.199609 created 2021-02-22 14:27:26.357269 min_mon_release 14 (nautilus) 0: [v2:192.168.1.69:3300/0,v1:192.168.1.69:6789/0] mon.ceph01 1: [v2:192.168.1.70:3300/0,v1:192.168.1.70:6789/0] mon.ceph02 2: [v2:192.168.1.71:3300/0,v1:192.168.1.71:6789/0] mon.ceph03 b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294为集群ID 监控节点地址：192.168.1.69:6789,192.168.1.70:6789,192.168.1.71:6789 更改 vim ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-config-map.yaml 更改后csi-config-map.yaml内容如下： --- apiVersion: v1 kind: ConfigMap data: config.json: |- [ { \"clusterID\": \"b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294\", \"monitors\": [ \"192.168.1.69:6789\", \"192.168.1.70:6789\", \"192.168.1.71:6789\" ] } ] metadata: name: ceph-csi-config 5.创建csi-config-map kubectl -n ceph-csi apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-config-map.yaml 6.创建csi-cephfs-secret 创建 cat ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-cephfs-secret.yaml --- apiVersion: v1 kind: Secret metadata: name: csi-cephfs-secret namespace: ceph-csi stringData: # Required for statically provisioned volumes userID: admin userKey: AQCJe+Bfb6JtOhAANdn/FmcTj179PW6EI4KTng== # Required for dynamically provisioned volumes adminID: admin adminKey: AQCJe+Bfb6JtOhAANdn/FmcTj179PW6EI4KTng== EOF AQCoW0dgQk4qGhAAwayKv70OSyyWB3XpZ1JLYQ====可通过在ceph服务端执行ceph auth get client.cephfs获取 发布 kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-cephfs-secret.yaml 7.配置清单中的namespace改成ceph-csi sed -i \"s/namespace: default/namespace: ceph-csi/g\" $(grep -rl \"namespace: default\" ./ceph-csi-3.2.0/deploy/cephfs/kubernetes) sed -i -e \"/^kind: ServiceAccount/{N;N;a\\ namespace: ceph-csi # 输入到这里的时候需要按一下回车键，在下一行继续输入 }\" $(egrep -rl \"^kind: ServiceAccount\" ./ceph-csi-3.2.0/deploy/cephfs/kubernetes) 8.创建ServiceAccount和RBAC ClusterRole/ClusterRoleBinding资源对象 kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-provisioner-rbac.yaml kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-nodeplugin-rbac.yaml 9.创建PodSecurityPolicy kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-provisioner-psp.yaml kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-nodeplugin-psp.yaml 10.调整csi-cephfsplugin-provisioner.yaml和csi-cephfsplugin.yaml 将csi-cephfsplugin.yaml中的image部分调整为可访问镜像地址 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 quay.io/cephcsi/cephcsi:v3.2.0 将csi-cephfsplugin-provisioner.yaml中的image部分调整为可访问镜像地址 quay.io/cephcsi/cephcsi:v3.2.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2 k8s.gcr.io/sig-storage/csi-attacher:v3.0.2 k8s.gcr.io/sig-storage/csi-resizer:v1.0.1 11.发布csi-cephfsplugin-provisioner.yaml和csi-cephfsplugin.yaml kubectl -n ceph-csi apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-cephfsplugin-provisioner.yaml kubectl -n ceph-csi apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/csi-cephfsplugin.yaml 12.查看运行状态 kubectl get pod -n ceph-csi 13.生成StorageClass配置文件 b1c2511e-a1a5-4d6d-a4be-0e7f0d6d4294为ceph集群ID注意替换 生成配置文件 cat ceph-csi-3.2.0/deploy/cephfs/kubernetes/storageclass.yaml --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: kubernetes-csi-fs-sc provisioner: cephfs.csi.ceph.com parameters: clusterID: 1fc9f495-498c-4fe2-b3d5-80a041bc5c49 pool: cephfs_data fsName: k8s-cephfs imageFeatures: layering csi.storage.k8s.io/provisioner-secret-name: csi-cephfs-secret csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi csi.storage.k8s.io/controller-expand-secret-name: csi-cephfs-secret csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi csi.storage.k8s.io/node-stage-secret-name: csi-cephfs-secret csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi csi.storage.k8s.io/fstype: ext4 reclaimPolicy: Delete allowVolumeExpansion: true mountOptions: - discard EOF 14.创建StorageClass kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/storageclass.yaml 15.配置为默认storage class(已有默认，需要编辑更改) kubectl patch storageclass ceph-csi-rbd-sc -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' 16.查看storage class [root@ceph01 ~]# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ceph-csi-cephfs-sc cephfs.csi.ceph.com Delete Immediate true 5s ceph-csi-rbd-sc (default) rbd.csi.ceph.com Delete Immediate true 27h 17.创建pvc验证可用性 生成配置 cat ceph-csi-3.2.0/deploy/cephfs/kubernetes/pvc-demo.yaml --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cephfs-pvc-demo namespace: default spec: storageClassName: ceph-csi-cephfs-sc accessModes: - ReadWriteMany resources: requests: storage: 1Gi EOF 创建 kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/pvc-demo.yaml 查看 [root@ceph01 ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ceph-pvc-demo Bound pvc-360f8c5b-0f82-4f17-957b-a7eb5cf93f7e 20Gi RWO ceph-csi-rbd-sc 28h cephfs-pvc-demo Bound pvc-9400e0ab-2e44-4ce6-af39-a403441931e5 1Gi RWX ceph-csi-cephfs-sc 71s 18.pvc扩容 生成配置 cat ceph-csi-3.2.0/deploy/cephfs/kubernetes/nginx-demo.yaml apiVersion: v1 kind: Pod metadata: name: cephfs-testpv labels: role: web-frontend spec: containers: - name: web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: cephfs-pvc-demo mountPath: \"/usr/share/nginx/html\" volumes: - name: cephfs-pvc-demo persistentVolumeClaim: claimName: cephfs-pvc-demo EOF 发布 kubectl apply -f ceph-csi-3.2.0/deploy/cephfs/kubernetes/nginx-demo.yaml 查看pvc [root@ceph01 ~]# kubectl get pvc cephfs-pvc-demo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cephfs-pvc-demo Bound pvc-9400e0ab-2e44-4ce6-af39-a403441931e5 1Gi RWX ceph-csi-cephfs-sc 7m2s 编辑修改pvc kubectl edit pvc cephfs-pvc-demo 修改以下内容，storage: 1Gi调整为storage: 10Gi ... spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi ... 再次查看pvc [root@ceph01 ~]# kubectl get pvc cephfs-pvc-demo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cephfs-pvc-demo Bound pvc-9400e0ab-2e44-4ce6-af39-a403441931e5 10Gi RWX ceph-csi-cephfs-sc 9m 与ceph rbd不同的是，扩容pvc时不需重启后端应用 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/05运维管理/01服务启停.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/01服务启停.html","title":"01服务启停","keywords":"","body":"服务启停 按节点启动所有ceph服务 systemctl start ceph.target 或 sudo systemctl start ceph-osd.target sudo systemctl start ceph-mon.target sudo systemctl start ceph-mds.target 按节点停止所有ceph服务 systemctl stop ceph\\*.service ceph\\*.target 或 sudo systemctl stop ceph-mon\\*.service ceph-mon.target sudo systemctl stop ceph-osd\\*.service ceph-osd.target sudo systemctl stop ceph-mds\\*.service ceph-mds.target 控制节点管理集群所有服务 启 sudo systemctl start ceph-osd@{id} sudo systemctl start ceph-mon@{hostname} sudo systemctl start ceph-mds@{hostname} 停 sudo systemctl stop ceph-osd@{id} sudo systemctl stop ceph-mon@{hostname} sudo systemctl stop ceph-mds@{hostname} 启动指定节点osd systemctl start ceph-osd@0 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/05运维管理/02pool的CRUD.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/02pool的CRUD.html","title":"02pool的CRUD","keywords":"","body":"池管理 查看pool [root@ceph01 ~]# ceph osd lspools 1 ssd-demo-pool 2 nvme-demo-pool 创建一个pool 格式 ceph osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] \\ [crush-rule-name] [expected-num-objects] 或 ceph osd pool create {pool-name} {pg-num} {pgp-num} erasure \\ [erasure-code-profile] [crush-rule-name] [expected_num_objects] 删除池 语法格式 ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it] 修改配置 vim /etc/ceph/ceph.conf 添加如下： [mon] mon_allow_pool_delete=true 更新 cd /etc/ceph ceph-deploy --overwrite-conf config push ceph01 ceph02 ceph03 重启 systemctl restart ceph-mon.target 删除ddd-pool [root@ceph01 ceph]# ceph osd pool delete ddd-pool ddd-pool --yes-i-really-really-mean-it pool 'ddd-pool' removed 池重命名 ceph osd pool rename {current-pool-name} {new-pool-name} 显示池统计信息 [root@ceph01 ~]# rados df POOL_NAME USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS RD WR_OPS WR USED COMPR UNDER COMPR nvme-demo-pool 12 KiB 1 0 3 0 0 0 0 0 B 1 1 KiB 0 B 0 B ssd-demo-pool 12 KiB 1 0 3 0 0 0 0 0 B 1 1 KiB 0 B 0 B total_objects 2 total_used 30 GiB total_avail 26 TiB total_space 26 TiB 查看池io [root@ceph01 ~]# ceph osd pool stats ssd-demo-pool pool ssd-demo-pool id 1 nothing is going on 创建池快照 # ceph osd pool mksnap {pool-name} {snap-name} [root@ceph01 ~]# ceph osd pool mksnap ssd-demo-pool ssd-demo-pool-snap-20210301 created pool ssd-demo-pool snap ssd-demo-pool-snap-20210301 删除池快照 # ceph osd pool rmsnap {pool-name} {snap-name} [root@ceph01 ~]# ceph osd pool rmsnap ssd-demo-pool ssd-demo-pool-snap-20210301 removed pool ssd-demo-pool snap ssd-demo-pool-snap-20210301 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/05运维管理/03pool的常用配置.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/03pool的常用配置.html","title":"03pool的常用配置","keywords":"","body":"池的常用配置 PG配置 设置池的放置组数 ceph osd pool set {pool-name} pgp_num {pgp_num} 获取池的放置组数 ceph osd pool get {pool-name} pg_num 获取集群的PG统计信息 # ceph pg dump [--format {format}] ceph pg dump -f json 将池与应用程序关联 池在使用之前需要与应用程序相关联。将与cepfs一起使用的池或由RGW自动创建的池将自动关联。 用于与RBD一起使用的池应该使用RBD工具进行初始化。 # ceph osd pool application enable {pool-name} {application-name}(cephfs, rbd, rgw) [root@ceph01 ~]# ceph osd pool application enable ssd-demo-pool rbd enabled application 'rbd' on pool 'ssd-demo-pool' [root@ceph01 ~]# ceph osd pool application enable nvme-demo-pool cephfs enabled application 'cephfs' on pool 'nvme-demo-pool' 池配额 您可以为每个池的最大字节数和/或最大对象数设置池配额。 # ceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}] ceph osd pool set-quota data max_objects 10000 要删除配额，请将其值设置为0 ceph osd pool set-quota data max_objects 0 设置对象副本数 默认为3 ceph osd pool set {poolname} size {num-replicas} pg_autoscale_mode 放置组（PGs）是Ceph分发数据的内部实现细节。 过启用pg autoscaling，您可以允许集群提出建议或根据集群的使用方式自动调整PGs。 系统中的每个池都有一个pg_autoscale_mode属性，可以设置为off、on或warn： off：禁用此池的自动缩放。由管理员为每个池选择适当的PG数量。 on：启用给定池的PG计数的自动调整。 warn：当PG计数需要调整时发出健康警报（默认） 为指定池设置放置组数自动伸缩 # ceph osd pool set pg_autoscale_mode [root@ceph01 ~]# ceph osd pool set ssd-demo-pool pg_autoscale_mode on set pool 1 pg_autoscale_mode to on 设置集群内所有池放置组数自动伸缩 # ceph config set global osd_pool_default_pg_autoscale_mode ceph config set global osd_pool_default_pg_autoscale_mode on 查看集群内放置组数伸缩策略 [root@ceph01 ~]# ceph osd pool autoscale-status POOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE nvme-demo-pool 7 3.0 11178G 0.0000 1.0 256 32 warn ssd-demo-pool 7 3.0 15202G 0.0000 1.0 32 on 创建ddd-pool，并查看集群内放置组数伸缩策略 [root@ceph01 ~]# ceph osd pool create ddd-pool 1 1 pool 'ddd-pool' created [root@ceph01 ~]# ceph osd pool autoscale-status POOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE nvme-demo-pool 7 3.0 26380G 0.0000 1.0 256 32 warn ssd-demo-pool 7 3.0 15202G 0.0000 1.0 32 on ddd-pool 0 3.0 26380G 0.0000 1.0 1 32 on 查看池伸缩建议 POOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE nvme-demo-pool 7 3.0 26380G 0.0000 1.0 256 32 warn ssd-demo-pool 7 3.0 15202G 0.0000 1.0 32 on ddd-pool 0 3.0 26380G 0.0000 1.0 32 on SIZE：存储在池中的数据量 TARGET SIZE：管理员指定的他们希望最终存储在此池中的数据量 RATE：池的乘数，用于确定消耗了多少原始存储容量。例如，3副本池的比率为3.0，而k=4，m=2擦除编码池的比率为1.5。 RAW CAPACITY：负责存储此池(可能还有其他池)数据的OSD上的裸存储容量的总和。 RATIO：当前池正在消耗的总容量的比率(即RATIO = size * rate / raw capacity)。 TARGET RATIO：管理员指定的期望此池消耗的存储空间相对于设置了目标比率的其他池的比率。如果同时指定了目标大小字节和比率，则该比率优先 EFFECTIVE RATIO：有效比率是通过两种方式调整后的目标比率： 减去设置了目标大小的池预期使用的任何容量 在设置了目标比率的池之间规范化目标比率，以便它们共同以空间的其余部分为目标。例如，4个池的目标收益率为1.0，其有效收益率为0.25。 系统使用实际比率和有效比率中的较大者进行计算。 PG_NUM：池的当前PG数 NEW PG_NUM：池的pgu NUM期望值。它始终是2的幂，并且只有当期望值与当前值相差超过3倍时才会出现。 AUTOSCALE：pool_pg_autosacle模式，可以是on、off或warn。 自动缩放 允许集群根据使用情况自动扩展PGs是最简单的方法。Ceph将查看整个系统的总可用存储和PGs的目标数量，查看每个池中存储了多少数据，并尝试相应地分配PGs。 系统采用的方法相对保守，仅在当前pg(pg_num)的数量比它认为应该的数量少3倍以上时才对池进行更改。 每个OSD的pg目标数基于mon_target_pg_per_OSD可配置（默认值：100），可通过以下方式进行调整: ceph config set global mon_target_pg_per_OSD 100 指定池的期望大小 当第一次创建集群或池时，它将消耗集群总容量的一小部分，并且在系统看来似乎只需要少量的放置组。 但是，在大多数情况下，集群管理员都很清楚，随着时间的推移，哪些池将消耗大部分系统容量。 通过向Ceph提供此信息，可以从一开始就使用更合适数量的pg，从而防止pg_num的后续更改以及在进行这些调整时与移动数据相关的开销 池的目标大小可以通过两种方式指定：要么根据池的绝对大小（即字节），要么作为相对于设置了目标大小比的其他池的权重。 ddd-pool预计使用1G存储空间 ceph osd pool set ddd-pool target_size_bytes 1G 相对于设置了target_size_ratio的其他池，mpool预计将消耗1.0。 如果mpool是集群中唯一的池，这意味着预计将使用总容量的100%。 如果有第二个带有target_size_ratio1.0的池，那么两个池都希望使用50%的集群容量。 ceph osd pool set mypool target_size_ratio 1.0 池其他配置 查看可配置项 [root@ceph01 ~]# ceph osd pool -h General usage: ============== usage: ceph [-h] [-c CEPHCONF] [-i INPUT_FILE] [-o OUTPUT_FILE] [--setuser SETUSER] [--setgroup SETGROUP] [--id CLIENT_ID] [--name CLIENT_NAME] [--cluster CLUSTER] [--admin-daemon ADMIN_SOCKET] [-s] [-w] [--watch-debug] [--watch-info] [--watch-sec] [--watch-warn] [--watch-error] [--watch-channel {cluster,audit,*}] [--version] [--verbose] [--concise] [-f {json,json-pretty,xml,xml-pretty,plain}] [--connect-timeout CLUSTER_TIMEOUT] [--block] [--period PERIOD] Ceph administration tool optional arguments: -h, --help request mon help -c CEPHCONF, --conf CEPHCONF ceph configuration file -i INPUT_FILE, --in-file INPUT_FILE input file, or \"-\" for stdin -o OUTPUT_FILE, --out-file OUTPUT_FILE output file, or \"-\" for stdout --setuser SETUSER set user file permission --setgroup SETGROUP set group file permission --id CLIENT_ID, --user CLIENT_ID client id for authentication --name CLIENT_NAME, -n CLIENT_NAME client name for authentication --cluster CLUSTER cluster name --admin-daemon ADMIN_SOCKET submit admin-socket commands (\"help\" for help -s, --status show cluster status -w, --watch watch live cluster changes --watch-debug watch debug events --watch-info watch info events --watch-sec watch security events --watch-warn watch warn events --watch-error watch error events --watch-channel {cluster,audit,*} which log channel to follow when using -w/--watch. One of ['cluster', 'audit', '*'] --version, -v display version --verbose make verbose --concise make less verbose -f {json,json-pretty,xml,xml-pretty,plain}, --format {json,json-pretty,xml,xml-pretty,plain} --connect-timeout CLUSTER_TIMEOUT set a timeout for connecting to the cluster --block block until completion (scrub and deep-scrub only) --period PERIOD, -p PERIOD polling period, default 1.0 second (for polling commands only) Local commands: =============== ping Send simple presence/life test to a mon may be 'mon.*' for all mons daemon {type.id|path} Same as --admin-daemon, but auto-find admin socket daemonperf {type.id | path} [stat-pats] [priority] [] [] daemonperf {type.id | path} list|ls [stat-pats] [priority] Get selected perf stats from daemon/admin socket Optional shell-glob comma-delim match string stat-pats Optional selection priority (can abbreviate name): critical, interesting, useful, noninteresting, debug List shows a table of all available stats Run times (default forever), once per seconds (default 1) Monitor commands: ================= osd pool application disable {--yes-i-really-mean-it} disables use of an application on pool osd pool application enable {--yes-i-really-mean-it} enable use of an application [cephfs,rbd,rgw] on pool osd pool application get {} {} {} get value of key of application on pool osd pool application rm removes application metadata key on pool osd pool application set sets application metadata key to on pool osd pool autoscale-status report on pool pg_num sizing recommendation and intent osd pool cancel-force-backfill [...] restore normal recovery priority of specified pool osd pool cancel-force-recovery [...] restore normal recovery priority of specified pool osd pool create {} {replicated|erasure} create pool {} {} {} {} {} {} {} osd pool deep-scrub [...] initiate deep-scrub on pool osd pool force-backfill [...] force backfill of specified pool first osd pool force-recovery [...] force recovery of specified pool first osd pool get size|min_size|pg_num|pgp_num|crush_rule| get pool parameter hashpspool|nodelete|nopgchange|nosizechange|write_fadvise_dontneed| noscrub|nodeep-scrub|hit_set_type|hit_set_period|hit_set_count|hit_set_ fpp|use_gmt_hitset|target_max_objects|target_max_bytes|cache_target_ dirty_ratio|cache_target_dirty_high_ratio|cache_target_full_ratio| cache_min_flush_age|cache_min_evict_age|erasure_code_profile|min_read_ recency_for_promote|all|min_write_recency_for_promote|fast_read|hit_ set_grade_decay_rate|hit_set_search_last_n|scrub_min_interval|scrub_ max_interval|deep_scrub_interval|recovery_priority|recovery_op_ priority|scrub_priority|compression_mode|compression_algorithm| compression_required_ratio|compression_max_blob_size|compression_min_ blob_size|csum_type|csum_min_block|csum_max_block|allow_ec_overwrites| fingerprint_algorithm|pg_autoscale_mode|pg_autoscale_bias|pg_num_min| target_size_bytes|target_size_ratio osd pool get-quota obtain object or byte limits for pool osd pool ls {detail} list pools osd pool mksnap make snapshot in osd pool rename rename to osd pool repair [...] initiate repair on pool osd pool rm {} {--yes-i-really-really-mean-it} {-- remove pool yes-i-really-really-mean-it-not-faking} osd pool rmsnap remove snapshot from osd pool scrub [...] initiate scrub on pool osd pool set size|min_size|pg_num|pgp_num|pgp_num_actual| set pool parameter to crush_rule|hashpspool|nodelete|nopgchange|nosizechange|write_fadvise_ dontneed|noscrub|nodeep-scrub|hit_set_type|hit_set_period|hit_set_ count|hit_set_fpp|use_gmt_hitset|target_max_bytes|target_max_objects| cache_target_dirty_ratio|cache_target_dirty_high_ratio|cache_target_ full_ratio|cache_min_flush_age|cache_min_evict_age|min_read_recency_ for_promote|min_write_recency_for_promote|fast_read|hit_set_grade_ decay_rate|hit_set_search_last_n|scrub_min_interval|scrub_max_interval| deep_scrub_interval|recovery_priority|recovery_op_priority|scrub_ priority|compression_mode|compression_algorithm|compression_required_ ratio|compression_max_blob_size|compression_min_blob_size|csum_type| csum_min_block|csum_max_block|allow_ec_overwrites|fingerprint_ algorithm|pg_autoscale_mode|pg_autoscale_bias|pg_num_min|target_size_ bytes|target_size_ratio {--yes-i-really-mean-it} osd pool set-quota max_objects|max_bytes set object or byte limit on pool osd pool stats {} obtain stats from all pools, or from specified pool Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/05运维管理/04配置pg放置组.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/04配置pg放置组.html","title":"04配置pg放置组","keywords":"","body":"放置组数配置 # ceph osd pool create {pool-name} pg_num 必须选择pg_num的值，因为它（当前）无法自动计算。以下是一些常用值： 少于5 OSDs设置pg_num为128 5-10 OSDs设置pg_num为512 10-50 OSDs设置pg_num为1024 如果您有超过50个OSD，则需要使用pgcalc 计算pg_num值 (OSDs * 100) Total PGs = ------------ pool size 结果应始终四舍五入到最接近的二次方,只有2的幂才能平衡放置组中对象的数量。其他值将导致数据在您的OSD中的不均匀分布。 例如，对于具有200个OSD和3个副本的池大小的群集，您可以按以下方式估计PG的数量： (200 * 100) ----------- = 6667. 最接近该数值的2的幂等为8192 3 放置组解析 放置组（PG）聚合池中的对象，因为在每个对象的基础上跟踪对象放置和对象元数据在计算上非常昂贵， 即，具有数百万个对象的系统无法在每个对象的基础上真实地跟踪放置。 Ceph客户端将计算对象应该在哪个放置组中。它通过散列对象ID并基于定义池中pg的数量和池的ID应用一个操作来实现这一点 放置组中的对象内容存储在一组OSD中。例如，在大小为2的复制池中，每个放置组将在两个OSD上存储对象. 如果OSD#2失败，另一个将被分配到放置组#1，并将填充OSD#1中所有对象的副本。如果池大小从2更改为3，则会为放置组分配一个附加的OSD，并接收放置组中所有对象的副本。 放置组不拥有OSD,它们与来自同一池甚至其他池的其他放置组共享OSD。如果OSD#2失败，放置组#2还必须使用OSD#3还原对象的副本。 当放置组的数量增加时，新的放置组将被分配OSD。挤压功能的结果也将更改，以前放置组中的某些对象将复制到新放置组中，并从旧放置组中删除。 放置组权衡 数据持久性和所有OSD之间的均匀分布需要更多的放置组，但是它们的数量应该减少到最小以节省CPU和内存。 数据持久性 单个OSD故障后，数据丢失的风险会增大，直到其中包含的数据被完全恢复。让我们想象一个场景，在单个放置组中导致永久的数据丢失: OSD故障（可看作磁盘故障），包含对象的所有副本丢失。对于放置组内的所有对象，副本的数量突然从3个下降到2个。 Ceph通过选择一个新的OSD来重新创建所有对象的第三个副本，从而开始恢复此放置组。 同一放置组中的另一个OSD在新OSD完全填充第三个拷贝之前失败。一些对象将只有一个幸存的副本。 Ceph会选择另一个OSD，并继续复制对象，以恢复所需的副本数量。 同一放置组中的第三个OSD在恢复完成之前失败。如果此OSD包含对象的唯一剩余副本，则它将永久丢失。 在一个包含10个OSD、在3个复制池中有512个放置组的集群中，CRUSH将给每个放置组三个OSD。 最终，每个OSD将托管(512 * 3)/ 10 ~= 150个放置组。 因此，当第一个OSD失败时，上述场景将同时启动所有150个安置组的恢复。 正在恢复的150个安置组可能均匀地分布在剩余的9个OSDs上。 因此，每个剩余的OSD都可能向其他所有OSD发送对象副本，并接收一些新对象来存储，因为它们成为了新的放置组的一部分。 完成此恢复所需的时间完全取决于Ceph集群的体系结构。假设每个OSD由一台机器上的1TB SSD托管， 所有这些OSD都连接到10Gb/s交换机，单个OSD的恢复在M分钟内完成。 如果每台机器有两个OSD、没有SSD、1Gb/s交换机，它至少会慢一个数量级。 在这种大小的集群中，放置组的数量对数据持f久性几乎没有影响。它可能是128或8192，恢复不会慢或快。 但是，将同一个Ceph集群增加到20个OSD而不是10个OSD可能会加快恢复速度，从而显著提高数据持久性。 每个OSD现在只参与~75个放置组，而不是在只有10个OSD时参与~150个，而且仍然需要剩下的19个OSD执行相同数量的对象副本才能恢复。 但是，以前10个OSD每个必须复制大约100GB，现在它们每个必须复制50GB。 如果网络是瓶颈，那么恢复的速度将是现在的两倍。换句话说，当OSDs的数量增加时，恢复速度会更快。 如果这个集群增长到40个OSD，那么每个OSD只能承载35个放置组。 如果OSD故障，除非它被另一个瓶颈阻塞，否则恢复将继续加快。 但是，如果这个集群增长到200个OSD，每个OSD只能承载~7个放置组。 如果一个OSD故障，那么在这些放置组中最多21个OSD（7*3）之间会发生恢复：恢复所需的时间将比有40个OSD时的集群长，这意味着放置组的数量应该增加。 无论恢复时间有多短，第二个OSD都有可能在恢复过程中失败。 在上述10个osd集群中，如果其中任何一个失败，那么~17个放置组（即~150/9个正在恢复的放置组）将只有一个幸存副本。 如果剩余的8个OSD中的任何一个失败，那么两个放置组的最后一个对象很可能会丢失（即大约17/8个放置组，只恢复了剩余的一个副本）。 当群集的大小增加到20个osd时，由于丢失3个osd而损坏的放置组的数量会下降。 丢失的第二个OSD将降级~4（即恢复~75/19个放置组），而不是~17，并且丢失的第三个OSD只有在包含幸存副本的四个OSD之一时才会丢失数据。 换句话说，如果在恢复时间范围内丢失一个OSD的概率为0.0001%，则在具有10个OSD的集群中，丢失的概率从17×10×0.0001%变为具有20个OSD的集群中的4×20×0.0001% 简而言之，更多的osd意味着更快的恢复和更低的导致永久性丢失放置组的级联故障风险。就数据持久性而言，512或4096个放置组在少于50个osd的集群中大致相当。 池中的对象分布 理想情况下，对象均匀分布在每个放置组中。由于CRUSH计算每个对象的放置组， 但实际上不知道该放置组中的每个OSD中存储了多少数据，因此放置组的数量与OSD的数量之间的比率可能会显著影响数据的分布。 例如，如果在一个三副本池中有十个OSD的单个放置组，那么只会使用三个OSD，因为CRUSH没有其他选择。 当有更多的放置组可用时，对象更可能均匀地分布在其中。CRUSH还尽一切努力将osd均匀地分布在所有现有的放置组中。 不均匀的数据分布可能是由osd和放置组之间的比率以外的因素造成的。 由于挤压不考虑对象的大小，一些非常大的对象可能会造成不平衡。 假设100万个4K对象（总共4GB）均匀分布在10个OSD上的1024个放置组中。他们将在每个OSD上使用4GB/10=400MB。 如果将一个400MB对象添加到池中，则支持放置该对象的放置组的三个OSD将被400MB+400MB=800MB填充，而其他七个OSD将仅被400MB占用。 内存、CPU和网络使用情况 对于每个放置组，osd和mon始终需要内存、网络和CPU，甚至在恢复期间需要更多。通过在放置组中聚集对象来共享此开销是它们存在的主要原因之一。 最小化放置组的数量可以节省大量资源。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/05运维管理/05配置管理块存储.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/05配置管理块存储.html","title":"05配置管理块存储","keywords":"","body":"rbd管理 创建rbd 1.创建池 [root@ceph01 ~]# ceph osd pool create rbd-demo-pool 64 64 pool 'rbd-demo-pool' created 2.设置配额 [root@ceph01 ~]# ceph osd pool set-quota rbd-demo-pool max_bytes 1G set-quota max_bytes = 1073741824 for pool rbd-demo-pool 3.关联rbd应用 [root@ceph01 ~]# ceph osd pool application enable rbd-demo-pool rbd enabled application 'rbd' on pool 'rbd-demo-pool' 4.初始化 rbd pool init rbd-demo-pool 5.创建rbd用户 语法格式 ceph auth get-or-create client.{ID} mon 'profile rbd' osd 'profile {profile name} [pool={pool-name}][, profile ...]' mgr 'profile rbd [pool={pool-name}]' 创建ID为qemu、对rbd-demo-pool池有读写权限的用户 ceph auth get-or-create client.qemu mon 'profile rbd' osd 'profile rbd pool=rbd-demo-pool' mgr 'profile rbd pool=rbd-demo-pool' -o /etc/ceph/ceph.client.qemu.keyring 6.创建rbd映像 在将块设备添加到节点之前，必须先在Ceph存储集群中为其创建映像。要创建块设备映像，请执行以下操作： # rbd create --size {megabytes} {pool-name}/{image-name} rbd create --size 1G rbd-demo-pool/rbd-demo-image 查看块设备映像 1.查看池内映像 # rbd ls {poolname} [root@ceph01 ~]# rbd ls rbd-demo-pool rbd-demo-image 2.查看块设备映像信息 [root@ceph01 ~]# rbd info rbd-demo-pool/rbd-demo-image rbd image 'rbd-demo-image': size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: d3ef49824934 block_name_prefix: rbd_data.d3ef49824934 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Mon Mar 1 15:48:05 2021 access_timestamp: Mon Mar 1 15:48:05 2021 modify_timestamp: Mon Mar 1 15:48:05 2021 块设备缩容 1.收缩大小为256M [root@ceph01 ~]# rbd resize --size 256M rbd-demo-pool/rbd-demo-image --allow-shrink Resizing image: 100% complete...done. 2.查看块设备映像信息 [root@ceph01 ~]# rbd info rbd-demo-pool/rbd-demo-image rbd image 'rbd-demo-image': size 256 MiB in 64 objects order 22 (4 MiB objects) snapshot_count: 0 id: d3ef49824934 block_name_prefix: rbd_data.d3ef49824934 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Mon Mar 1 15:48:05 2021 access_timestamp: Mon Mar 1 15:48:05 2021 modify_timestamp: Mon Mar 1 15:48:05 2021 块设备扩容 扩容大小上限为池配额大小 扩容大小至1G [root@ceph01 ~]# rbd resize --size 1G rbd-demo-pool/rbd-demo-image --allow-shrink Resizing image: 100% complete...done. 查看块设备映像信息 [root@ceph01 ~]# rbd info rbd-demo-pool/rbd-demo-image rbd image 'rbd-demo-image': size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: d3ef49824934 block_name_prefix: rbd_data.d3ef49824934 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Mon Mar 1 15:48:05 2021 access_timestamp: Mon Mar 1 15:48:05 2021 modify_timestamp: Mon Mar 1 15:48:05 2021 删除块设备映像 # rbd rm {pool-name}/{image-name} [root@ceph01 ~]# rbd rm rbd-demo-pool/rbd-demo-image Removing image: 100% complete...done. 挂载块设备 1.删除原有yum源repo文件 rm -f /etc/yum.repos.d/*.repo 2.创建yum源文件（客户端） online curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo offline 下载以下文件上传至/etc/yum.repos.d/ Centos-7.repo epel-7.repo 3.配置ceph镜像源仓库 cat > /etc/yum.repos.d/ceph.repo 4.配置yum代理 适用于主机通过代理访问互联网场景 以下变量注意替换 username: 代理用户名 password: 代理用户密码 proxy_host: 代理IP地址 proxy_port: 代理端口 echo \"proxy=http://username:password@proxy_host:proxy_port\" >> /etc/yum.conf 5.安装ceph-common yum install -y ceph-common 6.拷贝配置文件 mkdir -p /etc/ceph 7.客户端创建挂载目录 mkdir -p /ceph chmod 777 /ceph 8.从服务端scp以下文件至客户端/etc/ceph下 /etc/ceph/ceph.client.qemu.keyring /etc/ceph/ceph.conf scp /etc/ceph/{ceph.conf,ceph.client.admin.keyring} ip:/etc/ceph/ 9.映射块设备 [root@localhost ~]# rbd map rbd-demo-pool/rbd-demo-image --name client.qemu /dev/rbd0 [root@localhost ~]# echo \"rbd-demo-pool/rbd-demo-image id=qemu,keyring=/etc/ceph/ceph.client.qemu.keyring\" >> /etc/ceph/rbdmap 10.格式化块设备 mkfs.ext4 -q /dev/rbd0 11.挂载使用 mount /dev/rbd0 /ceph 12.查看挂载 lsblk 13.查看块设备映射 [root@localhost ~]# rbd device list id pool namespace image snap device 0 rbd-demo-pool rbd-demo-image - /dev/rbd0 14.修改fstab，设置开机挂载 echo \"/dev/rbd0 /ceph ext4 defaults,noatime,_netdev 0 0\" >> /etc/fstab 15.配置开机自启动 vim /etc/init.d/rbdmap 填充以下内容 #!/bin/bash #chkconfig: 2345 80 60 #description: start/stop rbdmap ### BEGIN INIT INFO # Provides: rbdmap # Required-Start: $network # Required-Stop: $network # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Ceph RBD Mapping # Description: Ceph RBD Mapping ### END INIT INFO DESC=\"RBD Mapping\" RBDMAPFILE=\"/etc/ceph/rbdmap\" . /lib/lsb/init-functions #. /etc/redhat-lsb/lsb_log_message，加入此行后不正长 do_map() { if [ ! -f \"$RBDMAPFILE\" ]; then echo \"$DESC : No $RBDMAPFILE found.\" exit 0 fi echo \"Starting $DESC\" # Read /etc/rbdtab to create non-existant mapping newrbd= RET=0 while read DEV PARAMS; do case \"$DEV\" in \"\"|\\#*) continue ;; */*) ;; *) DEV=rbd/$DEV ;; esac OIFS=$IFS IFS=',' for PARAM in ${PARAMS[@]}; do CMDPARAMS=\"$CMDPARAMS --$(echo $PARAM | tr '=' ' ')\" done IFS=$OIFS if [ ! -b /dev/rbd/$DEV ]; then echo $DEV rbd map $DEV $CMDPARAMS [ $? -ne \"0\" ] && RET=1 newrbd=\"yes\" fi done 16.赋权 yum install redhat-lsb -y chmod +x /etc/init.d/rbdmap service rbdmap start chkconfig rbdmap on Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/ceph/05运维管理/111卸载.html":{"url":"1.Linux基础/1.7存储/ceph/05运维管理/111卸载.html","title":"111卸载","keywords":"","body":"删除文件系统 删除文件系统 查看文件系统 [root@ceph01 ~]# ceph fs ls name: k8s-cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] 删除文件系统 [root@ceph01 ~]# ceph fs fail k8s-cephfs k8s-cephfs marked not joinable; MDS cannot join the cluster. All MDS ranks marked failed. [root@ceph01 ~]# ceph fs rm k8s-cephfs --yes-i-really-mean-it 删除池 查看池 [root@ceph01 ~]# ceph osd pool ls ssd-demo-pool nvme-demo-pool rbd-demo-pool rbd-k8s-pool cephfs_data cephfs_metadata 删除池 ceph osd pool delete ssd-demo-pool ssd-demo-pool --yes-i-really-really-mean-it ceph osd pool delete nvme-demo-pool nvme-demo-pool --yes-i-really-really-mean-it ceph osd pool delete rbd-demo-pool rbd-demo-pool --yes-i-really-really-mean-it ceph osd pool delete rbd-k8s-pool rbd-k8s-pool --yes-i-really-really-mean-it ceph osd pool delete cephfs_data cephfs_data --yes-i-really-really-mean-it ceph osd pool delete cephfs_metadata cephfs_metadata --yes-i-really-really-mean-it 删除OSD ceph管理节点执行 #!/bin/bash osd_list=`ceph osd ls` for var in $osd_list; do ceph osd crush rm osd.$var ceph auth del osd.$var done ceph osd down all ceph osd out all ceph osd rm all 所有ceph osd节点执行 for i in `ls /var/lib/ceph/osd/`; do umount /var/lib/ceph/osd/$i done 对于umount: /var/lib/ceph/osd/ceph-*：目标忙。的情况，执行以下操作 [root@node2 ~]# fuser -mv /var/lib/ceph/osd/ceph-1 用户 进程号 权限 命令 /var/lib/ceph/osd/ceph-1: root kernel mount /var/lib/ceph/osd/ceph-1 ceph 5979 F.... ceph-osd [root@node2 ~]# kill -9 5979 [root@node2 ~]# fuser -mv /var/lib/ceph/osd/ceph-1 用户 进程号 权限 命令 /var/lib/ceph/osd/ceph-1: root kernel mount /var/lib/ceph/osd/ceph-1 [root@node2 ~]# umount /var/lib/ceph/osd/ceph-1 手动擦除盘上数据(所有数据节点) # 从DM中移除硬盘对应的编码 dmsetup remove_all # 格式化分区 yum install gdisk -y sgdisk -z /dev/ ceph管理节点执行 ceph-deploy disk zap /dev/ 卸载组件并清空目录 ceph-deploy purge ceph01 ceph02 ceph03 ceph-deploy purgedata ceph01 ceph02 ceph03 ceph-deploy forgetkeys Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/raid/":{"url":"1.Linux基础/1.7存储/raid/","title":"raid","keywords":"","body":"Raid 简介 磁盘阵列（Redundant Arrays of Independent Drives，RAID），有“独立磁盘构成的具有冗余能力的阵列”之意。 磁盘阵列是由很多块独立的磁盘，组合成一个容量巨大的磁盘组，利用个别磁盘提供数据所产生加成效果提升整个磁盘系统效能。利用这项技术，将数据切割成许多区段，分别存放在各个硬盘上。 磁盘阵列还能利用同位检查（Parity Check）的观念，在数组中任意一个硬盘故障时，仍可读出数据，在数据重构时，将数据经计算后重新置入新硬盘中。 功能 RAID技术主要有以下三个基本功能： 通过对磁盘上的数据进行条带化，实现对数据成块存取，减少磁盘的机械寻道时间，提高了数据存取速度。 通过对一个阵列中的几块磁盘同时读取，减少了磁盘的机械寻道时间，提高数据存取速度。 通过镜像或者存储奇偶校验信息的方式，实现了对数据的冗余保护 分类 磁盘阵列其样式有三种，一是外接式磁盘阵列柜、二是内接式磁盘阵列卡，三是利用软件来仿真。 外接式磁盘阵列柜最常被使用大型服务器上，具可热交换（Hot Swap）的特性，不过这类产品的价格都很贵。 内接式磁盘阵列卡，因为价格便宜，但需要较高的安装技术，适合技术人员使用操作。硬件阵列能够提供在线扩容、动态修改阵列级别、自动数据恢复、驱动器漫游、超高速缓冲等功能。它能提供性能、数据保护、可靠性、可用性和可管理性的解决方案。阵列卡专用的处理单元来进行操作。 利用软件仿真的方式，是指通过网络操作系统自身提供的磁盘管理功能将连接的普通SCSI卡上的多块硬盘配置成逻辑盘，组成阵列。软件阵列可以提供数据冗余功能，但是磁盘子系统的性能会有所降低，有的降低幅度还比较大，达30%左右。因此会拖累机器的速度，不适合大数据流量的服务器。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/raid/01-raid0.html":{"url":"1.Linux基础/1.7存储/raid/01-raid0.html","title":"01-raid0","keywords":"","body":"RAID 0 原理 数据分片至不同的磁盘上 磁盘数量 2块以上 冗余能力 不具有冗余能力 磁盘利用率 100% 适用场景 数据安全性要求不高 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/raid/02-raid1.html":{"url":"1.Linux基础/1.7存储/raid/02-raid1.html","title":"02-raid1","keywords":"","body":"RAID 1 原理 把一个磁盘的数据镜像到另一个磁盘上 磁盘数量 偶数块（保证副本集非0） 冗余能力 具有冗余能力 磁盘利用率 50% 适用场景 保存关键性的重要数据：系统盘 实现raid1 1.虚拟机添加两块硬盘 2.安装raid管理工具mdadm yum install -y mdadm 3.查看磁盘情况 fdisk -l 4.创建raid1 -n表示副本集 mdadm -C /dev/md1 -n 2 -l 1 -a yes /dev/sd{b,c} 5.查看raid信息 cat /proc/mdstat 6.格式化 mkfs.ext4 -j -b 4096 /dev/md1 7.挂载 mkdir /mnt1 mount /dev/md1 /mnt1 echo \"/dev/md1 /mnt1 ext4 defaults 0 0\" >> /etc/fstab 8.写数据 mkdir /mnt1/abc && touch /mnt1/abc/123 9.模拟损坏其中一个磁盘块 mdadm /dev/md1 -f /dev/sdc 10.查看raid信息 11.新增磁盘设备，添加到md1 mdadm /dev/md1 -a /dev/sdd 12.查看raid信息 13.删除已损坏的硬盘 mdadm /dev/md1 -r /dev/sdc 14.关闭raid mdadm -S /dev/md1 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/raid/03-raid01.html":{"url":"1.Linux基础/1.7存储/raid/03-raid01.html","title":"03-raid01","keywords":"","body":"RAID0+1 原理 把一个磁盘的数据镜像到另一个磁盘上 磁盘数量 至少4个硬盘 冗余能力 具有冗余能力 磁盘利用率 50% 适用场景 保存关键性的重要数据 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/raid/04-raid5.html":{"url":"1.Linux基础/1.7存储/raid/04-raid5.html","title":"04-raid5","keywords":"","body":"raid5 原理 磁盘数量 至少3个硬盘 冗余能力 具有冗余能力 磁盘利用率 (N-1)/N，即只浪费一块磁盘用于奇偶校验 适用场景 保存关键性的重要数据 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.7存储/raid/05-raid10.html":{"url":"1.Linux基础/1.7存储/raid/05-raid10.html","title":"05-raid10","keywords":"","body":"raid10 Raid 10是一个Raid 1与Raid0的组合体，它是利用奇偶校验实现条带集镜像， 所以它继承了Raid0的快速和Raid1的安全。 我们知道，RAID 1在这里就是一个冗余的备份阵列，而RAID 0则负责数据的读写阵列。 其实，下图只是一种RAID 10方式，更多的情况是从主通路分出两路，做Striping操作，即把数据分割， 而这分出来的每一路则再分两路，做Mirroring操作，即互做镜像 磁盘数量 至少4个硬盘 冗余能力 具有冗余能力 磁盘利用率 50% 适用场景 保存关键性的重要数据 注意一下Raid 10和Raid 01的区别 RAID01又称为RAID0+1，先进行条带存放（RAID0），再进行镜像（RAID1）。 RAID10又称为RAID1+0，先进行镜像（RAID1），再进行条带存放（RAID0）。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.8网络/01-nat.html":{"url":"1.Linux基础/1.8网络/01-nat.html","title":"01-nat","keywords":"","body":" 清除nat规则 iptables -t nat -F 查看nat规则 iptables -t nat -nvL Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.8网络/02-路由配置.html":{"url":"1.Linux基础/1.8网络/02-路由配置.html","title":"02-路由配置","keywords":"","body":" windows route -p add 192.168.146.0 mask 255.255.255.0 192.168.121.1 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.8网络/03-wireshark.html":{"url":"1.Linux基础/1.8网络/03-wireshark.html","title":"03-wireshark","keywords":"","body":"keepalive及444状态码 keepalive 该配置官方文档给出的默认值为75s 官方文档地址 1、nginx keepalive配置方便起见配置为30s #配置于nginx.conf 中的 http{}内 keepalive_timeout 30s; 2、nginx server配置 server { listen 8089; location /123 { proxy_pass http://192.168.1.145:8080; } location / { index html/index.html; } } 3、开启wireshark监听虚拟网卡（nginx部署于本地vmware上的虚机，nat模式） 4、使用POSTMAN发送请求 5、wireshark过滤观察 keepalive与断开连接 444状态码 适用于屏蔽非安全请求或DDOS防御 1、nginx server配置 server { listen 8089; location /123 { proxy_pass http://192.168.1.145:8080; } location / { index html/index.html; } location /abc { return 444; } } 2、开启wireshark监听虚拟网卡（nginx部署于本地vmware上的虚机，nat模式） 3、发送请求 4、wireshark过滤观察 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.8网络/04-shadows.html":{"url":"1.Linux基础/1.8网络/04-shadows.html","title":"04-shadows","keywords":"","body":"服务端 yum -y install wget wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssr.sh \\ && chmod +x ssr.sh && bash ssr.sh shadowsocks客户端 项目地址 1.下载安装包 windows 2.解压，运行 加压到本地目录 3.配置 配置服务端IP、端口、加密算法、服务端口 4、配置系统模式 在任务栏找到 Shadowsocks 图标，选取PAC模式 其他使用说明 5、测试是否可用 twitter Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.8网络/05-网卡bond.html":{"url":"1.Linux基础/1.8网络/05-网卡bond.html","title":"05-网卡bond","keywords":"","body":"配置网卡Bond模式 网卡1配置 tee /etc/sysconfig/network-scripts/ifcfg-eno1 网卡2配置 tee /etc/sysconfig/network-scripts/ifcfg-ens4f0 bond0配置 tee /etc/sysconfig/network-scripts/ifcfg-bond0 重启网络 systemctl restart network Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/":{"url":"1.Linux基础/1.9安全/","title":"1.9安全","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/01-禁ping.html":{"url":"1.Linux基础/1.9安全/01-禁ping.html","title":"01-禁ping","keywords":"","body":"禁ping echo \"net.ipv4.icmp_echo_ignore_all=1\" >> /etc/sysctl.conf sysctl -p Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/02-关闭ICMP_TIMESTAMP应答.html":{"url":"1.Linux基础/1.9安全/02-关闭ICMP_TIMESTAMP应答.html","title":"02-关闭ICMP_TIMESTAMP应答","keywords":"","body":"关闭ICMP_TIMESTAMP应答 iptables -I INPUT -p ICMP --icmp-type timestamp-request -m comment --comment \"deny ICMP timestamp\" -j DROP iptables -I INPUT -p ICMP --icmp-type timestamp-reply -m comment --comment \"deny ICMP timestamp\" -j DROP Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/03-锁定系统关键文件.html":{"url":"1.Linux基础/1.9安全/03-锁定系统关键文件.html","title":"03-锁定系统关键文件","keywords":"","body":"锁定系统关键文件 防止被篡改 chattr +i /etc/group /etc/inittab /etc/services chmod 700 /etc/rc.d/init.d/* Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/04-ssh加固.html":{"url":"1.Linux基础/1.9安全/04-ssh加固.html","title":"04-ssh加固","keywords":"","body":"ssh加固 限制root用户直接登录 sed -i \"s#PermitRootLogin yes#PermitRootLogin no#g\" /etc/ssh/sshd_config systemctl restart sshd 修改允许密码错误次数 sed -i \"/MaxAuthTries/d\" /etc/ssh/sshd_config echo \"MaxAuthTries 3\" >> /etc/ssh/sshd_config systemctl restart sshd 关闭AgentForwarding和TcpForwarding sed -i \"/AgentForwarding/d\" /etc/ssh/sshd_config sed -i \"/TcpForwarding/d\" /etc/ssh/sshd_config echo \"AllowAgentForwarding no\" >> /etc/ssh/sshd_config echo \"AllowTcpForwarding no\" >> /etc/ssh/sshd_config systemctl restart sshd 关闭UseDNS sed -i \"/UseDNS/d\" /etc/ssh/sshd_config echo \"UseDNS no\" >> /etc/ssh/sshd_config systemctl restart sshd Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/05-升级sudo版本.html":{"url":"1.Linux基础/1.9安全/05-升级sudo版本.html","title":"05-升级sudo版本","keywords":"","body":"升级sudo版本 CVE-2021-3156等 sudo-1.9.7-3.el7.x86_64.rpm rpm -Uvh sudo-1.9.7-3.el7.x86_64.rpm 验证 sudo -V Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/06-设置会话超时.html":{"url":"1.Linux基础/1.9安全/06-设置会话超时.html","title":"06-设置会话超时","keywords":"","body":"设置会话超时（5分钟） 将值设置为readonly 防止用户更改 echo \"export TMOUT=300\" >>/etc/profile . /etc/profile Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/07-隐藏系统版本信息.html":{"url":"1.Linux基础/1.9安全/07-隐藏系统版本信息.html","title":"07-隐藏系统版本信息","keywords":"","body":"隐藏系统版本信息 mv /etc/issue /etc/issue.bak mv /etc/issue.net /etc/issue.net.bak Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/08-禁止Control-Alt-Delete键盘重启系统命令.html":{"url":"1.Linux基础/1.9安全/08-禁止Control-Alt-Delete键盘重启系统命令.html","title":"08-禁止Control-Alt-Delete键盘重启系统命令","keywords":"","body":"禁止Control-Alt-Delete 键盘重启系统命令 rm -rf /usr/lib/systemd/system/ctrl-alt-del.target Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/09-密码加固.html":{"url":"1.Linux基础/1.9安全/09-密码加固.html","title":"09-密码加固","keywords":"","body":"密码加固 PASS_MAX_DAYS=`grep -e ^PASS_MAX_DAYS /etc/login.defs |awk '{print $2}'` if [ $PASS_MAX_DAYS -gt 90 ];then echo \"密码最长保留期限为：$PASS_MAX_DAYS, 更改为90天\" sed -i \"/^PASS_MAX_DAYS/d\" /etc/login.defs echo \"PASS_MAX_DAYS 90\" >> /etc/login.defs fi PASS_MIN_DAYS=`grep -e ^PASS_MIN_DAYS /etc/login.defs |awk '{print $2}'` if [ $PASS_MIN_DAYS -ne 1 ];then echo \"密码最段保留期限为：$PASS_MIN_DAYS, 更改为1天\" sed -i \"/^PASS_MIN_DAYS/d\" /etc/login.defs echo \"PASS_MIN_DAYS 1\" >> /etc/login.defs fi PASS_MIN_LEN=`grep -e ^PASS_MIN_LEN /etc/login.defs |awk '{print $2}'` if [ $PASS_MIN_LEN -lt 8 ];then echo \"密码最少字符为：$PASS_MIN_LEN, 更改为8\" sed -i \"/^PASS_MIN_LEN/d\" /etc/login.defs echo \"PASS_MIN_LEN 8\" >> /etc/login.defs fi PASS_WARN_AGE=`grep -e ^PASS_WARN_AGE /etc/login.defs |awk '{print $2}'` if [ $PASS_WARN_AGE -ne 7 ];then echo \"密码到期前$PASS_MIN_LEN天提醒, 更改为7\" sed -i \"/^PASS_WARN_AGE/d\" /etc/login.defs echo \"PASS_WARN_AGE 7\" >> /etc/login.defs fi Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/10-删除无用账号.html":{"url":"1.Linux基础/1.9安全/10-删除无用账号.html","title":"10-删除无用账号","keywords":"","body":"groupdel mail groupdel games userdel adm userdel lp userdel sync userdel shutdown userdel halt userdel mail userdel operator userdel games userdel ftp Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/1.9安全/11-计划任务授权.html":{"url":"1.Linux基础/1.9安全/11-计划任务授权.html","title":"11-计划任务授权","keywords":"","body":"echo root > /etc/cron.allow rm -f /etc/{cron.deny,at.deny} echo root > /etc/cron.allow [ -e /etc/cron.allow-preCIS ] && diff /etc/cron.allow-preCIS /etc/cron.allow echo root > /etc/at.allow chown root:root /etc/{cron.allow,at.allow} chmod 400 /etc/{cron.allow,at.allow} sed -i \"s#rotate 4#rotate 30#g\" /etc/logrotate.conf systemctl restart rsyslog.service Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/00背景介绍.html":{"url":"1.Linux基础/rockylinux/00背景介绍.html","title":"00背景介绍","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/01系统安装.html":{"url":"1.Linux基础/rockylinux/01系统安装.html","title":"01系统安装","keywords":"","body":"进入安装引导（与centos安装流程一致） 选择语言 设置密码、系统分区 安装完毕、重启 总结: Rocky linux整体安装流程与CentOS基本无区别 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/02ssh相关.html":{"url":"1.Linux基础/rockylinux/02ssh相关.html","title":"02ssh相关","keywords":"","body":"开启root远程访问 不建议 当rocky linux默认安装完毕后，无法通过ssh进行root远程登录 如需开启root远程登录，变更以下配置： $ sed -i \"s;#PermitRootLogin prohibit-password;PermitRootLogin yes;g\" /etc/ssh/sshd_config $ systemctl restart sshd Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/03包管理器.html":{"url":"1.Linux基础/rockylinux/03包管理器.html","title":"03包管理器","keywords":"","body":"配置本地镜像源 挂载镜像DVD 挂载至本地 $ mount -o loop /dev/cdrom /media 配置本地镜像源 $ rm -rf /etc/yum.repos.d/* $ tee /etc/yum.repos.d/media.repo 建立缓存 $ dnf makecache 测试可用性 $ dnf install gcc -y 上次元数据过期检查：0:00:03 前，执行于 2022年08月16日 星期二 14时32分17秒。 依赖关系解决。 ================================================================================================================================================== 软件包 架构 版本 仓库 大小 ================================================================================================================================================== 安装: gcc x86_64 11.2.1-9.4.el9 media-appstream 32 M 安装依赖关系: binutils x86_64 2.35.2-17.el9 media-baseos 5.0 M binutils-gold x86_64 2.35.2-17.el9 media-baseos 735 k cpp x86_64 11.2.1-9.4.el9 media-appstream 11 M elfutils-debuginfod-client x86_64 0.186-1.el9 media-baseos 38 k glibc-devel x86_64 2.34-28.el9_0 media-appstream 29 k glibc-headers x86_64 2.34-28.el9_0 media-appstream 426 k kernel-headers x86_64 5.14.0-70.13.1.el9_0 media-appstream 1.9 M libmpc x86_64 1.2.1-4.el9 media-appstream 61 k libpkgconf x86_64 1.7.3-9.el9 media-baseos 35 k libxcrypt-devel x86_64 4.4.18-3.el9 media-appstream 28 k make x86_64 1:4.3-7.el9 media-baseos 530 k pkgconf x86_64 1.7.3-9.el9 media-baseos 40 k pkgconf-m4 noarch 1.7.3-9.el9 media-baseos 14 k pkgconf-pkg-config x86_64 1.7.3-9.el9 media-baseos 9.9 k 事务概要 ================================================================================================================================================== 安装 15 软件包 总计：51 M 安装大小：156 M 下载软件包： Rocky Linux - Media - BaseOS 1.2 MB/s | 1.7 kB 00:00 导入 GPG 公钥 0x350D275D: Userid: \"Rocky Enterprise Software Foundation - Release key 2022 \" 指纹: 21CB 256A E16F C54C 6E65 2949 702D 426D 350D 275D 来自: /etc/pki/rpm-gpg/RPM-GPG-KEY-Rocky-9 导入公钥成功 运行事务检查 事务检查成功。 运行事务测试 事务测试成功。 运行事务 准备中 : 1/1 安装 : libmpc-1.2.1-4.el9.x86_64 1/15 安装 : elfutils-debuginfod-client-0.186-1.el9.x86_64 2/15 安装 : binutils-2.35.2-17.el9.x86_64 3/15 运行脚本: binutils-2.35.2-17.el9.x86_64 3/15 安装 : binutils-gold-2.35.2-17.el9.x86_64 4/15 安装 : cpp-11.2.1-9.4.el9.x86_64 5/15 安装 : glibc-headers-2.34-28.el9_0.x86_64 6/15 安装 : kernel-headers-5.14.0-70.13.1.el9_0.x86_64 7/15 安装 : make-1:4.3-7.el9.x86_64 8/15 安装 : libpkgconf-1.7.3-9.el9.x86_64 9/15 安装 : pkgconf-1.7.3-9.el9.x86_64 10/15 安装 : pkgconf-m4-1.7.3-9.el9.noarch 11/15 安装 : pkgconf-pkg-config-1.7.3-9.el9.x86_64 12/15 安装 : glibc-devel-2.34-28.el9_0.x86_64 13/15 安装 : libxcrypt-devel-4.4.18-3.el9.x86_64 14/15 安装 : gcc-11.2.1-9.4.el9.x86_64 15/15 运行脚本: gcc-11.2.1-9.4.el9.x86_64 15/15 验证 : binutils-gold-2.35.2-17.el9.x86_64 1/15 验证 : binutils-2.35.2-17.el9.x86_64 2/15 验证 : pkgconf-pkg-config-1.7.3-9.el9.x86_64 3/15 验证 : pkgconf-m4-1.7.3-9.el9.noarch 4/15 验证 : pkgconf-1.7.3-9.el9.x86_64 5/15 验证 : libpkgconf-1.7.3-9.el9.x86_64 6/15 验证 : elfutils-debuginfod-client-0.186-1.el9.x86_64 7/15 验证 : make-1:4.3-7.el9.x86_64 8/15 验证 : libmpc-1.2.1-4.el9.x86_64 9/15 验证 : libxcrypt-devel-4.4.18-3.el9.x86_64 10/15 验证 : kernel-headers-5.14.0-70.13.1.el9_0.x86_64 11/15 验证 : gcc-11.2.1-9.4.el9.x86_64 12/15 验证 : cpp-11.2.1-9.4.el9.x86_64 13/15 验证 : glibc-headers-2.34-28.el9_0.x86_64 14/15 验证 : glibc-devel-2.34-28.el9_0.x86_64 15/15 已安装: binutils-2.35.2-17.el9.x86_64 binutils-gold-2.35.2-17.el9.x86_64 cpp-11.2.1-9.4.el9.x86_64 elfutils-debuginfod-client-0.186-1.el9.x86_64 gcc-11.2.1-9.4.el9.x86_64 glibc-devel-2.34-28.el9_0.x86_64 glibc-headers-2.34-28.el9_0.x86_64 kernel-headers-5.14.0-70.13.1.el9_0.x86_64 libmpc-1.2.1-4.el9.x86_64 libpkgconf-1.7.3-9.el9.x86_64 libxcrypt-devel-4.4.18-3.el9.x86_64 make-1:4.3-7.el9.x86_64 pkgconf-1.7.3-9.el9.x86_64 pkgconf-m4-1.7.3-9.el9.noarch pkgconf-pkg-config-1.7.3-9.el9.x86_64 完毕！ 也可使用yum进行安装 配置阿里源 $ sed -e 's|^mirrorlist=|#mirrorlist=|g' \\ -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\ -i.bak \\ /etc/yum.repos.d/rocky*.repo $ dnf makecache yum vs dnf TODO Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/04防火墙与selinux.html":{"url":"1.Linux基础/rockylinux/04防火墙与selinux.html","title":"04防火墙与selinux","keywords":"","body":"selinux关闭方式 查看状态 $ sestatus -v SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: targeted Current mode: enforcing Mode from config file: enforcing Policy MLS status: enabled Policy deny_unknown status: allowed Memory protection checking: actual (secure) Max kernel policy version: 33 Process contexts: Current context: unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 Init context: system_u:system_r:init_t:s0 /usr/sbin/sshd system_u:system_r:sshd_t:s0-s0:c0.c1023 File contexts: Controlling terminal: unconfined_u:object_r:user_devpts_t:s0 /etc/passwd system_u:object_r:passwd_file_t:s0 /etc/shadow system_u:object_r:shadow_t:s0 /bin/bash system_u:object_r:shell_exec_t:s0 /bin/login system_u:object_r:login_exec_t:s0 /bin/sh system_u:object_r:bin_t:s0 -> system_u:object_r:shell_exec_t:s0 /sbin/agetty system_u:object_r:getty_exec_t:s0 /sbin/init system_u:object_r:bin_t:s0 -> system_u:object_r:init_exec_t:s0 /usr/sbin/sshd system_u:object_r:sshd_exec_t:s0 临时关闭 $ setenforce 0 永久关闭(重启系统后生效) $ sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 关闭防火墙 $ systemctl disable firewalld --now Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/05安装docker.html":{"url":"1.Linux基础/rockylinux/05安装docker.html","title":"05安装docker","keywords":"","body":"基于二进制离线安装docker 下载最新二进制文件： https://download.docker.com/linux/static/stable/x86_64/ 如：https://download.docker.com/linux/static/stable/x86_64/docker-20.10.17.tgz 解压并配置到系统PATH $ tar zxvf docker-20.10.17.tgz $ cp docker/* /usr/bin/ 配置dockerd $ tee /usr/lib/systemd/system/docker.service 配置containerd $ tee /etc/systemd/system/containerd.service 生成docker.socket $ tee /etc/systemd/system/docker.socket 启动containerd、dockerd $ groupadd docker $ systemctl enable --now containerd.service $ systemctl enable --now docker.service 测试docker $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE 安装docker-compose 下载二进制文件 https://github.com/docker/compose/releases/download/v2.9.0/docker-compose-linux-x86_64 拷贝到bin目录 $ cp docker-compose-linux-x86_64 /usr/bin/docker-compose $ chmod +x /usr/bin/docker-compose Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/05安装redis.html":{"url":"1.Linux基础/rockylinux/05安装redis.html","title":"05安装redis","keywords":"","body":"基于源代码编译安装 该种方式跨平台 下载源码包： https://download.redis.io/releases/redis-5.0.14.tar.gz 安装编译依赖 $ yum install -y gcc 编译安装 $ tar zxvf redis-5.0.14.tar.gz $ cd redis-5.0.14 $ make && make install 启动 $ /usr/local/bin/redis-server redis.conf 7181:C 17 Aug 2022 09:30:39.730 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 7181:C 17 Aug 2022 09:30:39.730 # Redis version=5.0.12, bits=64, commit=00000000, modified=0, pid=7181, just started 7181:C 17 Aug 2022 09:30:39.730 # Configuration loaded 7181:M 17 Aug 2022 09:30:39.731 * Increased maximum number of open files to 10032 (it was originally set to 1024). _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 5.0.14 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 7181 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 7181:M 17 Aug 2022 09:30:39.732 # Server initialized 7181:M 17 Aug 2022 09:30:39.732 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 7181:M 17 Aug 2022 09:30:39.732 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 7181:M 17 Aug 2022 09:30:39.732 * Ready to accept connections Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/06安装nginx.html":{"url":"1.Linux基础/rockylinux/06安装nginx.html","title":"06安装nginx","keywords":"","body":"基于源代码编译安装 该种方式跨平台 下载源码包： http://nginx.org/download/nginx-1.22.0.tar.gz 安装编译依赖 $ yum install -y gcc pcre-devel zlib-devel 编译安装 $ tar zxvf nginx-1.22.0.tar.gz $ cd nginx-1.22.0 $ ./configure --prefix=/etc/nginx $ make && make install 启动 $ /etc/nginx/sbin/nginx $ ps -ef|grep nginx root 11230 1 0 09:36 ? 00:00:00 nginx: master process /etc/nginx/sbin/nginx nobody 11231 11230 0 09:36 ? 00:00:00 nginx: worker process root 11237 2920 0 09:37 pts/1 00:00:00 grep --color=auto nginx Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/07安装jdk.html":{"url":"1.Linux基础/rockylinux/07安装jdk.html","title":"07安装jdk","keywords":"","body":"安装oracle jdk 该种方式跨平台 准备安装包： jdk-8u281-linux-x64.tar.gz 解压 $ http://192.168.174.80:9998/download/deploy/soft/jdk/jdk-8u281-linux-x64.tar.gz 解压安装 $ tar zxvf jdk-8u281-linux-x64.tar.gz -C /usr/local/ $ echo \"JAVA_HOME=/usr/local/jdk1.8.0_281\" >> ~/.bash_profile $ echo \"export PATH=\\$PATH:/usr/local/jdk1.8.0_281/bin\" >> ~/.bash_profile $ source ~/.bash_profile 测试 $ java -version java version \"1.8.0_281\" Java(TM) SE Runtime Environment (build 1.8.0_281-b09) Java HotSpot(TM) 64-Bit Server VM (build 25.281-b09, mixed mode) $ java -jar golf-cloud-eureka-1.0.0-SNAPSHOT.jar 2022-08-17 09:45:59.230 INFO 11265 --- [ main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration' of type [org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration$$EnhancerBySpringCGLIB$$fe6a1bcc] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying) . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.1.6.RELEASE) 2022-08-17 09:45:59.798 INFO 11265 --- [ main] c.n.g.c.eureka.EurekaServerApplication : The following profiles are active: dev 2022-08-17 09:46:02.041 WARN 11265 --- [ main] o.s.boot.actuate.endpoint.EndpointId : Endpoint ID 'service-registry' contains invalid characters, please migrate to a valid format. 2022-08-17 09:46:03.936 WARN 11265 --- [ main] c.n.c.sources.URLConfigurationSource : No URLs will be polled as dynamic configuration sources. 2022-08-17 09:46:06.416 WARN 11265 --- [ main] c.n.c.sources.URLConfigurationSource : No URLs will be polled as dynamic configuration sources. 2022-08-17 09:46:08.780 INFO 11265 --- [ main] c.n.g.c.eureka.EurekaServerApplication : Started EurekaServerApplication in 12.333 seconds (JVM running for 13.134) 安装openjdk 预编译包 安装 $ yum install -y java-1.8.0-openjdk-devel.x86_64 测试 $ java -version openjdk version \"1.8.0_332\" OpenJDK Runtime Environment (build 1.8.0_332-b09) OpenJDK 64-Bit Server VM (build 25.332-b09, mixed mode) $ java -jar golf-cloud-eureka-1.0.0-SNAPSHOT.jar 2022-08-17 09:45:59.230 INFO 11265 --- [ main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration' of type [org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration$$EnhancerBySpringCGLIB$$fe6a1bcc] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying) . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.1.6.RELEASE) 2022-08-17 09:45:59.798 INFO 11265 --- [ main] c.n.g.c.eureka.EurekaServerApplication : The following profiles are active: dev 2022-08-17 09:46:02.041 WARN 11265 --- [ main] o.s.boot.actuate.endpoint.EndpointId : Endpoint ID 'service-registry' contains invalid characters, please migrate to a valid format. 2022-08-17 09:46:03.936 WARN 11265 --- [ main] c.n.c.sources.URLConfigurationSource : No URLs will be polled as dynamic configuration sources. 2022-08-17 09:46:06.416 WARN 11265 --- [ main] c.n.c.sources.URLConfigurationSource : No URLs will be polled as dynamic configuration sources. 2022-08-17 09:46:08.780 INFO 11265 --- [ main] c.n.g.c.eureka.EurekaServerApplication : Started EurekaServerApplication in 12.333 seconds (JVM running for 13.134) Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/08安装mysql.html":{"url":"1.Linux基础/rockylinux/08安装mysql.html","title":"08安装mysql","keywords":"","body":"安装mysql 基于二进制方式 准备安装包： https://downloads.mysql.com/archives/get/p/23/file/mysql-5.7.38-linux-glibc2.12-x86_64.tar.gz 解压 tar zxvf mysql-5.7.38-linux-glibc2.12-x86_64.tar.gz -C /usr/local 创建用户授权 useradd mysql mv /usr/local/mysql-5.7.38-linux-glibc2.12-x86_64 /usr/local/mysql chown mysql:mysql -R /usr/local/mysql echo \"export PATH=\\$PATH:/usr/local/mysql/bin\" >> ~/.bash_profile source ~/.bash_profile 创建配置文件 $ tee /etc/my.cnf 创建所需目录 mkdir -p /var/run/mysqld chown mysql:mysql /var/run/mysqld touch /var/log/mysqld.log /var/log/slow-query.log chown mysql:mysql /var/log/mysqld.log chown mysql:mysql /var/log/slow-query.log 创建service $ tee /usr/lib/systemd/system/mysqld.service 初始化数据 /usr/local/mysql/bin/mysql_install_db --user=mysql --datadir=/var/lib/mysql 启动mysql $ systemctl enable --now mysqld 查看状态 $ systemctl status mysqld ● mysqld.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2022-08-17 10:47:31 CST; 11s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Process: 13894 ExecStart=/usr/local/mysql/bin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid (code=exited, status=0/SUCCESS) Main PID: 13896 (mysqld) Tasks: 30 (limit: 23408) Memory: 400.3M CPU: 307ms CGroup: /system.slice/mysqld.service └─13896 /usr/local/mysql/bin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid 8月 17 10:47:31 localhost.localdomain systemd[1]: Starting MySQL Server... 8月 17 10:47:31 localhost.localdomain systemd[1]: Started MySQL Server. 建立动态库链接 $ ln -s /usr/lib64/libncurses.so.6 /usr/lib64/libncurses.so.5 $ ln -s /usr/lib64/libtinfo.so.6 /usr/lib64/libtinfo.so.5 初始化root口令测试 $ systemctl stop mysqld $ mysqld_safe --skip-grant-tables & $ mysql -u root Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 3 Server version: 5.7.38-log MySQL Community Server (GPL) Copyright (c) 2000, 2022, Oracle and/or its affiliates. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql> use mysql Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> update user set authentication_string = password(\"Mysql@d523\") WHERE user='root'; Query OK, 1 row affected, 1 warning (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 1 mysql> FLUSH PRIVILEGES; Query OK, 0 rows affected (0.01 sec) mysql> exit Bye Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/09安装ks.html":{"url":"1.Linux基础/rockylinux/09安装ks.html","title":"09安装ks","keywords":"","body":"离线安装ks v3.3.0 环境准备 环境说明： 操作系统：Rocky Linux release 9.0 (Blue Onyx) 私有镜像库: harbor（http -> http://192.168.1.3） 样例主机IP：192.168.1.2 样例主机配置: 8C 16G 100G 镜像准备 下载以下镜像并导入本地私有harbor库内（创建对应的project，如kubesphere/openpitrix-jobs:v3.2.1需要创建kubesphere项目并设置公开权限 ） kubesphere/openpitrix-jobs:v3.2.1 kubesphere/kube-apiserver:v1.23.9 kubesphere/kube-controller-manager:v1.23.9 kubesphere/kube-proxy:v1.23.9 kubesphere/kube-scheduler:v1.23.9 openebs/provisioner-localpv:3.3.0 openebs/linux-utils:3.3.0 kubesphere/ks-installer:v3.3.0 calico/kube-controllers:v3.23.2 calico/cni:v3.23.2 calico/pod2daemon-flexvol:v3.23.2 calico/node:v3.23.2 kubesphere/ks-controller-manager:v3.3.0 kubesphere/ks-apiserver:v3.3.0 kubesphere/ks-console:v3.3.0 kubesphere/ks-jenkins:v3.3.0-2.319.1 kubesphere/fluent-bit:v1.8.11 kubesphere/s2ioperator:v3.2.1 argoproj/argocd:v2.3.3 kubesphere/prometheus-config-reloader:v0.55.1 kubesphere/prometheus-operator:v0.55.1 prom/prometheus:v2.34.0 kubesphere/fluentbit-operator:v0.13.0 argoproj/argocd-applicationset:v0.4.1 kubesphere/kube-events-ruler:v0.4.0 kubesphere/kube-events-operator:v0.4.0 kubesphere/kube-events-exporter:v0.4.0 kubesphere/elasticsearch-oss:6.8.22 kubesphere/kube-state-metrics:v2.3.0 prom/node-exporter:v1.3.1 library/redis:6.2.6-alpine dexidp/dex:v2.30.2 library/alpine:3.14 kubesphere/kubectl:v1.22.0 kubesphere/notification-manager:v1.4.0 jaegertracing/jaeger-operator:1.27 coredns/coredns:1.8.6 jaegertracing/jaeger-collector:1.27 jaegertracing/jaeger-query:1.27 jaegertracing/jaeger-agent:1.27 kubesphere/notification-tenant-sidecar:v3.2.0 kubesphere/notification-manager-operator:v1.4.0 kubesphere/pause:3.6 prom/alertmanager:v0.23.0 istio/pilot:1.11.1 kubesphere/kube-auditing-operator:v0.2.0 kubesphere/kube-auditing-webhook:v0.2.0 kubesphere/kube-rbac-proxy:v0.11.0 kubesphere/kiali-operator:v1.38.1 kubesphere/kiali:v1.38 kubesphere/metrics-server:v0.4.2 jimmidyson/configmap-reload:v0.5.0 csiplugin/snapshot-controller:v4.0.0 kubesphere/kube-rbac-proxy:v0.8.0 library/docker:19.03 kubesphere/log-sidecar-injector:1.1 osixia/openldap:1.3.0 kubesphere/k8s-dns-node-cache:1.15.12 minio/mc:RELEASE.2019-08-07T23-14-43Z minio/minio:RELEASE.2019-08-07T01-59-21Z mirrorgooglecontainers/defaultbackend-amd64:1.4 介质准备 下载离线必需安装介质 kubekey-v2.3.0-rc.1-linux-amd64.tar.gz kubekey-v2.3.0-rc.1-linux-amd64.tar.gz helm-v3.9.0-linux-amd64.tar.gz kubeadm kubectl kubelet helm-v3.9.0-linux-amd64.tar.gz cni-plugins-linux-amd64-v0.9.1.tgz crictl-v1.24.0-linux-amd64.tar.gz etcd-v3.4.13-linux-amd64.tar.gz docker-20.10.8.tgz 创建工作目录，上传安装介质 文件目录结构如下 $ /work ├── kubekey │ ├── cni │ │ └── v0.9.1 │ │ └── amd64 │ │ └── cni-plugins-linux-amd64-v0.9.1.tgz │ ├── crictl │ │ └── v1.24.0 │ │ └── amd64 │ │ └── crictl-v1.24.0-linux-amd64.tar.gz │ ├── docker │ │ └── 20.10.8 │ │ └── amd64 │ │ └── docker-20.10.8.tgz │ ├── etcd │ │ └── v3.4.13 │ │ └── amd64 │ │ └── etcd-v3.4.13-linux-amd64.tar.gz │ ├── helm │ │ └── v3.9.0 │ │ └── amd64 │ │ └── helm-v3.9.0-linux-amd64.tar.gz │ ├── kube │ │ └── v1.23.9 │ │ └── amd64 │ │ ├── kubeadm │ │ ├── kubectl │ │ └── kubelet └── kubekey-v2.3.0-rc.1-linux-amd64.tar.gz 部分介质解压 $ cd /work/kubekey/helm/v3.9.0/amd64 && tar -zxf helm-v3.9.0-linux-amd64.tar.gz && mv linux-amd64/helm . && rm -rf *linux-amd64* && cd - $ cd /work && tar zxvf kubekey-v2.3.0-rc.1-linux-amd64.tar.gz 配置本地镜像源 挂载镜像DVD 挂载至本地 $ mount -o loop /dev/cdrom /media 配置本地镜像源 $ rm -rf /etc/yum.repos.d/* $ tee /etc/yum.repos.d/media.repo 建立缓存 $ dnf makecache 安装部署 安装依赖 $ dnf install conntrack socat chrony ipvsadm -y 初始化配置文件 $ ./kk create config --with-kubesphere v3.3.0 调整配置 样例信息已脱敏，仅作说明使用，变更内容如下： 配置hosts节点与角色组(hosts、roleGroups) 配置私有镜像库（privateRegistry、insecureRegistries） 注释掉controlPlaneEndpoint 开启以下组件： alerting auditing devops events logging metrics_server openpitrix servicemesh apiVersion: kubekey.kubesphere.io/v1alpha2 kind: Cluster metadata: name: sample spec: hosts: - {name: node1, address: 172.16.0.2, internalAddress: 172.16.0.2, user: ubuntu, password: \"Qcloud@123\"} roleGroups: etcd: - node1 control-plane: - node1 worker: - node1 #controlPlaneEndpoint: ## Internal loadbalancer for apiservers # internalLoadbalancer: haproxy # domain: lb.kubesphere.local # address: \"\" # port: 6443 kubernetes: version: v1.23.9 clusterName: cluster.local autoRenewCerts: true containerManager: docker etcd: type: kubekey network: plugin: calico kubePodsCIDR: 10.233.64.0/18 kubeServiceCIDR: 10.233.0.0/18 ## multus support. https://github.com/k8snetworkplumbingwg/multus-cni multusCNI: enabled: false registry: privateRegistry: \"harbor.wl.io\" namespaceOverride: \"\" registryMirrors: [] insecureRegistries: [\"harbor.wl.io\"] addons: [] 配置harbor host解析 由于habror使用的是假域名，需要配置自定义解析 $ echo \"192.168.1.3 harbor.wl.io\" >> /etc/hosts 创建dns配置文件 $ touch /etc/resolv.conf 否则初始化沙箱会异常 $ Sep 15 08:48:39 node1 kubelet[35254]: E0915 08:48:39.708357 35254 pod_workers.go:951] \"Error syncing pod, skipping\" err=\"failed to \\\"CreatePodSandbox\\\" for \\\"kube-scheduler-node1_kube-system(868ca46a733b98e2a3523d80b3c75243)\\\" with CreatePodSandboxError: \\\"Failed to generate sandbox config for pod \\\\\\\"kube-scheduler-node1_kube-system(868ca46a733b98e2a3523d80b3c75243)\\\\\\\": open /etc/resolv.conf: no such file or directory\\\"\" pod=\"kube-system/kube-scheduler-node1\" podUID=868ca46a733b98e2a3523d80b3c75243 初始化集群 $ ./kk create cluster --with-kubesphere v3.3.0 -f config-sample.yaml -y 安装补全 $ dnf install -y bash-completion $ source /usr/share/bash-completion/bash_completion $ source > ~/.bashrc 配置内网dns（可选） 设置DNS $ nmcli connection modify ens192 ipv4.dns \"10.10.1.254\" $ nmcli connection up ens192 ens192: 网卡名称 10.10.1.254: dns地址 配置core dns解析 加入自定义Host $ kubectl edit configmap coredns -n kube-system 修改前： apiVersion: v1 data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } kind: ConfigMap metadata: creationTimestamp: \"2022-09-15T00:48:59Z\" name: coredns namespace: kube-system resourceVersion: \"232\" uid: 4a4a69f2-b151-4323-b5b2-ae9d2867e58f 修改后 apiVersion: v1 data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 hosts { 192.168.1.3 harbor.wl.io fallthrough } forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } kind: ConfigMap metadata: creationTimestamp: \"2022-09-15T00:48:59Z\" name: coredns namespace: kube-system resourceVersion: \"232\" uid: 4a4a69f2-b151-4323-b5b2-ae9d2867e58f 重载 $ kubectl rollout restart deploy coredns -n kube-system 修改nodelocaldns $ kubectl edit cm -n kube-system nodelocaldns 修改前 apiVersion: v1 data: Corefile: | cluster.local:53 { errors cache { success 9984 30 denial 9984 5 } reload loop bind 169.254.25.10 forward . 10.233.0.3 { force_tcp } prometheus :9253 health 169.254.25.10:9254 } in-addr.arpa:53 { errors cache 30 reload loop bind 169.254.25.10 forward . 10.233.0.3 { force_tcp } prometheus :9253 } ip6.arpa:53 { errors cache 30 reload loop bind 169.254.25.10 forward . 10.233.0.3 { force_tcp } prometheus :9253 } .:53 { errors cache 30 reload loop bind 169.254.25.10 forward . /etc/resolv.conf prometheus :9253 } kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"data\":{\"Corefile\":\"cluster.local:53 {\\n errors\\n cache {\\n success 9984 30\\n denial 9984 5\\n }\\n reload\\n loop\\n bind 169.254.25.10\\n forward . 10.233.0.3 {\\n force_tcp\\n }\\n prometheus :9253\\n health 169.254.25.10:9254\\n}\\nin-addr.arpa:53 {\\n errors\\n cache 30\\n reload\\n loop\\n bind 169.254.25.10\\n forward . 10.233.0.3 {\\n force_tcp\\n }\\n prometheus :9253\\n}\\nip6.arpa:53 {\\n errors\\n cache 30\\n reload\\n loop\\n bind 169.254.25.10\\n forward . 10.233.0.3 {\\n force_tcp\\n }\\n prometheus :9253\\n}\\n.:53 {\\n errors\\n cache 30\\n reload\\n loop\\n bind 169.254.25.10\\n forward . /etc/resolv.conf\\n prometheus :9253\\n}\\n\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"labels\":{\"addonmanager.kubernetes.io/mode\":\"EnsureExists\"},\"name\":\"nodelocaldns\",\"namespace\":\"kube-system\"}} creationTimestamp: \"2022-09-15T00:49:03Z\" labels: addonmanager.kubernetes.io/mode: EnsureExists name: nodelocaldns namespace: kube-system resourceVersion: \"368\" uid: adb09cd0-b5c1-4939-98bf-b48bfb5418ce 修改后 apiVersion: v1 data: Corefile: | cluster.local:53 { errors cache { success 9984 30 denial 9984 5 } reload loop bind 169.254.25.10 forward . 10.233.0.3 { force_tcp } prometheus :9253 health 169.254.25.10:9254 } in-addr.arpa:53 { errors cache 30 reload loop bind 169.254.25.10 forward . 10.233.0.3 { force_tcp } prometheus :9253 } ip6.arpa:53 { errors cache 30 reload loop bind 169.254.25.10 forward . 10.233.0.3 { force_tcp } prometheus :9253 } .:53 { errors cache 30 reload loop bind 169.254.25.10 forward . 10.233.0.3 { force_tcp } prometheus :9253 } kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"data\":{\"Corefile\":\"cluster.local:53 {\\n errors\\n cache {\\n success 9984 30\\n denial 9984 5\\n }\\n reload\\n loop\\n bind 169.254.25.10\\n forward . 10.233.0.3 {\\n force_tcp\\n }\\n prometheus :9253\\n health 169.254.25.10:9254\\n}\\nin-addr.arpa:53 {\\n errors\\n cache 30\\n reload\\n loop\\n bind 169.254.25.10\\n forward . 10.233.0.3 {\\n force_tcp\\n }\\n prometheus :9253\\n}\\nip6.arpa:53 {\\n errors\\n cache 30\\n reload\\n loop\\n bind 169.254.25.10\\n forward . 10.233.0.3 {\\n force_tcp\\n }\\n prometheus :9253\\n}\\n.:53 {\\n errors\\n cache 30\\n reload\\n loop\\n bind 169.254.25.10\\n forward . /etc/resolv.conf\\n prometheus :9253\\n}\\n\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{},\"labels\":{\"addonmanager.kubernetes.io/mode\":\"EnsureExists\"},\"name\":\"nodelocaldns\",\"namespace\":\"kube-system\"}} creationTimestamp: \"2022-09-15T00:49:03Z\" labels: addonmanager.kubernetes.io/mode: EnsureExists name: nodelocaldns namespace: kube-system resourceVersion: \"6905\" uid: adb09cd0-b5c1-4939-98bf-b48bfb5418ce 即修改.:53 {}块内容 重载 $ kubectl rollout restart ds nodelocaldns -n kube-system Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/10配置chrony.html":{"url":"1.Linux基础/rockylinux/10配置chrony.html","title":"10配置chrony","keywords":"","body":"配置chrony $ yum install -y chrony $ cat > /etc/chrony.conf 启动 $ systemctl enable chronyd --now 手动同步 timedatectl set-ntp true Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/rockylinux/系统软件版本信息.html":{"url":"1.Linux基础/rockylinux/系统软件版本信息.html","title":"系统软件版本信息","keywords":"","body":"rocky9 openssl: 3.0.1 openssh: OpenSSH_8.7p1 kernel: 5.14.0-70.13.1.el9_0.x86_64 cgroup: v2 CentOS7 Rocky Linux9 openssl OpenSSL 1.0.2k-fips 26 Jan 2017 OpenSSL 3.0.1 14 Dec 2021 openssh OpenSSH_7.4p1, OpenSSL 1.0.2k-fips 26 Jan 2017 OpenSSH_8.7p1, OpenSSL 3.0.1 14 Dec 2021 kernel 3.10.0-957.12.2.el7.x86_64 5.14.0-70.13.1.el9_0.x86_64 cgroup v1 v2 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/ssl/ssl.html":{"url":"1.Linux基础/ssl/ssl.html","title":"ssl","keywords":"","body":"查看目标地址套件列表 openssl s_client -connect -tls1_2 https://blog.csdn.net/zclmoon/article/details/132219226 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"1.Linux基础/skill.html":{"url":"1.Linux基础/skill.html","title":"skill","keywords":"","body":"技巧 linux读取移动硬盘数据 安装ntfs-3g yum -y install ntfs-3g 或离线安装，离线包下载地址 yum -y install gcc tar -zxvf ntfs-3g_ntfsprogs-2017.3.23.tgz cd ntfs-3g_ntfsprogs-2017.3.23/ ./configure && make && make install 查询移动硬盘所在设备接口 [root@node3 windows]# fdisk -l | grep NTFS WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. /dev/sdl1 2048 3907026943 1953512448 7 HPFS/NTFS/exFAT 创建挂载点，挂载 mkdir -p /ntfs mount -t ntfs-3g /dev/sdl1 /ntfs windows任务栏卡死 任务栏关闭资讯和兴趣 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/cni/cilium/01介绍Cilium与Hubble.html":{"url":"2.容器/cni/cilium/01介绍Cilium与Hubble.html","title":"01介绍Cilium与Hubble","keywords":"","body":"Cilium是什么？ Introduction to Cilium & Hubble Cilium是一款开源软件，用于透明地保护使用Docker和Kubernetes等Linux容器管理平台部署的应用程序服务之间的网络连接。 Cilium基于一种名为eBPF的新的Linux内核技术实现，它支持在Linux本身中动态插入强大的安全可见性和控制逻辑。 因为eBPF在Linux内核内部运行，所以，可以无需更改应用程序代码或容器配置应用便可更新Cilium安全策略。 Hubble是什么？ Hubble是一个完全分布式的网络和安全可观测平台。它构建在Cilium和eBPF之上，以完全透明的方式支持对服务的通信和行为以及网络基础设施的深度可见性。 通过在Cilium的顶部建造，Hubble可以利用eBPF来提高能见度。 通过依赖于eBPF，所有的可见性都是可编程的，并允许采用动态方法，在根据用户的要求提供深度和详细的可见性的同时最小化开销。 Hubble的诞生和设计就是为了充分利用eBPF的新能力。 Hubble具备以下能力： 服务依赖关系和通信映射 哪些服务在相互通信?通信频率?服务依赖关系图是什么样子的? 正在进行哪些HTTP调用?服务从哪些Kafka主题中消费或产生? 网络监控&告警 是否有网络通信故障?为什么通信失败了?DNS存在问题吗?是应用程序问题还是网络问题?通信中断是在第4层(TCP)还是第7层(HTTP)? 最近5分钟内，哪些服务出现DNS解析问题?哪些服务最近经历过TCP连接中断或连接超时?TCP SYN请求的未应答率是多少? 应用程序监控 对于特定服务或跨所有集群的5xx或4xx HTTP响应码的比率是多少? 集群内，HTTP请求和响应之间的第95和99个百分位延迟是多少?哪些服务表现最差?两个服务之间的延迟是多少? 安全可观测性 由于网络策略，哪些服务的连接被阻塞?从集群外部访问了哪些服务?哪些服务解析了特定的DNS名称? 为什么要选择Cilium与Hubble? eBPF在粒度和效率上实现了对系统和应用程序的可观测性和控制，这在以前是不可能的。 它以一种完全透明的方式做到这一点，不需要应用程序以任何方式进行更改。 eBPF同样能够很好地处理现代的容器化工作负载以及更传统的工作负载(如虚拟机和标准Linux进程)。 现代数据中心应用程序的开发已转向面向服务的体系结构，通常称为微服务，其中大型应用程序被分割为小型独立服务，这些服务通过使用HTTP等轻量级协议的api彼此通信。 微服务应用程序往往是高度动态的，随着应用程序向外/向内扩展以适应负载变化，以及在作为持续交付的一部分部署的滚动更新期间，单个容器被启动或销毁。 这种向高度动态微服务的转变在确保微服务之间的连接方面既是挑战，也是机遇。 传统的Linux网络安全方法(例如，iptables)过滤IP地址和TCP/UDP端口，但IP地址在动态微服务环境中经常变动。 容器的高度不稳定的生命周期导致这些方法难以与应用程序同时扩展，因为负载平衡表和访问控制列表包含数十万条规则，需要以不断增长的频率更新。 协议端口(例如用于HTTP流量的TCP端口80)不再用于出于安全目的区分应用程序流量，因为该端口用于跨服务的大量消息。 另一个挑战是提供精确可见性的能力，因为传统系统使用IP地址作为主要识别工具，这可能会大大缩短微服务体系结构中的生命周期，只有几秒钟。 通过利用Linux eBPF, Cilium保留了透明插入安全可见性和强制的能力，但这是以一种基于服务/pod/容器标识(与传统系统中的IP地址标识相反)的方式进行的， 并且可以在应用层(例如HTTP)进行过滤。因此，Cilium不仅通过解耦安全性与寻址使得在高度动态的环境中应用安全策略变得简单， 而且除了提供传统的第3层和第4层分割之外，还可以通过在http层操作提供更强的安全隔离。 eBPF的使用使Cilium能够以一种即使在大规模环境中也具有高度可伸缩性的方式实现所有这些。（对比iptables等） 功能概述 透明地保护API 能够保护现代应用协议，如REST/HTTP, gRPC和Kafka。传统的防火墙在第3层和第4层运行。 在特定端口上运行的协议要么是完全信任的，要么是完全阻塞的。Cilium提供了过滤单个应用协议请求的能力，例如: 允许所有使用GET方法和/public/.*路径的HTTP请求，拒绝所有其他请求。 允许service1在Kafka主题topic1上生产，允许service2在topic1上消费，拒绝所有其他Kafka消息。 要求HTTP报头X-Token:[0-9]+出现在所有REST调用中。 基于身份的服务与服务之间的安全通信 现代分布式应用程序依赖于容器等技术来促进部署的敏捷性和按需向外扩展。这将导致在短时间内启动大量应用程序容器。 典型的容器防火墙通过对源IP地址和目的端口进行过滤来保护工作负载。这个概念要求在集群中的任何位置启动容器时，都要操纵所有服务器上的防火墙。 为了避免这种限制规模的情况，Cilium将一个安全标识分配给共享相同安全策略的应用程序容器组。 然后，该标识与应用程序容器发出的所有网络数据包相关联，允许在接收节点验证该标识。安全标识管理使用键值存储来执行。 对外部服务的安全访问 基于标签的安全性是集群内部访问控制的首选工具。 为了保证对外部服务的安全访问，支持基于传统CIDR的入口和出口安全策略。这允许将对应用程序容器的访问限制在特定的IP范围内。 简单网络 具有跨多个集群能力的简单平面第三层网络连接了所有应用程序容器。通过使用主机作用域分配器，IP分配变得简单。这意味着每台主机无需协调就可以分配ip。 支持以下多节点组网模式: 1.Overlay: 基于封装的虚拟网络，跨越所有主机。 使用场景: 该模式对基础设施和集成的需求最小。它适用于几乎所有的网络基础设施，因为唯一的要求是主机之间的IP连接。 2.本地路由: Linux主机的常规路由表的使用。网络需要能够路由应用程序容器的IP地址。 使用场景: 此模式适用于高级用户，需要了解底层网络基础设施。此模式适用于: IPv6 结合云网络路由器 如果您已经在运行路由守护进程 负载均衡 Cilium为应用程序容器之间和与外部服务之间的通信实现分布式负载平衡，并且能够完全替换kube-proxy等组件。 负载平衡是在eBPF中使用高效的哈希表实现的，允许几乎无限的规模。 带宽管理 Cilium通过高效的EDT-based (最早出发时间)的速率限制实现带宽管理，并使用eBPF对出口节点的容器流量进行限速。 相比于传统的方法，例如在带宽CNI插件中使用的HTB(层次令牌桶)或TBF(令牌桶过滤器)，Cilium显著降低应用程序的传输尾延迟，并避免在多队列网卡下的锁定。 监控和故障诊断 提供优于tcpdump和ping网络诊断方式: 基于元数据的事件监控：当数据包被丢弃时，该工具不仅报告数据包的源和目的IP，还提供发送方和接收方的完整标签信息和许多其他信息。 关键指标通过Prometheus导出，以便与现有的仪表板集成。 Hubble：专门为Cilium编写的可观察性平台。它基于流日志提供服务依赖关系映射、操作监视和警报，以及应用程序和安全可见性。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/cni/cilium/02组件概览.html":{"url":"2.容器/cni/cilium/02组件概览.html","title":"02组件概览","keywords":"","body":"cilium组件介绍 cilium架构图 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/cni/cilium/cilium替换calico.html":{"url":"2.容器/cni/cilium/cilium替换calico.html","title":"cilium替换calico","keywords":"","body":"# kubectl -n kube-system delete ds calico-node kubectl -n kube-system delete deploy calico-kube-controllers kubectl -n kube-system delete sa calico-node kubectl -n kube-system delete sa calico-kube-controllers kubectl -n kube-system delete cm calico-config kubectl -n kube-system delete secret calico-config kubectl get crd | grep calico | awk '{print $1}' | xargs kubectl delete crd helm install cilium devops/cilium --version 1.12.3 \\ --namespace kube-system\\ --set hubble.metrics.enabled=\"{dns:query;ignoreAAAA;destinationContext=pod-short,drop:sourceContext=pod;destinationContext=pod,tcp,flow,port-distribution,icmp,http}\" \\ --set hubble.relay.enabled=true \\ --set hubble.ui.enabled=true [root@node1 ~]# kubectl -n kube-system get svc hubble-ui NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hubble-ui ClusterIP 10.233.21.93 80/TCP 18s [root@node1 ~]# kubectl -n kube-system patch svc hubble-ui -p '{\"spec\": {\"type\": \"NodePort\"}}' service/hubble-ui patched [root@node1 ~]# kubectl -n kube-system get svc hubble-ui NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hubble-ui NodePort 10.233.21.93 80:30065/TCP 28s https://www.cnblogs.com/dudu/p/16269093.html 查看模式 $ kubectl exec -it -n kube-system ds/cilium -- cilium status | grep KubeProxyReplacement 查看服务列表 $ kubectl exec -it -n kube-system daemonset/cilium -- cilium service list 删除iptables $ iptables-save | grep -v KUBE | iptables-restore $ # 首先备份 kube-system ConfigMap $ kubectl get cm kube-proxy -n kube-system -o yaml > kube-proxy-cm.yaml $ kubectl -n kube-system delete ds kube-proxy $ kubectl -n kube-system delete cm kube-proxy Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/cni/cilium/ddd.html":{"url":"2.容器/cni/cilium/ddd.html","title":"ddd","keywords":"","body":"cat apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: clusterconfigurations.installer.kubesphere.io spec: group: installer.kubesphere.io versions: name: v1alpha1 served: true storage: true schema: openAPIV3Schema:type: object properties: spec: type: object x-kubernetes-preserve-unknown-fields: true status: type: object x-kubernetes-preserve-unknown-fields: true scope: Namespaced names: plural: clusterconfigurations singular: clusterconfiguration kind: ClusterConfiguration shortNames: cc apiVersion: v1 kind: Namespace metadata: name: kubesphere-system apiVersion: v1 kind: ServiceAccount metadata: name: ks-installer namespace: kubesphere-system apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ks-installer rules: apiGroups: \"\" resources: '*' verbs: '*' apiGroups: apps resources: '*' verbs: '*' apiGroups: extensions resources: '*' verbs: '*' apiGroups: batch resources: '*' verbs: '*' apiGroups: rbac.authorization.k8s.io resources: '*' verbs: '*' apiGroups: apiregistration.k8s.io resources: '*' verbs: '*' apiGroups: apiextensions.k8s.io resources: '*' verbs: '*' apiGroups: tenant.kubesphere.io resources: '*' verbs: '*' apiGroups: certificates.k8s.io resources: '*' verbs: '*' apiGroups: devops.kubesphere.io resources: '*' verbs: '*' apiGroups: monitoring.coreos.com resources: '*' verbs: '*' apiGroups: logging.kubesphere.io resources: '*' verbs: '*' apiGroups: jaegertracing.io resources: '*' verbs: '*' apiGroups: storage.k8s.io resources: '*' verbs: '*' apiGroups: admissionregistration.k8s.io resources: '*' verbs: '*' apiGroups: policy resources: '*' verbs: '*' apiGroups: autoscaling resources: '*' verbs: '*' apiGroups: networking.istio.io resources: '*' verbs: '*' apiGroups: config.istio.io resources: '*' verbs: '*' apiGroups: iam.kubesphere.io resources: '*' verbs: '*' apiGroups: notification.kubesphere.io resources: '*' verbs: '*' apiGroups: auditing.kubesphere.io resources: '*' verbs: '*' apiGroups: events.kubesphere.io resources: '*' verbs: '*' apiGroups: core.kubefed.io resources: '*' verbs: '*' apiGroups: installer.kubesphere.io resources: '*' verbs: '*' apiGroups: storage.kubesphere.io resources: '*' verbs: '*' apiGroups: security.istio.io resources: '*' verbs: '*' apiGroups: monitoring.kiali.io resources: '*' verbs: '*' apiGroups: kiali.io resources: '*' verbs: '*' apiGroups: networking.k8s.io resources: '*' verbs: '*' apiGroups: kubeedge.kubesphere.io resources: '*' verbs: '*' apiGroups: types.kubefed.io resources: '*' verbs: '*' apiGroups: scheduling.k8s.io resources: '*' verbs: '*' apiGroups: kubevirt.io resources: '*' verbs: '*' apiGroups: cdi.kubevirt.io resources: '*' verbs: '*' apiGroups: network.kubesphere.io resources: '*' verbs: '*' apiGroups: virtualization.kubesphere.io resources: '*' verbs: '*' apiGroups: snapshot.storage.k8s.io resources: '*' verbs: '*' kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ks-installer subjects: kind: ServiceAccount name: ks-installer namespace: kubesphere-system roleRef: kind: ClusterRole name: ks-installer apiGroup: rbac.authorization.k8s.io apiVersion: apps/v1 kind: Deployment metadata: name: ks-installer namespace: kubesphere-system labels: app: ks-install spec: replicas: 1 selector: matchLabels: app: ks-install template: metadata: labels: app: ks-install spec: serviceAccountName: ks-installer containers: name: installer image: harbor.chs.neusoft.com/kubespheredev/ksv-installer:v1.6.1 imagePullPolicy: \"Always\" resources: limits: cpu: \"1\" memory: 1Gi requests: cpu: 20m memory: 100Mi volumeMounts: mountPath: /etc/localtime name: host-time volumes: hostPath: path: /etc/localtime type: \"\" name: host-time EOF Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/addons/addons.html":{"url":"2.容器/k8s/addons/addons.html","title":"addons","keywords":"","body":" 负载均衡器Porter 配置热重载 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/addons/openkruise/openkruise.html":{"url":"2.容器/k8s/addons/openkruise/openkruise.html","title":"openkruise","keywords":"","body":"安装（离线） 下载最新chart 下载镜像上传至私有镜像库 $ docker pull openkruise/kruise-manager:v1.2.0 $ docker tag openkruise/kruise-manager:v1.2.0 harbor.wl.io/openkruise/kruise-manager:v1.2.0 $ docker push harbor.wl.io/openkruise/kruise-manager:v1.2.0 离线安装 $ helm install openkruise kruise-1.2.0.tgz \\ --set manager.image.repository=harbor.wl.io/openkruise/kruise-manager:v1.2.0 观察部署状态 $ kubectl get pod -n kruise-system -w NAME READY STATUS RESTARTS AGE kruise-controller-manager-6d5fdbbb4-7m9wx 0/1 Running 0 11s kruise-controller-manager-6d5fdbbb4-8cjjh 0/1 Running 0 11s kruise-daemon-4q7qv 1/1 Running 0 11s kruise-daemon-5m252 0/1 ContainerCreating 0 11s kruise-daemon-zmjbx 0/1 ContainerCreating 0 11s kruise-daemon-zmjbx 1/1 Running 0 12s Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/addons/reloader/":{"url":"2.容器/k8s/addons/reloader/","title":"reloader","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/addons/reloader/01Reloader介绍.html":{"url":"2.容器/k8s/addons/reloader/01Reloader介绍.html","title":"01Reloader介绍","keywords":"","body":"Reloader项目介绍 项目信息 项目地址 LICENSE: Apache 2.0 项目介绍 以下内容翻自项目README.md Reloader是什么？ 有些时候，我们需要监控k8s中ConfigMap和/或Secret变化。 当配置发生变更，需滚动升级相关Deployment、Daemonset、Statefulset以便重新加载配置。 而Reloader便是以上需求的一个具体实现，Reloader基于kubernetes 1.9。 对比k8s-trigger-controller Reloader和k8s触发器控制器都是为了相同的目的而构建的。所以它们之间有很多相似和不同之处。 共同点： 两者均支持检测ConfigMap与Secret变更 两者均支持Deployment滚动更新 两者均使用SHA1进行哈希 两者均有端到端的单元测试用例 不同点： k8s-trigger-controller不支持StatefulSet与DaemonSet类型滚动更新，而Reloader支持 k8s-trigger-controller将哈希值存于注释中(trigger.k8s.io/[secret|configMap]-NAME-last-hash) 而Reloader将哈希值存于环境变量中（STAKATER_NAME_[SECRET|CONFIGMAP]） k8s-trigger-controller限制使用哈希值（trigger.k8s.io/[secret|configMap]-NAME-last-hash），而Reloader可定制化更强。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/application/helm.html":{"url":"2.容器/k8s/application/helm.html","title":"helm","keywords":"","body":" Table of Contents generated with DocToc helm 离线安装tiller helm 安装 curl -O https://storage.googleapis.com/kubernetes-helm/helm-v2.14.3-linux-amd64.tar.gz tar zxvf helm-v2.12.1-linux-amd64.tar.gz chmod +x linux-amd64/helm mv linux-amd64/helm /usr/local/bin 添加helm service account 并添加到clusteradmin 这个clusterrole上 kubectl create serviceaccount --namespace=kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller 安装tiller helm init -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.3 --stable-repo-url http://mirror.azure.cn/kubernetes/charts/ --service-account tiller --override spec.selector.matchLabels.'name'='tiller',spec.selector.matchLabels.'app'='helm' --output yaml | sed 's@apiVersion: extensions/v1beta1@apiVersion: apps/v1@' | kubectl apply -f - 查看 kubectl get pods -n kube-system | grep tiller 离线安装tiller 搭建本地仓储 http://10.16.48.44/ docker pull fishead/gcr.io.kubernetes-helm.tiller:v2.11.0 上传导入 helm init --upgrade --service-account tiller --tiller-image fishead/gcr.io.kubernetes-helm.tiller:v2.11.0 --stable-repo-url http://10.16.48.44/ Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/base/":{"url":"2.容器/k8s/base/","title":"base","keywords":"","body":"基本概念 容器的本质 一个视图被隔离、资源受限的进程 容器里PID=1的进程就是应用本身 管理虚拟机=管理基础设施 管理容器=直接管理应用本身 什么是kubernetes 云时代的操作系统 以此类推，容器镜像->操作系统的软件安装包 容器 -> 系统进程 Pod -> 线程组（一个pod可以包含多个容器） pod 为什么需要pod? 首先容器是单进程模型，容器的PID=1的进程应为应用本身进程 其次同一容器内启动多个进程，难以管理进程生命周期，也起不到隔离作用（ns,cgroup） 尽可能遵循: 一个容器只做一件事 同一pod网络共享 通过infra容器实现： 同一pod内的容器挂载infra容器（k8s.gcr.io/pause）的网络命名空间，实现网络共享 同一pod内的容器通信地址为localhost 同一pod内的容器共享同一ip地址，端口不可重复 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/base/finalizers.html":{"url":"2.容器/k8s/base/finalizers.html","title":"finalizers","keywords":"","body":"使用Finalizers控制k8s资源删除 文章引用 using-finalizers-to-control-deletion 你有没有在使用k8s过程中遇到过这种情况: 通过kubectl delete指令删除一些资源时，一直处于Terminating状态。 这是为什么呢？ 本文将介绍当你执行kubectl delete语句时，K8s内部都执行了哪些操作。 以及为何有些资源'删除不掉'(具体表现为一直Terminating，删除namespace时很容易遇到这种情况) 接下来，我们聚焦讨论以下四个方面: 资源的哪些属性会对删除操作产生影响？ finalizers与owner references属性是如何影响删除操作的？ 如何利用Propagation Policy（分发策略）更改删除顺序？ 删除操作的工作原理？ 方便起见，以下所有示例都将使用ConfigMaps和基本shell命令来演示该过程 词汇表 资源: k8s的资源对象（如configmap, secret, pod...） finalizers: 终结器，存放键的列表。列表内的键为空时资源才可被删除 owner references: 所有者引用（归谁管理/父资源对象是谁） kubectl: K8s客户端工具 基本删除操作 Kubernetes提供了几个不同的命令，您可以使用它们来创建、读取、更新和删除对象。 出于本文的目的，我们将重点讨论四个kubectl命令:create、get、patch和delete. 下面是kubectl delete命令的基本示例 创建名为mymap的configmap对象 $ kubectl create configmap mymap configmap/mymap created 查看名为mymap的configmap对象 $ kubectl get configmap/mymap NAME DATA AGE mymap 0 12s 删除名为mymap的configmap对象 $ kubectl delete configmap/mymap configmap \"mymap\" deleted 查看名为mymap的configmap对象 $ kubectl get configmap/mymap Error from server (NotFound): configmaps \"mymap\" not found 基本delete命令的删除操作状态图非常简单: 删除操作看似简单，但是有很多因素可能会干扰删除，包括finalizers与owner references属性 Finalizers是什么？ 上面我们提到了两个属性：finalizers与owner references可能会干扰删除操作，导致删除阻塞或失败。 那Finalizers是什么？会对删除有何影响呢？ 当要理解Kubernetes中的资源删除原理时，了解finalizers（以下我们称finalizers为终结器）的工作原理是很有帮助的， 可以帮助您理解为什么有些对象无法被删除。 终结器是资源发出预删除操作信号的属性， 控制着资源的垃圾收集，并用于提示控制器在删除资源之前执行哪些清理操作。 finalizers本质是包含键的列表，不具有实际意义。与annotations（注释）类似，finalizers是可以被操作的（增删改）。 以下终结器您可能遇到过： kubernetes.io/pv-protection kubernetes.io/pvc-protection 这两个终结器作用于卷，以防止卷被意外删除。 类似地，一些终结器可用于防止资源被删除，但不由任何控制器管理。 下面是一个自定义的configmap，它没有具体值，但包含一个终结器: $ cat 终结器通常用于名称空间(namespace)，而管理configmap资源的控制器不知道该如何处理finalizers字段。 下面我们尝试删除这个configmap对象: $ kubectl delete configmap/mymap & configmap \"mymap\" deleted $ jobs [1]+ Running kubectl delete configmap/mymap Kubernetes返回该对象已被删除，然而它并没有真正意义上被删除，而是在删除的过程中。 当我们试图再次获取该对象时，我们发现该对象多了个deletionTimestamp(删除时间戳)字段。 $ kubectl get cm mymap -o yaml apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: \"2021-09-29T11:04:40Z\" deletionGracePeriodSeconds: 0 deletionTimestamp: \"2021-09-29T11:04:55Z\" finalizers: - kubernetes managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:metadata: f:finalizers: .: {} v:\"kubernetes\": {} manager: kubectl operation: Update time: \"2021-09-29T11:04:40Z\" name: mymap namespace: default resourceVersion: \"1378430\" selfLink: /api/v1/namespaces/default/configmaps/mymap uid: 8d6ca0b1-4840-4597-8164-a63b526dbf5f 简而言之，当我们删除带有finalizers字段的对象时，该对象仅仅是被更新了，而不是被删除了。 这是因为Kubernetes获取到该对象包含终结器，通过添加deletionTimestamp（删除时间戳）字段将其置于只读状态（删除终结器键更新除外）。 换句话说，在删除该对象终结器之前，删除都不会完成。 接下来我们尝试通过patch命令删除终结器，并观察configmap/mymap是否会被'真正'删除。 $ kubectl patch configmap/mymap \\ --type json \\ --patch='[ { \"op\": \"remove\", \"path\": \"/metadata/finalizers\" } ]' configmap/mymap patched 再次检索该对象 $ kubectl get cm mymap Error from server (NotFound): configmaps \"mymap\" not found 发现该对象已被真正删除，下图描述了带有finalizers字段的对象删除流程： 总结：当您试图删除一个带有终结器的对象，它将一直处于预删除只读状态， 直到控制器删除了终结器键或使用Kubectl删除了终结器。一旦终结器列表为空，Kubernetes就可以回收该对象，并将其放入要从注册表中删除的队列中 带有finalizers字段的对象无法删除的原因大致如下： 对象存在finalizers，关联的控制器故障未能执行或执行finalizer函数hang住: 比如namespace控制器无法删除完空间内所有的对象， 特别是在使用aggregated apiserver时，第三方apiserver服务故障导致无法删除其对象。 此时，需要会恢复第三方apiserver服务或移除该apiserver的聚合，具体选择哪种方案需根据实际情况而定。 集群内安装的控制器给一些对象增加了自定义finalizers，未删除完fianlizers就下线了该控制器，导致这些fianlizers没有控制器来移除他们。 此时，需要恢复该控制器会手动移除finalizers(多出现于自定义operator)，具体选择哪种方案根据实际情况而定。 Owner References又是什么？ 上面我们提到了两个属性：finalizers与owner references可能会干扰删除操作，导致删除阻塞或失败。 并介绍了Finalizers，接下来我们聊聊Owner References. Owner References（所有者引用或所有者归属）描述了对象组之间的关系。 指定了资源彼此关联的属性，因此可以级联删除整个资源树。 当存在所有者引用时，将处理终结器规则。所有者引用由名称和UID组成 所有者引用相同名称空间内的链接资源，它还需要UID以使该引用生效(确保唯一)。 Pods通常具有对所属副本集的所有者引用。 因此，当Deloyment或有StatefulSet被删除时，子ReplicaSet和Pod将在流程中被删除。 我们通过下面的例子，来理解Owner References（所有者引用）的工作原理： 创建cm/mymap-parent对象$ cat 获取cm/mymap-parent的UIDCM_UID=$(kubectl get configmap mymap-parent -o jsonpath=\"{.metadata.uid}\") 创建cm/mymap-child对象，并设置ownerReferences字段声明所有者引用（通过kind、name、uid字段确保选择器可以匹配到）cat 即cm/mymap-parent为cm/mymap-child的父对象，此时我们删除cm/mymap-parent对象并观察cm/mymap-child对象状态 $ kubectl get cm NAME DATA AGE mymap-child 0 2m44s mymap-parent 0 3m $ kubectl delete cm mymap-parent configmap \"mymap-parent\" deleted $ kubectl get cm No resources found in default namespace. 即我们通过删除父对象，间接删除了父对象下的所有子对象。 这种删除k8s中被称为级联删除。我们可不可以只删除父对象，而不删除子对象呢？ 答案是: 可以的，删除时通过添加--cascade=false参数实现，我们通过下面的例子来验证： $ cat --cascade=false参数实际改变了父-子资源的删除顺序，k8s中关于父-子资源删除策略有以下三种： Foreground: 子资源在父资源之前被删除(post-order) Background: 父资源在子资源之前被删除 (pre-order) Orphan: 忽略所有者引用进行删除 下面这段内容比较晦涩，没太理解： Keep in mind that when you delete an object and owner references have been specified, finalizers will be honored in the process. This can result in trees of objects persisting, and you end up with a partial deletion. At that point, you have to look at any existing owner references on your objects, as well as any finalizers, to understand what’s happening 强制删除命名空间 有一种情况可能需要强制删除命名空间： 如果您已经删除了一个命名空间，并删除了它下面的所有对象，但名称空间仍然存在，一般为Terminating状态。 则可以通过更新名称空间的finalize属性来强制删除该名称空间。 会话1 $ kubectl proxy 会话2 $ NAMESPACE_NAME=test cat 我们应该谨慎思考是否强制删除命名空间，因为这样做可能只删除名称空间，命名空间下的其他资源删不完全，最终导致留下孤儿对象。 比如资源对象A存在于ddd命名空间，此时若强制删除ddd命名空间, 且对象A又未被删除，那么对象A便成了孤儿对象。 当出现孤儿对象时，可以手动重新创建名称空间，随后可以手动清理和恢复该对象。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/changelog/changelog.html":{"url":"2.容器/k8s/changelog/changelog.html","title":"changelog","keywords":"","body":"k8s版本更新说明 发布记录 2017 年 6 月 29 日，Kubernetes 1.7 发布 2017 年 9 月 28 日，Kubernetes 1.8 发布 2017 年 12 月 15 日，Kubernetes 1.9 发布 2018 年 3 月 26 日，Kubernetes 1.10 发布 2018 年 6 月 27 日，Kubernetes 1.11 发布 2018 年 9 月 27 日，Kubernetes 1.12 发布 2018 年 12 月 3 日，Kubernetes 1.13 发布 2019 年 3 月 26 日，Kubernetes 1.14 发布 2019 年 6 月 20 日，Kubernetes 1.15 发布 2019 年 9 月 19 日，Kubernetes 1.16 发布 2019 年 12 月 10 日，Kubernetes 1.17 发布 2020 年 3 月 25 日，Kubernetes 1.18 发布 2020 年 8 月 26 日，Kubernetes 1.19 发布1.2 1.3 1.4 1.5 1.6 1.7 2017年6月29日，kuberentes1.7发布。该版本的kubernetes在安全性、存储和可扩展性方面有了很大的提升。 这些新特性中包含了安全性更高的加密的secret、pod间通讯的网络策略，限制kubelet访问的节点授权程序以及客户端/服务器TLS证书轮换。 对于那些在Kubernetes上运行横向扩展数据库的人来说，这个版本有一个主要的特性，可以为StatefulSet添加自动更新并增强DaemonSet的更新。我们还宣布了对本地存储的Alpha支持，以及用于更快地缩放StatefulSets的突发模式。 此外，对于高级用户，此发行版中的API聚合允许使用用于自定义的API与API server同时运行。其他亮点包括支持可扩展的准入控制器，可插拔云供应商程序和容器运行时接口（CRI）增强功能。 新功能 安全 Network Policy API提升为稳定版本。用户可以通过使用网络插件实现的网络策略来控制哪些Pod之间能够互相通信。 节点授权和准入控制插件是新增加的功能，可以用于限制kubelet可以访问的secret、pod和其它基于节点的对象。 加密的Secret和etcd中的其它资源，现在是alpha版本。 Kubelet TLS bootstrapping现在支持客户端和服务器端的证书轮换。 由API server存储的审计日志现在更具可定制性和可扩展性，支持事件过滤和webhook。它们还为系统审计提供更丰富的数据。 有状态负载 StatefulSet更新是1.7版本的beta功能，它允许使用包括滚动更新在内的一系列更新策略自动更新诸如Kafka，Zookeeper和etcd等有状态应用程序。 StatefulSets现在还支持对不需要通过Pod管理策略进行排序的应用程序进行快速扩展和启动。这可以是主要的性能改进。 本地存储（alpha）是有状态应用程序最常用的功能之一。用户现在可以通过标准的PVC/PV接口和StatefulSet中的StorageClass访问本地存储卷。 DaemonSet——为每个节点创建一个Pod，现在有了更新功能，在1.7中增加了智能回滚和历史记录功能。 新的StorageOS Volume插件可以使用本地或附加节点存储中以提供高可用的集群范围的持久卷。 可扩展性 运行时的API聚合是此版本中最强大的扩展功能，允许高级用户将Kubernetes风格的预先构建的第三方或用户创建的API添加到其集群中。 容器运行时接口（CRI）已经增强，可以使用新的RPC调用从运行时检索容器度量。 CRI的验证测试已经发布，与containerd进行了Alpha集成，现在支持基本的生命周期和镜像管理。 其它功能 引入了对外部准入控制器的Alpha支持，提供了两个选项，用于向API server添加自定义业务逻辑，以便在创建对象和验证策略时对其进行修改。 基于策略的联合资源布局提供Alpha版本，用于根据自定义需求（如法规、定价或性能）为联合（federated）集群提供布局策略。 弃用 第三方资源（TPR）已被自定义资源定义（Custom Resource Definitions，CRD）取代，后者提供了一个更清晰的API，并解决了TPR测试期间引发的问题和案例。如果您使用TPR测试版功能，则建议您迁移，因为它将在Kubernetes 1.8中被移除。 1.8 1.9 1.10 1.11 1.12 1.13 1.14 1.15 1.16 1.17 1.18 1.19 1.20 参考文献 开源中文手册 k8s中文社区 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/cmd/cmd.html":{"url":"2.容器/k8s/cmd/cmd.html","title":"cmd","keywords":"","body":"禁止/允许调度 允许调度 kubectl patch node -p \"{\\\"spec\\\":{\\\"unschedulable\\\":false}}\" 修改默认sc kubectl patch storageclass -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' 节点标签 # kubectl label nodes key=value kubectl label nodes ceph01 role=storage-node 驱逐业务容器 kubectl drain --ignore-daemonsets --delete-local-data 清理Evicted状态pod for ns in `kubectl get ns | awk 'NR>1{print $1}'` do kubectl get pods -n ${ns} | grep Evicted | awk '{print $1}' | xargs kubectl delete pod -n ${ns} done 配置 修改kubernetes限制节点pod数量 默认100 编辑 vim /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 调整修改 Environment=\"KUBELET_NODE_MAX_PODS=--max-pods=600\" ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS $KUBELET_NODE_MAX_PODS 重启kubelet systemctl daemon-reload systemctl restart kubelet 删除命名空间 reason ceph-csi注意替换 curl -H \"Content-Type: application/json\" -XPUT -d '{\"apiVersion\":\"v1\",\"kind\":\"Namespace\",\"metadata\":{\"name\":\"ceph-csi\"},\"spec\":{\"finalizers\":[]}}' http://localhost:8001/api/v1/namespaces/ceph-csi/finalize 变更default StorageClass kubectl patch storageclass -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' 检查证书是否过期 kubeadm alpha certs check-expiration 或 openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text |grep ' Not ' 手动更新证书 kubeadm alpha certs renew all docker ps | grep -v pause | grep -E \"etcd|scheduler|controller|apiserver\" | awk '{print $1}' | awk '{print \"docker\",\"restart\",$1}' | bash cp -i /etc/kubernetes/admin.conf $HOME/.kube/config 删除孤儿Pod #!/bin/sh orphanedPods=`cat /var/log/messages|grep 'orphaned pod'|awk -F '\"' '{print $2}'|uniq`; orphanedPodsNum=`echo $orphanedPods|awk -F ' ' '{print NF}'`; echo -e \"orphanedPods: $orphanedPodsNum \\n$orphanedPods\"; for i in $orphanedPods do echo \"Deleting Orphaned pod id: $i\"; rm -rf /var/lib/kubelet/pods/$i; done +-----------------------------------------------------------------------+------------------+-------------------+---------------+-----------+ | CONTROL NAME | FAILED RESOURCES | WARNING RESOURCES | ALL RESOURCES | % SUCCESS | +-----------------------------------------------------------------------+------------------+-------------------+---------------+-----------+ | Allow privilege escalation | 0 | 0 | 267 | 100% | | Allowed hostPath | 103 | 0 | 267 | 61% | | Applications credentials in configuration files | 13 | 0 | 499 | 97% | | Automatic mapping of service account | 72 | 0 | 72 | 0% | | CVE-2021-25741 - Using symlink for arbitrary host file system access. | 2 | 0 | 274 | 99% | | Cluster-admin binding | 12 | 0 | 851 | 98% | | Container hostPort | 2 | 0 | 267 | 99% | | Control plane hardening | 0 | 0 | 267 | 100% | | Dangerous capabilities | 1 | 0 | 267 | 99% | | Exec into container | 13 | 0 | 851 | 98% | | Exposed dashboard | 0 | 0 | 336 | 100% | | Host PID/IPC privileges | 1 | 0 | 267 | 99% | | Immutable container filesystem | 267 | 0 | 267 | 0% | | Ingress and Egress blocked | 267 | 0 | 267 | 0% | | Insecure capabilities | 0 | 0 | 267 | 100% | | Linux hardening | 265 | 0 | 267 | 0% | | Network policies | 26 | 0 | 26 | 0% | | Non-root containers | 7 | 0 | 267 | 97% | | Privileged container | 1 | 0 | 267 | 99% | | Resource policies | 189 | 0 | 267 | 29% | | hostNetwork access | 3 | 0 | 267 | 98% | +-----------------------------------------------------------------------+------------------+-------------------+---------------+-----------+ | 21 | 1244 | 0 | 6647 | 81% | +-----------------------------------------------------------------------+------------------+-------------------+---------------+-----------+ calico https://blog.csdn.net/u010039418/article/details/120797425 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/cgroups/01k8s下cgroups管理.html":{"url":"2.容器/k8s/core/cgroups/01k8s下cgroups管理.html","title":"01k8s下cgroups管理","keywords":"","body":"k8s下cgroups管理 k8s对cgroups的管理是通过kubelet组件完成的，涉及的cgroups分类如下： 组件级cgroups: 容器运行时控制组：--runtime-cgroups 1.组件级cgroups 容器级资源控制 默认情况下，容器可以无限制地使用主机的cpu资源，可以通过设置参数来进行限制。一般都采用Linux默认的CFS调度法，当然也可以使用实时调度。CFS调度可以使用如下参数来进行限制： 常用指标： cpu.cfs_period_us：指定cpu CFS的周期，通常和--cpu-quota一起使用，单位是us，默认值是100毫秒 cpu.cfs_quota_us：指定容器在一个cpu CFS调度周期中可以使用cpu的时间，单位是us。默认不限制 memory.limit_in_bytes: 内存使用量，默认不限制 pod级资源控制 pod级cgroup + QoS Guaranteed （该策略下，设置的requests 等于 limits） Burstable（该策略下，设置的requests 小于 limits） BestEffort（该策略下，没有设置requests 、 limits） 当某个node内存被严重消耗时，BestEffort策略的pod会最先被kubelet杀死， 其次Burstable（该策略的pods如有多个，也是按照内存使用率来由高到低地终止）， 再其次Guaranteed。 Node级别资源控制 kubelet会将所有的pod都创建一个kubepods的cgroup下，通过该cgroup来限制node上运行的pod最大可以使用的资源 如：/sys/fs/cgroup/cpu/kubepods.slice/ 该cgroup的资源限制取值为: ${Node Capacity} - ${Kube-Reserved} - ${System-Reserved}， 即：节点总配额 - K8s预留 - 系统服务预留 ${Allocatable} = ${Node Capacity} - ${Kube-Reserved} - ${System-Reserved} - ${Hard-Eviction-Threshold} 即：pod可申请资源配额 = 节点总配额 - K8s预留 - 系统服务预留 - 驱逐低优先级pod的阈值 其中kube-reserved是为kubernetes组件提供的资源预留，system-reserved是为系统组件预留的资源， 分别通过--kube-reserved, --system-reserved来指定，例如--kube-reserved=cpu=100m,memory=100Mi {Hard-Eviction-Threshold}为在资源紧张的时候kubelet主动驱逐低优先级pod的阈值 组件级资源控制 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/cgroups/02k8s下pod控制组解析.html":{"url":"2.容器/k8s/core/cgroups/02k8s下pod控制组解析.html","title":"02k8s下pod控制组解析","keywords":"","body":"Kubernetes下pod控制组管理解析 开始之前我们先了解下Kubernetes QoS概念 在Kubernetes里面，将资源分成不同的QoS类别，并且通过pod里面的资源定义来区分pod对于平台提供的资源保障的SLA等级： Guaranteed: pod中每个容器都为CPU和内存设置了相同的requests和limits，此类pod具有最高优先级 Burstable: 至少有一个容器设置了CPU或内存的requests属性，但不满足Guaranteed类别要求的pod，它们具有中等优先级 BestEffort: 未为任何一个容器设置requests和limits属性的pod，它们的优先级为最低级别 针对不同的优先级的业务，在资源紧张或者超出的时候会有不同的处理策略。 同时，针对CPU和内存两类不同类型的资源，一种是可压缩的，一种是不可压缩的，所以资源的分配和调控策略也会有很大区别。 CPU资源紧缺时，如果节点处于超卖状态，则会根据各自的requests配置，按比例分配CPU时间片，而内存资源紧缺时需要内核的oom killer进行管控， Kubernetes负责为OOM killer提供管控依据： BestEfford类容器由于没有要求系统供任何级别的资源保证，将最先被终止；但是在资源不紧张时，它们能尽可能多地占用资源，实现资源的复用和部署密度的提高 如果BestEfford类容器都已经终止，Burstable中等优先级的的pod将被终止 Guaranteed类容器拥有最高优先级，只有在内存资源使用超出limits的时候或者节点OOM时得分最高，才会被终止； OOM得分主要根据QoS类和容器的requests内存占机器总内存比来计算： OOM得分越高，该进程的优先级越低，越容易被终止； 根据公式，Burstable优先级的pod中，requests内存申请越多，越容易在OOM的时候被终止。 pod的cpu控制组解析 首先我们先看下pod的控制组层级 $ ls -l /sys/fs/cgroup/cpu/kubepods.slice total 0 -rw-r--r-- 1 root root 0 Aug 23 16:32 cgroup.clone_children -rw-r--r-- 1 root root 0 Aug 23 16:32 cgroup.procs -r--r--r-- 1 root root 0 Aug 23 16:32 cpuacct.stat -rw-r--r-- 1 root root 0 Aug 23 16:32 cpuacct.usage -r--r--r-- 1 root root 0 Aug 23 16:32 cpuacct.usage_all -r--r--r-- 1 root root 0 Aug 23 16:32 cpuacct.usage_percpu -r--r--r-- 1 root root 0 Aug 23 16:32 cpuacct.usage_percpu_sys -r--r--r-- 1 root root 0 Aug 23 16:32 cpuacct.usage_percpu_user -r--r--r-- 1 root root 0 Aug 23 16:32 cpuacct.usage_sys -r--r--r-- 1 root root 0 Aug 23 16:32 cpuacct.usage_user -rw-r--r-- 1 root root 0 Aug 23 16:32 cpu.cfs_period_us -rw-r--r-- 1 root root 0 Aug 23 16:32 cpu.cfs_quota_us -rw-r--r-- 1 root root 0 Aug 23 16:32 cpu.rt_period_us -rw-r--r-- 1 root root 0 Aug 23 16:32 cpu.rt_runtime_us -rw-r--r-- 1 root root 0 Sep 1 17:38 cpu.shares -r--r--r-- 1 root root 0 Aug 23 16:32 cpu.stat drwxr-xr-x 55 root root 0 Aug 23 16:32 kubepods-besteffort.slice drwxr-xr-x 51 root root 0 Aug 23 16:32 kubepods-burstable.slice drwxr-xr-x 4 root root 0 Aug 23 16:54 kubepods-pod934b0aa2_1d1b_4a81_bfcf_89c4beef899e.slice drwxr-xr-x 4 root root 0 Aug 23 16:39 kubepods-podca849e84_aa86_4402_bf31_e7e73faa77fe.slice -rw-r--r-- 1 root root 0 Aug 23 16:32 notify_on_release -rw-r--r-- 1 root root 0 Aug 23 16:32 tasks 其中kubepods-besteffort.slice存放besteffort类型pod配置，kubepods-burstable.slice存放burstable类型pod配置。 kubepods-pod934b0aa2_1d1b_4a81_bfcf_89c4beef899e.slice、kubepods-podca849e84_aa86_4402_bf31_e7e73faa77fe.slice则为Guaranteed类型pod 为了更好的解释说明，我们创建一个新的Guaranteed类型的pod用于测试: cat 再次查看/sys/fs/cgroup/cpu/kubepods.slice下，发现新增了一个kubepods-podf56bf66f_3efb_4c80_8818_37de69ee5b72.slice目录 名称解析 kubepods-podf56bf66f_3efb_4c80_8818_37de69ee5b72.slice这个名称是怎么命名的呢？ 命名格式为：kubepods-pod.slice，并且会将uid中-转换为_ $ kubectl get pod nginx-demo -o yaml|grep uid uid: f56bf66f-3efb-4c80-8818-37de69ee5b72 目录解析 $ ls -l /sys/fs/cgroup/cpu/kubepods.slice/kubepods-podf56bf66f_3efb_4c80_8818_37de69ee5b72.slice -rw-r--r-- 1 root root 0 Nov 17 11:23 cgroup.clone_children -rw-r--r-- 1 root root 0 Nov 17 11:23 cgroup.procs -r--r--r-- 1 root root 0 Nov 17 11:23 cpuacct.stat -rw-r--r-- 1 root root 0 Nov 17 11:23 cpuacct.usage -r--r--r-- 1 root root 0 Nov 17 11:23 cpuacct.usage_all -r--r--r-- 1 root root 0 Nov 17 11:23 cpuacct.usage_percpu -r--r--r-- 1 root root 0 Nov 17 11:23 cpuacct.usage_percpu_sys -r--r--r-- 1 root root 0 Nov 17 11:23 cpuacct.usage_percpu_user -r--r--r-- 1 root root 0 Nov 17 11:23 cpuacct.usage_sys -r--r--r-- 1 root root 0 Nov 17 11:23 cpuacct.usage_user -rw-r--r-- 1 root root 0 Nov 17 11:23 cpu.cfs_period_us -rw-r--r-- 1 root root 0 Nov 17 11:23 cpu.cfs_quota_us -rw-r--r-- 1 root root 0 Nov 17 11:23 cpu.rt_period_us -rw-r--r-- 1 root root 0 Nov 17 11:23 cpu.rt_runtime_us -rw-r--r-- 1 root root 0 Nov 17 11:23 cpu.shares -r--r--r-- 1 root root 0 Nov 17 11:23 cpu.stat drwxr-xr-x 2 root root 0 Nov 17 11:23 docker-08974ffd61043b34e4cd5710d5446eb423c6371afb4c9d106e608f08cc1182a3.scope drwxr-xr-x 2 root root 0 Nov 17 11:24 docker-d33dc12340fd32b35148293c21f84dab14f2274046056bbeef9e9666d1d0dc2a.scope -rw-r--r-- 1 root root 0 Nov 17 11:23 notify_on_release -rw-r--r-- 1 root root 0 Nov 17 11:23 tasks 我们发现怎么有两个容器呢？（docker-08974ffd61043b34e4cd5710d5446eb423c6371afb4c9d106e608f08cc1182a3.scope、docker-d33dc12340fd32b35148293c21f84dab14f2274046056bbeef9e9666d1d0dc2a.scope） 其实是业务容器 + infra沙箱容器，并且命名格式遵循：docker-.scope $ docker ps|grep nginx d33dc12340fd nginx \"/docker-entrypoint.…\" 7 minutes ago Up 7 minutes k8s_nginx_nginx-demo_default_f56bf66f-3efb-4c80-8818-37de69ee5b72_0 08974ffd6104 harbor.chs.neusoft.com/kubesphere/pause:3.2 \"/pause\" 8 minutes ago Up 8 minutes k8s_POD_nginx-demo_default_f56bf66f-3efb-4c80-8818-37de69ee5b72_0 我们可根据以下命令获取业务容器id： $ kubectl get pod nginx-demo -o yaml|grep containerID - containerID: docker://d33dc12340fd32b35148293c21f84dab14f2274046056bbeef9e9666d1d0dc2a 业务容器cgroup解析 $ cd /sys/fs/cgroup/cpu/kubepods.slice/kubepods-podf56bf66f_3efb_4c80_8818_37de69ee5b72.slice/docker-d33dc12340fd32b35148293c21f84dab14f2274046056bbeef9e9666d1d0dc2a.scope $ ls -l total 0 -rw-r--r-- 1 root root 0 Nov 17 11:24 cgroup.clone_children -rw-r--r-- 1 root root 0 Nov 17 11:24 cgroup.procs -r--r--r-- 1 root root 0 Nov 17 11:24 cpuacct.stat -rw-r--r-- 1 root root 0 Nov 17 11:24 cpuacct.usage -r--r--r-- 1 root root 0 Nov 17 11:24 cpuacct.usage_all -r--r--r-- 1 root root 0 Nov 17 11:24 cpuacct.usage_percpu -r--r--r-- 1 root root 0 Nov 17 11:24 cpuacct.usage_percpu_sys -r--r--r-- 1 root root 0 Nov 17 11:24 cpuacct.usage_percpu_user -r--r--r-- 1 root root 0 Nov 17 11:24 cpuacct.usage_sys -r--r--r-- 1 root root 0 Nov 17 11:24 cpuacct.usage_user -rw-r--r-- 1 root root 0 Nov 17 11:24 cpu.cfs_period_us -rw-r--r-- 1 root root 0 Nov 17 11:24 cpu.cfs_quota_us -rw-r--r-- 1 root root 0 Nov 17 11:24 cpu.rt_period_us -rw-r--r-- 1 root root 0 Nov 17 11:24 cpu.rt_runtime_us -rw-r--r-- 1 root root 0 Nov 17 11:24 cpu.shares -r--r--r-- 1 root root 0 Nov 17 11:24 cpu.stat -rw-r--r-- 1 root root 0 Nov 17 11:24 notify_on_release -rw-r--r-- 1 root root 0 Nov 17 11:24 tasks 我们上述对pod配额的定义为: resources: limits: cpu: \"1\" memory: 1Gi requests: cpu: 1 memory: 1Gi 其实等同于以以下方式启动docker容器: $ docker run --rm -dt --cpu-shares=1024 --cpu-quota=1024 --memory=1g nginx 我们可以看下docker容器的配额： $ docker inspect d33dc12340fd32b35148293c21f84dab14f2274046056bbeef9e9666d1d0dc2a -f {{.HostConfig.CpuShares}} 1024 $ docker inspect d33dc12340fd32b35148293c21f84dab14f2274046056bbeef9e9666d1d0dc2a -f {{.HostConfig.CpuQuota}} 100000 $ docker inspect d33dc12340fd32b35148293c21f84dab14f2274046056bbeef9e9666d1d0dc2a -f {{.HostConfig.CpuPeriod}} 100000 .HostConfig.CpuShares对应控制内的cpu.shares文件内容 .HostConfig.CpuPeriod对应控制内的cpu.cpu.cfs_period_us文件内容 .HostConfig.CpuQuota对应控制内的cpu.cfs_quota_us文件内容 并且我们发现k8s基于pod管理控制组（同一pod内的容器所属同一控制组） \"CgroupParent\": \"kubepods-podf56bf66f_3efb_4c80_8818_37de69ee5b72.slice\", 我们可以得出记录：k8s通过控制组的cpu.shares、cpu.cpu.cfs_period_us、cpu.cfs_quota_us配置，达到限制CPU的目的。 那么这三个文件是用来干嘛的？ cpu.shares解析 cpu.shares用来设置CPU的相对值，并且是针对所有的CPU（内核），默认值是1024等同于一个cpu核心。 CPU Shares将每个核心划分为1024个片，并保证每个进程将按比例获得这些片的份额。如果有1024个片(即1核)，并且两个进程设置cpu.shares均为1024，那么这两个进程中每个进程将获得大约一半的cpu可用时间。 当系统中有两个cgroup，分别是A和B，A的shares值是1024，B 的shares值是512， 那么A将获得1024/(1024+512)=66%的CPU资源，而B将获得33%的CPU资源。shares有两个特点： 如果A不忙，没有使用到66%的CPU时间，那么剩余的CPU时间将会被系统分配给B，即B的CPU使用率可以超过33%。 如果添加了一个新的cgroup C，且它的shares值是1024，那么A的限额变成了1024/(1024+512+1024)=40%，B的变成了20%。 从上面两个特点可以看出： 在闲的时候，shares基本上不起作用，只有在CPU忙的时候起作用，这是一个优点。 由于shares是一个绝对值，需要和其它cgroup的值进行比较才能得到自己的相对限额，而在一个部署很多容器的机器上，cgroup的数量是变化的，所以这个限额也是变化的，自己设置了一个高的值，但别人可能设置了一个更高的值，所以这个功能没法精确的控制CPU使用率。 cpu.shares对应k8s内的resources.requests.cpu字段： 值对应关系为：resources.requests.cpu * 1024 = cpu.shares 如：resources.requests.cpu为3的时候，cpu.shares值为3072；resources.requests.cpu为100m的时候，cpu.shares值为102 cpu.cpu.cfs_period_us、cpu.cfs_quota_us解析 cpu.cfs_period_us用来配置时间周期长度，cpu.cfs_quota_us用来配置当前cgroup在设置的周期长度内所能使用的CPU时间数。 两个文件配合起来设置CPU的使用上限。两个文件的单位都是微秒（us），cfs_period_us的取值范围为1毫秒（ms）到1秒（s），cfs_quota_us的取值大于1ms即可，如果cfs_quota_us的值为-1（默认值），表示不受cpu时间的限制。 cpu.cpu.cfs_period_us、cpu.cfs_quota_us对应k8s中的resources.limits.cpu字段： resources.limits.cpu = cpu.cfs_quota_us/cpu.cfs_period_us 并且k8s下容器控制组的cpu.cpu.cfs_period_us值固定为100000，实际只设置cpu.cfs_quota_us值 例如： cpu.cpu.cfs_period_us为100000（单位微妙，即0.1秒），cpu.cfs_quota_us为500000（单位微妙，即0.5秒）时，resources.limits.cpu为5，即5个cpu核心。 cpu.cpu.cfs_period_us为100000（单位微妙，即0.1秒），cpu.cfs_quota_us为10000（单位微妙，即0.01秒）时，resources.limits.cpu为0.1（或100m），即0.1个cpu核心。 pod的内存控制组解析 与cpu不同，k8s里pod容器的requests.memory在控制组内没有对应的属性，未起到限制作用，只是协助k8s调度计算。 而pod容器的limits.memory对应控制组里的memory.limit_in_bytes值。 总结 k8s基于pod管理控制组，同一pod内的容器所属同一控制组，并且每个控制组内包含一个infra沙箱容器 k8s基于.spec.containers[x].resources对pod划分了三种类型，对应控制组路径如下: Pod类型 描述 控制组 Guaranteed 内存与CPU设置了相同的requests和limits /sys/fs/cgroup//kubepods.slice/kubepods-pod.slice Burstable 至少有一个容器设置了CPU或内存的requests属性 /sys/fs/cgroup//kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod.slice BestEffort 所有容器均未设置requests和limits /sys/fs/cgroup//kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod.slice 控制组中cpu.shares对应k8s内的resources.requests.cpu字段，值对应关系为： resources.requests.cpu * 1024 = cpu.shares 控制组中cpu.cpu.cfs_period_us、cpu.cfs_quota_us对应k8s中的resources.limits.cpu字段，值对应关系为： resources.limits.cpu = cpu.cfs_quota_us/cpu.cfs_period_us 控制组里的memory.limit_in_bytes对应k8s中的resources.limits.memory值 参考文章 Kubernetes生产实践系列之三十：Kubernetes基础技术之集群计算资源管理 Understanding resource limits in kubernetes: cpu time Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/cri/CRI&OCI.html":{"url":"2.容器/k8s/core/cri/CRI&OCI.html","title":"CRI&OCI","keywords":"","body":"docker组件介绍 docker: dockerd客户端工具 dockerd: 即docker daemon，docker守护进程 containerd: 工业级标准的容器运行时，主要功能： 管理容器的生命周期(从创建容器到销毁容器) 拉取/推送容器镜像 存储管理(管理镜像及容器数据的存储) 调用runC运行容器(与runC等容器运行时交互) 管理容器网络接口及网络 ctr: containerd的客户端工具 runC: runC是一个符合OCI规范的命令行工具，用来运行容器。 kubelet创建容器流程 运行时为docker时 kubelet通过CRI远程调用内部的dockershim套接字（/var/run/dockershim.sock） dockershim远程调用dockerd套接字（/var/run/docker.sock） dockerd守护进程调用containerd守护进程套接字（/run/containerd/containerd.sock） containerd进程fork出一个container-shim进程，通过调用runc命令行启动、管理容器，并作为容器进程的父进程。 +--------------------+ | | | | CRI gRPC | kubelet +-----+ +---------------+ +--------------+ | | | | | | | | | | +---------------+ +--------------+ fork |container-shim +-----> container | | +-------------+ | | | | +-------> | | | | | | | | | | | +---------------+ +--------------+ | | A+B +------->C | | | | | | | | | | | +------->container-shim +-----> container | | | | | | | | | | | | +------+-------------+ +---------------+ +--------------+ +---------------+ +--------------+ | A:unix:///var/run/dockershim.sock +------> ...... C:/run/containerd/containerd.sock B:/var/run/docker.sock container-shim有什么作用? 兼容多种OCI运行时：为了能够支持多种OCI Runtime，containerd内部使用containerd-shim， 每启动一个容器都会创建一个新的containerd-shim进程，指定容器ID、Bundle目录、运行时的二进制（比如runc） 作为容器进程的父进程：避免containerd进程意外退出导致所有容器进程退出。而containerd进程又是所有containerd-shim进程父进程 在containerd运行的情况下，杀死containerd-shim进程，容器进程会退出。 在containerd运行的情况下，杀死容器进程，conainerd-shim进程主动退出，containerd触发exit事件以清理该容器。 进程关系 $ pstree 2762 containerd─┬─6*[containerd-shim─┬─sh───java───21*[{java}]] │ └─9*[{containerd-shim}]] ├─6*[containerd-shim─┬─java───48*[{java}]] │ └─9*[{containerd-shim}]] ├─163*[containerd-shim─┬─pause] │ └─9*[{containerd-shim}]] ├─13*[containerd-shim─┬─redis-server───3*[{redis-server}]] │ └─11*[{containerd-shim}]] ├─containerd-shim─┬─java───69*[{java}] │ └─9*[{containerd-shim}] └─577*[{containerd}] 参考文章 容器运行时笔记 containerd、containerd-shim和runc的依存关系 https://xuanwo.io/2019/08/06/oci-intro/ Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/cri/关于k8s弃用docker-shim.html":{"url":"2.容器/k8s/core/cri/关于k8s弃用docker-shim.html","title":"关于k8s弃用docker-shim","keywords":"","body":"深入探究kubernetes弃用dockershim 关于弃用dockershim的快速问答 为什么kubernetes要弃用dockershim？ 首先，先了解下dockershim是什么。 dockershim是kubelet内的一个组件，主要目的是为了通过其操作Docker来管理容器。 Dockershim Deprecation FAQ Don't Panic: Kubernetes and Docker Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/05启动容器/":{"url":"2.容器/k8s/core/kubelet/创建pod/05启动容器/","title":"05启动容器","keywords":"","body":"启动容器 启动pod容器流程分为三个步骤： 启动临时容器 启动初始化容器 启动业务容器 以上三种类型的启动流程基本一致，均通过调用startContainer函数实现 启动pod容器的源码实现 func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) { ... // Helper containing boilerplate common to starting all types of containers. // typeName is a label used to describe this type of container in log messages, // currently: \"container\", \"init container\" or \"ephemeral container\" start := func(typeName string, spec *startSpec) error { startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, spec.container.Name) result.AddSyncResult(startContainerResult) isInBackOff, msg, err := m.doBackOff(pod, spec.container, podStatus, backOff) if isInBackOff { startContainerResult.Fail(err, msg) klog.V(4).Infof(\"Backing Off restarting %v %+v in pod %v\", typeName, spec.container, format.Pod(pod)) return err } klog.V(4).Infof(\"Creating %v %+v in pod %v\", typeName, spec.container, format.Pod(pod)) // NOTE (aramase) podIPs are populated for single stack and dual stack clusters. Send only podIPs. if msg, err := m.startContainer(podSandboxID, podSandboxConfig, spec, pod, podStatus, pullSecrets, podIP, podIPs); err != nil { startContainerResult.Fail(err, msg) // known errors that are logged in other places are logged at higher levels here to avoid // repetitive log spam switch { case err == images.ErrImagePullBackOff: klog.V(3).Infof(\"%v start failed: %v: %s\", typeName, err, msg) default: utilruntime.HandleError(fmt.Errorf(\"%v start failed: %v: %s\", typeName, err, msg)) } return err } return nil } // Step 5: start ephemeral containers // These are started \"prior\" to init containers to allow running ephemeral containers even when there // are errors starting an init container. In practice init containers will start first since ephemeral // containers cannot be specified on pod creation. if utilfeature.DefaultFeatureGate.Enabled(features.EphemeralContainers) { for _, idx := range podContainerChanges.EphemeralContainersToStart { start(\"ephemeral container\", ephemeralContainerStartSpec(&pod.Spec.EphemeralContainers[idx])) } } // Step 6: start the init container. if container := podContainerChanges.NextInitContainerToStart; container != nil { // Start the next init container. if err := start(\"init container\", containerStartSpec(container)); err != nil { return } // Successfully started the container; clear the entry in the failure klog.V(4).Infof(\"Completed init container %q for pod %q\", container.Name, format.Pod(pod)) } // Step 7: start containers in podContainerChanges.ContainersToStart. for _, idx := range podContainerChanges.ContainersToStart { start(\"container\", containerStartSpec(&pod.Spec.Containers[idx])) } return } startContainer函数主要逻辑如下： 拉取镜像 创建容器 启动容器 执行容器的postStart钩子(容器启动完毕后要执行的逻辑) 后续我们将对startContainer函数着重讨论 startContainer函数源码实现 kubernetes\\pkg\\kubelet\\kuberuntime\\kuberuntime_container.go func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { container := spec.container // Step 1: pull the image. imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets, podSandboxConfig) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return msg, err } // Step 2: create the container. // 生成容器引用， ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil { klog.Errorf(\"Can't make a ref to pod %q, container %v: %v\", format.Pod(pod), container.Name, err) } klog.V(4).Infof(\"Generating ref for container %s: %#v\", container.Name, ref) // For a new container, the RestartCount should be 0 // 如果该容器存在，则重启次数加1 restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil { restartCount = containerStatus.RestartCount + 1 } // 生成容器配置-获取临时容器ID target, err := spec.getTargetID(podStatus) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return s.Message(), ErrCreateContainerConfig } // 生成创建容器所需配置：设备列表、volumes列表、环境变量列表、注释列表等 containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, podIPs, target) if cleanupAction != nil { defer cleanupAction() } if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return s.Message(), ErrCreateContainerConfig } containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return s.Message(), ErrCreateContainer } err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Internal PreStartContainer hook failed: %v\", s.Message()) return s.Message(), ErrPreStartHook } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.CreatedContainer, fmt.Sprintf(\"Created container %s\", container.Name)) if ref != nil { m.containerRefManager.SetRef(kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, }, ref) } // Step 3: start the container. err = m.runtimeService.StartContainer(containerID) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Error: %v\", s.Message()) return s.Message(), kubecontainer.ErrRunContainer } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.StartedContainer, fmt.Sprintf(\"Started container %s\", container.Name)) // Symlink container logs to the legacy container log location for cluster logging // support. // TODO(random-liu): Remove this after cluster logging supports CRI container log path. // /var/log/containers containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) // podSandboxConfig.LogDirectory=\"/var/log/pods/default_stakater-reloader-598f958967-ddkl7_2681cb7f-daa4-4620-bb60-1d449709181c\" // containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) // only create legacy symlink if containerLog path exists (or the error is not IsNotExist). // Because if containerLog path does not exist, only dandling legacySymlink is created. // This dangling legacySymlink is later removed by container gc, so it does not make sense // to create it in the first place. it happens when journald logging driver is used with docker. if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) { if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil { klog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\", legacySymlink, containerID, containerLog, err) } } // Step 4: execute the post start hook. if container.Lifecycle != nil && container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { m.recordContainerEvent(pod, container, kubeContainerID.ID, v1.EventTypeWarning, events.FailedPostStartHook, msg) if err := m.killContainer(pod, kubeContainerID, container.Name, \"FailedPostStartHook\", nil); err != nil { klog.Errorf(\"Failed to kill container %q(id=%q) in pod %q: %v, %v\", container.Name, kubeContainerID.String(), format.Pod(pod), ErrPostStartHook, err) } return msg, fmt.Errorf(\"%s: %v\", ErrPostStartHook, handlerErr) } } return \"\", nil } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/":{"url":"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/","title":"02创建容器","keywords":"","body":"创建容器流程概念 本文主要分析创建pod容器时，kubelet所需做的操作，针对容器运行时创建容器的具体操作不作讨论。 主要分为以下几个步骤： 设置容器重启次数: 该步骤根据容器名称查询pod的status中容器状态，若查询不到则重启次数设置为0，如查询到该容器状态则重启次数基于原值加1。 生成创建容器所需配置，主要逻辑如下： 根据镜像名称，调用容器运行时，获取运行容器启动命令的用户 检测运行容器启动命令的用户判是否违反pod安全上下文设置（runAsNonRoot: true时，不允许容器以root用户启动） 生成日志目录（格式为: /var/log/pods/__/） 针对windows平台，定义额外配置 定义容器内的环境变量 组装创建容器所需配置项并返回，配置项包括: 主机名 环境变量列表 挂载点信息列表 映射到容器中的主机设备列表 容器端口映射列表 容器注解列表 容器标签列表 容器根文件系统是否只读 容器资源配额 容器安全上下文配置 创建容器: 调用容器运行时创建容器，返回成功创建的容器id，其中入参为：沙箱（pause容器）元数据 + 容器元数据 预启动容器: 设置pod容器的cpu管理策略与拓扑管理策略。 生成容器引用信息: 根据pod与容器实例元数据生成容器引用，引用用于报告事件，如创建、失败等。 源码部分 func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { ... // Step 2: create the container. ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil { klog.Errorf(\"Can't make a ref to pod %q, container %v: %v\", format.Pod(pod), container.Name, err) } klog.V(4).Infof(\"Generating ref for container %s: %#v\", container.Name, ref) // For a new container, the RestartCount should be 0 // 如果该容器存在，则重启次数加1 restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil { restartCount = containerStatus.RestartCount + 1 } // 生成容器配置-获取临时容器ID target, err := spec.getTargetID(podStatus) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return s.Message(), ErrCreateContainerConfig } containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, podIPs, target) if cleanupAction != nil { defer cleanupAction() } if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return s.Message(), ErrCreateContainerConfig } containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return s.Message(), ErrCreateContainer } err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Internal PreStartContainer hook failed: %v\", s.Message()) return s.Message(), ErrPreStartHook } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.CreatedContainer, fmt.Sprintf(\"Created container %s\", container.Name)) if ref != nil { m.containerRefManager.SetRef(kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, }, ref) } ... } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/01设置容器重启次数.html":{"url":"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/01设置容器重启次数.html","title":"01设置容器重启次数","keywords":"","body":"设置容器重启次数 基于kubernetes v1.18.6，关于基于windows平台运行kubelet的相关代码逻辑不作解析。 概述 kubelet通过以下四个步骤，来启动pod容器： 拉取镜像 创建容器 启动容器 执行容器启动后的钩子 其中创建容器又分为以下子步骤： 设置容器重启次数 生成创建容器所需配置 创建容器 预启动容器 生成容器引用信息 本文主要解析创建容器/设置容器重启次数阶段kubelet所做工作，首先我们先看下设置容器重启次数阶段的代码逻辑 设置容器重启次数 流程解析 该步骤根据容器名称查询pod的status中容器状态，若查询不到则重启次数设置为0，如查询到该容器状态则重启次数基于原值加1。 该值主要用来生成日志文件链接: // kubelet管理的日志（软链接） /var/log/pods/__//重启重启次数.log 对应 // 容器日志 /var/lib/docker/containers//-json.log 源码实现 kubernetes\\pkg\\kubelet\\kuberuntime\\kuberuntime_container.go func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { ... // For a new container, the RestartCount should be 0 restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil { restartCount = containerStatus.RestartCount + 1 } ... } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/02生成创建容器所需配置.html":{"url":"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/02生成创建容器所需配置.html","title":"02生成创建容器所需配置","keywords":"","body":"生成创建容器所需配置 基于kubernetes v1.18.6，关于基于windows平台运行kubelet的相关代码逻辑不作解析。 概述 kubelet通过以下四个步骤，来启动pod容器： 拉取镜像 创建容器 启动容器 执行容器启动后的钩子 其中创建容器又分为以下子步骤： 设置容器重启次数 生成创建容器所需配置 创建容器 预启动容器 生成容器引用信息 本文主要解析创建容器/生成创建容器所需配置阶段kubelet所做工作，首先我们先看下生成创建容器所需配置阶段的代码逻辑 func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { ... // 生成容器配置-获取临时容器ID target, err := spec.getTargetID(podStatus) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return s.Message(), ErrCreateContainerConfig } containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, podIPs, target) if cleanupAction != nil { defer cleanupAction() } if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return s.Message(), ErrCreateContainerConfig } ... } 其核心调用为m.generateContainerConfig()，接下来我们对其深入分析： 配置生成逻辑解析 m.generateContainerConfig()主要逻辑如下 生成创建容器所需配置 根据镜像名称，调用容器运行时，获取运行容器启动命令的用户 检测运行容器启动命令的用户判是否违反pod安全上下文设置（runAsNonRoot: true时，不允许容器以root用户启动） 生成日志目录（格式为: /var/log/pods/__/） 针对windows平台，定义额外配置 定义容器内的环境变量 组装配置项并返回 源码实现: kubernetes\\pkg\\kubelet\\kuberuntime\\kuberuntime_container.go // generateContainerConfig generates container config for kubelet runtime v1. func (m *kubeGenericRuntimeManager) generateContainerConfig(container *v1.Container, pod *v1.Pod, restartCount int, podIP, imageRef string, podIPs []string, nsTarget *kubecontainer.ContainerID) (*runtimeapi.ContainerConfig, func(), error) { // 生成创建容器所需配置：环境变量列表、挂载点信息列表、映射到容器中的主机设备列表、容器端口映射列表、容器注解列表、容器根文件系统是否只读、主机名、 // opts, cleanupAction, err := m.runtimeHelper.GenerateRunContainerOptions(pod, container, podIP, podIPs) if err != nil { return nil, nil, err } // 根据镜像名称，调用容器运行时，获取运行容器启动命令的用户 uid, username, err := m.getImageUser(container.Image) if err != nil { return nil, cleanupAction, err } // Verify RunAsNonRoot. Non-root verification only supports numeric user. // 检测运行容器启动命令的用户判是否违反pod安全上下文设置（runAsNonRoot: true时，不允许容器以root用户启动） if err := verifyRunAsNonRoot(pod, container, uid, username); err != nil { return nil, cleanupAction, err } // 解析容器的启动命令与参数 command, args := kubecontainer.ExpandContainerCommandAndArgs(container, opts.Envs) // 生成日志目录（格式为: /var/log/pods/__/） logDir := BuildContainerLogsDirectory(pod.Namespace, pod.Name, pod.UID, container.Name) err = m.osInterface.MkdirAll(logDir, 0755) if err != nil { return nil, cleanupAction, fmt.Errorf(\"create container log directory for container %s failed: %v\", container.Name, err) } // 定义pod下容器日志路径：/.log containerLogsPath := buildContainerLogsPath(container.Name, restartCount) restartCountUint32 := uint32(restartCount) // 组装容器配置 config := &runtimeapi.ContainerConfig{ Metadata: &runtimeapi.ContainerMetadata{ Name: container.Name, Attempt: restartCountUint32, }, Image: &runtimeapi.ImageSpec{Image: imageRef}, Command: command, Args: args, WorkingDir: container.WorkingDir, Labels: newContainerLabels(container, pod), Annotations: newContainerAnnotations(container, pod, restartCount, opts), Devices: makeDevices(opts), Mounts: m.makeMounts(opts, container), LogPath: containerLogsPath, Stdin: container.Stdin, StdinOnce: container.StdinOnce, Tty: container.TTY, } // set platform specific configurations. // 针对windows，定义额外配置 if err := m.applyPlatformSpecificContainerConfig(config, container, pod, uid, username, nsTarget); err != nil { return nil, cleanupAction, err } // set environment variables // 定义容器内的环境变量 envs := make([]*runtimeapi.KeyValue, len(opts.Envs)) for idx := range opts.Envs { e := opts.Envs[idx] envs[idx] = &runtimeapi.KeyValue{ Key: e.Name, Value: e.Value, } } config.Envs = envs return config, cleanupAction, nil } 接下来我们分析下generateContainerConfig()的返回值 m.generateContainerConfig()返回值解析 返回值一containerConfig: ContainerConfig对象，容器配置属性。 用以创建容器所需信息，该对象属性根据pod清单文件生成。数据结构如下: 容器原生配置 Metadata: 主要定义容器名称 Image: 运行该容器的镜像 Command: 容器执行的命令 Args: 容器执行的命令的参数 WorkingDir: 容器工作目录（容器运行后执行命令的上下文目录） Envs: 容器的环境变量列表 Mounts: 容器的挂载点集合 Devices: 映射到容器中的主机设备列表 Labels: 容器标签列表（k8s会注入额外的标签，如io.kubernetes.pod.name、 io.kubernetes.pod.uid等） Annotations: 容器注解列表（k8s会注入额外的注解，如io.kubernetes.container.hash、io.kubernetes.container.restartCount等） k8s下容器额外配置 Stdin: 标准输入 StdinOnce Tty Linux: 包含容器配额(LinuxContainerResources)及安全配置(LinuxContainerSecurityContext) // ContainerConfig holds all the required and optional fields for creating a // container. type ContainerConfig struct { // Metadata of the container. This information will uniquely identify the // container, and the runtime should leverage this to ensure correct // operation. The runtime may also use this information to improve UX, such // as by constructing a readable name. Metadata *ContainerMetadata `protobuf:\"bytes,1,opt,name=metadata,proto3\" json:\"metadata,omitempty\"` // Image to use. Image *ImageSpec `protobuf:\"bytes,2,opt,name=image,proto3\" json:\"image,omitempty\"` // Command to execute (i.e., entrypoint for docker) Command []string `protobuf:\"bytes,3,rep,name=command,proto3\" json:\"command,omitempty\"` // Args for the Command (i.e., command for docker) Args []string `protobuf:\"bytes,4,rep,name=args,proto3\" json:\"args,omitempty\"` // Current working directory of the command. WorkingDir string `protobuf:\"bytes,5,opt,name=working_dir,json=workingDir,proto3\" json:\"working_dir,omitempty\"` // List of environment variable to set in the container. Envs []*KeyValue `protobuf:\"bytes,6,rep,name=envs,proto3\" json:\"envs,omitempty\"` // Mounts for the container. Mounts []*Mount `protobuf:\"bytes,7,rep,name=mounts,proto3\" json:\"mounts,omitempty\"` // Devices for the container. Devices []*Device `protobuf:\"bytes,8,rep,name=devices,proto3\" json:\"devices,omitempty\"` // Key-value pairs that may be used to scope and select individual resources. // Label keys are of the form: // label-key ::= prefixed-name | name // prefixed-name ::= prefix '/' name // prefix ::= DNS_SUBDOMAIN // name ::= DNS_LABEL Labels map[string]string `protobuf:\"bytes,9,rep,name=labels,proto3\" json:\"labels,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"` // Unstructured key-value map that may be used by the kubelet to store and // retrieve arbitrary metadata. // // Annotations MUST NOT be altered by the runtime; the annotations stored // here MUST be returned in the ContainerStatus associated with the container // this ContainerConfig creates. // // In general, in order to preserve a well-defined interface between the // kubelet and the container runtime, annotations SHOULD NOT influence // runtime behaviour. Annotations map[string]string `protobuf:\"bytes,10,rep,name=annotations,proto3\" json:\"annotations,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"` // Path relative to PodSandboxConfig.LogDirectory for container to store // the log (STDOUT and STDERR) on the host. // E.g., // PodSandboxConfig.LogDirectory = `/var/log/pods//` // ContainerConfig.LogPath = `containerName/Instance#.log` // // WARNING: Log management and how kubelet should interface with the // container logs are under active discussion in // https://issues.k8s.io/24677. There *may* be future change of direction // for logging as the discussion carries on. LogPath string `protobuf:\"bytes,11,opt,name=log_path,json=logPath,proto3\" json:\"log_path,omitempty\"` // Variables for interactive containers, these have very specialized // use-cases (e.g. debugging). // TODO: Determine if we need to continue supporting these fields that are // part of Kubernetes's Container Spec. Stdin bool `protobuf:\"varint,12,opt,name=stdin,proto3\" json:\"stdin,omitempty\"` StdinOnce bool `protobuf:\"varint,13,opt,name=stdin_once,json=stdinOnce,proto3\" json:\"stdin_once,omitempty\"` Tty bool `protobuf:\"varint,14,opt,name=tty,proto3\" json:\"tty,omitempty\"` // Configuration specific to Linux containers. Linux *LinuxContainerConfig `protobuf:\"bytes,15,opt,name=linux,proto3\" json:\"linux,omitempty\"` // Configuration specific to Windows containers. Windows *WindowsContainerConfig `protobuf:\"bytes,16,opt,name=windows,proto3\" json:\"windows,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } 容器资源配额对象LinuxContainerResources type LinuxContainerResources struct { // CPU CFS (Completely Fair Scheduler) period. Default: 0 (not specified). CpuPeriod int64 `protobuf:\"varint,1,opt,name=cpu_period,json=cpuPeriod,proto3\" json:\"cpu_period,omitempty\"` // CPU CFS (Completely Fair Scheduler) quota. Default: 0 (not specified). CpuQuota int64 `protobuf:\"varint,2,opt,name=cpu_quota,json=cpuQuota,proto3\" json:\"cpu_quota,omitempty\"` // CPU shares (relative weight vs. other containers). Default: 0 (not specified). CpuShares int64 `protobuf:\"varint,3,opt,name=cpu_shares,json=cpuShares,proto3\" json:\"cpu_shares,omitempty\"` // Memory limit in bytes. Default: 0 (not specified). MemoryLimitInBytes int64 `protobuf:\"varint,4,opt,name=memory_limit_in_bytes,json=memoryLimitInBytes,proto3\" json:\"memory_limit_in_bytes,omitempty\"` // OOMScoreAdj adjusts the oom-killer score. Default: 0 (not specified). OomScoreAdj int64 `protobuf:\"varint,5,opt,name=oom_score_adj,json=oomScoreAdj,proto3\" json:\"oom_score_adj,omitempty\"` // CpusetCpus constrains the allowed set of logical CPUs. Default: \"\" (not specified). CpusetCpus string `protobuf:\"bytes,6,opt,name=cpuset_cpus,json=cpusetCpus,proto3\" json:\"cpuset_cpus,omitempty\"` // CpusetMems constrains the allowed set of memory nodes. Default: \"\" (not specified). CpusetMems string `protobuf:\"bytes,7,opt,name=cpuset_mems,json=cpusetMems,proto3\" json:\"cpuset_mems,omitempty\"` // List of HugepageLimits to limit the HugeTLB usage of container per page size. Default: nil (not specified). HugepageLimits []*HugepageLimit `protobuf:\"bytes,8,rep,name=hugepage_limits,json=hugepageLimits,proto3\" json:\"hugepage_limits,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } 容器安全配置对象LinuxContainerSecurityContext // LinuxContainerSecurityContext holds linux security configuration that will be applied to a container. type LinuxContainerSecurityContext struct { // Capabilities to add or drop. Capabilities *Capability `protobuf:\"bytes,1,opt,name=capabilities,proto3\" json:\"capabilities,omitempty\"` // 特权模式下，可以做以下事情: // 1. 所有的linux能力将被添加. // 2. 敏感路径(例如sysfs中的内核模块路径)不会被屏蔽。 // 3. 任何sysfs和procfs都以读写权限挂载。 // 4. Apparmor将不会被配置 // 5. Seccomp将不会被配置 // 6. 设备cgroup不限制对任何设备的访问 // 7. 主机/dev中的所有设备都可以在容器中使用。 // 8. SELinux将不会被配置 Privileged bool `protobuf:\"varint,2,opt,name=privileged,proto3\" json:\"privileged,omitempty\"` // Configurations for the container's namespaces. // Only used if the container uses namespace for isolation. NamespaceOptions *NamespaceOption `protobuf:\"bytes,3,opt,name=namespace_options,json=namespaceOptions,proto3\" json:\"namespace_options,omitempty\"` // SELinux context to be optionally applied. SelinuxOptions *SELinuxOption `protobuf:\"bytes,4,opt,name=selinux_options,json=selinuxOptions,proto3\" json:\"selinux_options,omitempty\"` // 以那个用户运行容器（用户id） RunAsUser *Int64Value `protobuf:\"bytes,5,opt,name=run_as_user,json=runAsUser,proto3\" json:\"run_as_user,omitempty\"` // GID to run the container process as. run_as_group should only be specified // when run_as_user or run_as_username is specified; otherwise, the runtime // MUST error. RunAsGroup *Int64Value `protobuf:\"bytes,12,opt,name=run_as_group,json=runAsGroup,proto3\" json:\"run_as_group,omitempty\"` // 以那个用户运行容器（用户名称，该用户必需存在，不会自动创建） RunAsUsername string `protobuf:\"bytes,6,opt,name=run_as_username,json=runAsUsername,proto3\" json:\"run_as_username,omitempty\"` // 根文件系统是否只读 ReadonlyRootfs bool `protobuf:\"varint,7,opt,name=readonly_rootfs,json=readonlyRootfs,proto3\" json:\"readonly_rootfs,omitempty\"` // List of groups applied to the first process run in the container, in // addition to the container's primary GID. SupplementalGroups []int64 `protobuf:\"varint,8,rep,packed,name=supplemental_groups,json=supplementalGroups,proto3\" json:\"supplemental_groups,omitempty\"` // AppArmor profile for the container, candidate values are: // * runtime/default: equivalent to not specifying a profile. // * unconfined: no profiles are loaded // * localhost/: profile loaded on the node // (localhost) by name. The possible profile names are detailed at // http://wiki.apparmor.net/index.php/AppArmor_Core_Policy_Reference ApparmorProfile string `protobuf:\"bytes,9,opt,name=apparmor_profile,json=apparmorProfile,proto3\" json:\"apparmor_profile,omitempty\"` // Seccomp profile for the container, candidate values are: // * runtime/default: the default profile for the container runtime // * unconfined: unconfined profile, ie, no seccomp sandboxing // * localhost/: the profile installed on the node. // is the full path of the profile. // Default: \"\", which is identical with unconfined. SeccompProfilePath string `protobuf:\"bytes,10,opt,name=seccomp_profile_path,json=seccompProfilePath,proto3\" json:\"seccomp_profile_path,omitempty\"` // no_new_privs defines if the flag for no_new_privs should be set on the // container. NoNewPrivs bool `protobuf:\"varint,11,opt,name=no_new_privs,json=noNewPrivs,proto3\" json:\"no_new_privs,omitempty\"` // 需要隐藏的路径 MaskedPaths []string `protobuf:\"bytes,13,rep,name=masked_paths,json=maskedPaths,proto3\" json:\"masked_paths,omitempty\"` // readonly_paths is a slice of paths that should be set as readonly by the // container runtime, this can be passed directly to the OCI spec. ReadonlyPaths []string `protobuf:\"bytes,14,rep,name=readonly_paths,json=readonlyPaths,proto3\" json:\"readonly_paths,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } 返回值二cleanupAction: 容器带有子路径的卷成功运行或启动失败后的回调函数 返回值三为异常（error） GenerateRunContainerOptions()函数解析 函数逻辑如下： 生成RunContainerOptions对象，赋值以下字段: Devices: 设备列表 Annotations: 注释列表 PortMappings: 端口映射列表 Envs: 环境变量列表（从ConfigMap、Secret中获取） Hostname: 主机名称（默认pod名称，可通过.spec.hostname设置，格式必须为: 由小写字母数字字符或'-'组成，并且必须以字母数字字符开始和结束，并且小于等于63个字符。如果大于63个字符会自动截取） EnableHostUserNamespace: 是否使用host命名空间 PodContainerDir: 源码实现 kubernetes\\pkg\\kubelet\\kubelet_pods.go // GenerateRunContainerOptions generates the RunContainerOptions, which can be used by // the container runtime to set parameters for launching a container. func (kl *Kubelet) GenerateRunContainerOptions(pod *v1.Pod, container *v1.Container, podIP string, podIPs []string) (*kubecontainer.RunContainerOptions, func(), error) { opts, err := kl.containerManager.GetResources(pod, container) if err != nil { return nil, nil, err } // 定义pod的hostname与hostDomainName hostname, hostDomainName, err := kl.GeneratePodHostNameAndDomain(pod) if err != nil { return nil, nil, err } opts.Hostname = hostname podName := volumeutil.GetUniquePodName(pod) volumes := kl.volumeManager.GetMountedVolumesForPod(podName) opts.PortMappings = kubecontainer.MakePortMappings(container) blkutil := volumepathhandler.NewBlockVolumePathHandler() blkVolumes, err := kl.makeBlockVolumes(pod, container, volumes, blkutil) if err != nil { return nil, nil, err } opts.Devices = append(opts.Devices, blkVolumes...) envs, err := kl.makeEnvironmentVariables(pod, container, podIP, podIPs) if err != nil { return nil, nil, err } opts.Envs = append(opts.Envs, envs...) // only podIPs is sent to makeMounts, as podIPs is populated even if dual-stack feature flag is not enabled. mounts, cleanupAction, err := makeMounts(pod, kl.getPodDir(pod.UID), container, hostname, hostDomainName, podIPs, volumes, kl.hostutil, kl.subpather, opts.Envs) if err != nil { return nil, cleanupAction, err } opts.Mounts = append(opts.Mounts, mounts...) // adding TerminationMessagePath on Windows is only allowed if ContainerD is used. Individual files cannot // be mounted as volumes using Docker for Windows. supportsSingleFileMapping := kl.containerRuntime.SupportsSingleFileMapping() if len(container.TerminationMessagePath) != 0 && supportsSingleFileMapping { p := kl.getPodContainerDir(pod.UID, container.Name) if err := os.MkdirAll(p, 0750); err != nil { klog.Errorf(\"Error on creating %q: %v\", p, err) } else { opts.PodContainerDir = p } } // only do this check if the experimental behavior is enabled, otherwise allow it to default to false if kl.experimentalHostUserNamespaceDefaulting { opts.EnableHostUserNamespace = kl.enableHostUserNamespace(pod) } return opts, cleanupAction, nil } GenerateRunContainerOptions函数涉及几个比较重要的调用，我们逐一解析: makeBlockVolumes(): 解析pod内容器定义的原生块设备 makeMounts(): 解析pod内容器定义的卷挂载 makeBlockVolumes()函数解析 首先我们先介绍下块设备与kubernetes下原生的块设备使用 什么是块设备？ 块设备允许对固定大小的块中的数据进行随机访问。硬盘驱动器、SSD和CD-ROM驱动器都是块设备的例子。 通常，持久性存储是在通过在块设备（例如磁盘或SSD）之上构造文件系统（例如ext4）的分层方式实现的。 这样应用程序就可以读写文件而不是直接操作数据块。操作系统负责使用指定的文件系统将文件读写转换为对底层设备的数据块读写。 值得注意的是，整个磁盘都是块设备，磁盘分区也是如此，存储区域网络（SAN）设备中的LUN也是一样的。 什么场景需要使用原生的块设备呢？ 有些特殊的应用程序需要直接访问块设备，原因例如，文件系统层会引入不必要的开销。最常见的情况是数据库，通常会直接在底层存储上组织数据。 原生的块设备（Raw Block Devices）还通常由能自己实现某种存储服务的软件（软件定义的存储系统）使用。 从程序员的角度来看，块设备是一个非常大的字节数组，具有某种最小读写粒度，通常为512个字节，大部分情况为4K或更大。 随着在Kubernetes中运行数据库软件和存储基础架构软件变得越来越普遍，在Kubernetes中支持原生块设备的需求变得越来越重要。 首先我们先通过以下例子，了解原生块设备使用方式。（CSI插件需要支持块设备创建） 原生块设备使用样例 创建pvc，其中kubernetes-csi-rbd-sc为ceph rbd类型 $ cat pod定义使用my-pvc卷 $ cat 进入pod内查看块设备 $ kubectl exec -it my-pod -- sh / # ls -l /dev/block brwxrwxrwx 1 root disk 252, 352 Nov 22 05:48 /dev/block 我们发现容器内部确实为块设备类型，这里介绍原生块设备是为了下面对其讨论。 接下来我们分析下，makeBlockVolumes()函数具体做了哪些操作： makeBlockVolumes()函数源码 kubernetes\\pkg\\kubelet\\kubelet_pods.go func (kl *Kubelet) makeBlockVolumes(pod *v1.Pod, container *v1.Container, podVolumes kubecontainer.VolumeMap, blkutil volumepathhandler.BlockVolumePathHandler) ([]kubecontainer.DeviceInfo, error) { var devices []kubecontainer.DeviceInfo for _, device := range container.VolumeDevices { // check path is absolute if !filepath.IsAbs(device.DevicePath) { return nil, fmt.Errorf(\"error DevicePath `%s` must be an absolute path\", device.DevicePath) } vol, ok := podVolumes[device.Name] if !ok || vol.BlockVolumeMapper == nil { klog.Errorf(\"Block volume cannot be satisfied for container %q, because the volume is missing or the volume mapper is nil: %+v\", container.Name, device) return nil, fmt.Errorf(\"cannot find volume %q to pass into container %q\", device.Name, container.Name) } // Get a symbolic link associated to a block device under pod device path dirPath, volName := vol.BlockVolumeMapper.GetPodDeviceMapPath() symlinkPath := path.Join(dirPath, volName) if islinkExist, checkErr := blkutil.IsSymlinkExist(symlinkPath); checkErr != nil { return nil, checkErr } else if islinkExist { // Check readOnly in PVCVolumeSource and set read only permission if it's true. permission := \"mrw\" if vol.ReadOnly { permission = \"r\" } klog.V(4).Infof(\"Device will be attached to container %q. Path on host: %v\", container.Name, symlinkPath) devices = append(devices, kubecontainer.DeviceInfo{PathOnHost: symlinkPath, PathInContainer: device.DevicePath, Permissions: permission}) } } return devices, nil } makeBlockVolumes()函数主要遍历容器内声明的原生块设备列表，并执行以下操作: 判断声明的块设备挂载路径（devicePath）是否为绝对路径（如: /dev/block），如果非绝对路径（block）返回异常。 判断卷组（spec.volumes）内是否含有该设备的pvc（卷声明） 映射主机上块设备路径与容器内路径: 块设备链接如下： $ ls -l /var/lib/kubelet/pods/66d92c5f-ef2f-40a4-9e6c-bc46235db4cb/volumeDevices/kubernetes.io~csi/pvc-26cf725d-be5b-4ba8-9d59-540a35014df1 lrwxrwxrwx 1 root root 142 Nov 22 13:48 /var/lib/kubelet/pods/66d92c5f-ef2f-40a4-9e6c-bc46235db4cb/volumeDevices/kubernetes.io~csi/pvc-26cf725d-be5b-4ba8-9d59-540a35014df1 -> /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-26cf725d-be5b-4ba8-9d59-540a35014df1/66d92c5f-ef2f-40a4-9e6c-bc46235db4cb $ ls -l /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-26cf725d-be5b-4ba8-9d59-540a35014df1/66d92c5f-ef2f-40a4-9e6c-bc46235db4cb brwxrwxrwx 1 root disk 252, 352 Nov 22 13:48 /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-26cf725d-be5b-4ba8-9d59-540a35014df1/66d92c5f-ef2f-40a4-9e6c-bc46235db4cb 格式为: src: /{kubelet data dir}/pods/{podUid}/{DefaultKubeletVolumeDevicesDirName}/{escapeQualifiedPluginName}/, {volumeName} dst: /var/lib/kubelet/plugins/kubernetes.io/{PluginName}/{DefaultKubeletVolumeDevicesDirName}/{volumePluginDependentPath}/{pod uuid} 我们可以在宿主机对该块设备操作: $ fdisk /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-26cf725d-be5b-4ba8-9d59-540a35014df1/66d92c5f-ef2f-40a4-9e6c-bc46235db4cb Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): n Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p Partition number (1-4, default 1): 1 First sector (2048-2097151, default 2048): Using default value 2048 Last sector, +sectors or +size{K,M,G} (2048-2097151, default 2097151): Using default value 2097151 Partition 1 of type Linux and of size 1023 MiB is set Command (m for help): w The partition table has been altered! Calling ioctl() to re-read partition table. Syncing disks. 注意：这里只初始化RunContainerOptions.Devices的值（检测容器中.volumeDevices字段的合法性，并赋值给RunContainerOptions.Devices数组），并不会执行具体操作。 思考一个问题：为什么会在pod宿主机上，创建出一个块设备？ 由于没有看过CSI的源码，这里推测：这个块设备应该是起到桥梁的作用(桥接CSI Agent与pod容器内的原生块设备)，本质为链接而非实体，块设备实体由CSI管理。 对应关系可能如下: pod内容器的原生块设备 -> pod宿主机上kubelet数据目录下创建块设备 CSI makeMounts()函数解析 该函数，包含一个cleanupAction返回值，该函数为清理卷的subPath函数，容器启动阶段该值为空。该返回值即为上文m.runtimeHelper.GenerateRunContainerOptions()函数调用的返回值。 主要逻辑为遍历pod下容器的卷，执行以下操作: 判断pod下卷组是否含有容器所定义的卷,不存在返回异常 如果卷支持SELinux，并且它还没有被重新标记，而且它不是只读卷，重新标记它并将其标记为已标记卷 判断卷的挂载路径（volumeMounts.mountPath）是否为空，为空的话返回异常 解析volumeMounts.subPath与volumeMounts.subPathExpr（同一个卷只能存在其中一个字段，否则异常返回）： 当卷volumeMounts.subPathExpr不为空时，需开启VolumeSubpath、VolumeSubpathEnvExpansion特性门控（v1.18.6默认开启），否则返回异常 当卷volumeMounts.subPath不为空时，需开启VolumeSubpath特性门控（v1.18.6默认开启），否则返回异常 当卷volumeMounts.subPath不为空时，值不能为绝对路径，否则返回异常 当卷volumeMounts.subPath不为空时，值不能包含..（如: /opt/../root/1.yaml），否则返回异常 当卷volumeMounts.subPath不为空时，拼接挂载路径值(如：volumeMounts.mountPath为/opt，volumeMounts.subPath为/opt/1.yaml，最终挂载路径为/opt/1.yaml)，容器内不能存在该路径(/opt/1.yaml)，否则返回异常 解析pod的spec.hostAliases数组，写入容器的/etc/hosts内。如果该Pod使用主机网络命名空间，主机的/etc/hosts内容也将写入容器的/etc/hosts内 值得注意的是NSA&CISA发布的Kubernetes加固指南 认为子路径存在安全隐患，不建议使用 源码实现 `` // makeMounts determines the mount points for the given container. func makeMounts(pod *v1.Pod, podDir string, container *v1.Container, hostName, hostDomain string, podIPs []string, podVolumes kubecontainer.VolumeMap, hu hostutil.HostUtils, subpather subpath.Interface, expandEnvs []kubecontainer.EnvVar) ([]kubecontainer.Mount, func(), error) { // Kubernetes only mounts on /etc/hosts if: // - container is not an infrastructure (pause) container // - container is not already mounting on /etc/hosts // - OS is not Windows // Kubernetes will not mount /etc/hosts if: // - when the Pod sandbox is being created, its IP is still unknown. Hence, PodIP will not have been set. mountEtcHostsFile := len(podIPs) > 0 && runtime.GOOS != \"windows\" klog.V(3).Infof(\"container: %v/%v/%v podIPs: %q creating hosts mount: %v\", pod.Namespace, pod.Name, container.Name, podIPs, mountEtcHostsFile) mounts := []kubecontainer.Mount{} var cleanupAction func() for i, mount := range container.VolumeMounts { // do not mount /etc/hosts if container is already mounting on the path mountEtcHostsFile = mountEtcHostsFile && (mount.MountPath != etcHostsPath) vol, ok := podVolumes[mount.Name] if !ok || vol.Mounter == nil { klog.Errorf(\"Mount cannot be satisfied for container %q, because the volume is missing (ok=%v) or the volume mounter (vol.Mounter) is nil (vol=%+v): %+v\", container.Name, ok, vol, mount) return nil, cleanupAction, fmt.Errorf(\"cannot find volume %q to mount into container %q\", mount.Name, container.Name) } relabelVolume := false // If the volume supports SELinux and it has not been // relabeled already and it is not a read-only volume, // relabel it and mark it as labeled if vol.Mounter.GetAttributes().Managed && vol.Mounter.GetAttributes().SupportsSELinux && !vol.SELinuxLabeled { vol.SELinuxLabeled = true relabelVolume = true } hostPath, err := volumeutil.GetPath(vol.Mounter) if err != nil { return nil, cleanupAction, err } subPath := mount.SubPath if mount.SubPathExpr != \"\" { if !utilfeature.DefaultFeatureGate.Enabled(features.VolumeSubpath) { return nil, cleanupAction, fmt.Errorf(\"volume subpaths are disabled\") } if !utilfeature.DefaultFeatureGate.Enabled(features.VolumeSubpathEnvExpansion) { return nil, cleanupAction, fmt.Errorf(\"volume subpath expansion is disabled\") } subPath, err = kubecontainer.ExpandContainerVolumeMounts(mount, expandEnvs) if err != nil { return nil, cleanupAction, err } } if subPath != \"\" { if !utilfeature.DefaultFeatureGate.Enabled(features.VolumeSubpath) { return nil, cleanupAction, fmt.Errorf(\"volume subpaths are disabled\") } if filepath.IsAbs(subPath) { return nil, cleanupAction, fmt.Errorf(\"error SubPath `%s` must not be an absolute path\", subPath) } err = volumevalidation.ValidatePathNoBacksteps(subPath) if err != nil { return nil, cleanupAction, fmt.Errorf(\"unable to provision SubPath `%s`: %v\", subPath, err) } volumePath := hostPath hostPath = filepath.Join(volumePath, subPath) if subPathExists, err := hu.PathExists(hostPath); err != nil { klog.Errorf(\"Could not determine if subPath %s exists; will not attempt to change its permissions\", hostPath) } else if !subPathExists { // Create the sub path now because if it's auto-created later when referenced, it may have an // incorrect ownership and mode. For example, the sub path directory must have at least g+rwx // when the pod specifies an fsGroup, and if the directory is not created here, Docker will // later auto-create it with the incorrect mode 0750 // Make extra care not to escape the volume! perm, err := hu.GetMode(volumePath) if err != nil { return nil, cleanupAction, err } if err := subpather.SafeMakeDir(subPath, volumePath, perm); err != nil { // Don't pass detailed error back to the user because it could give information about host filesystem klog.Errorf(\"failed to create subPath directory for volumeMount %q of container %q: %v\", mount.Name, container.Name, err) return nil, cleanupAction, fmt.Errorf(\"failed to create subPath directory for volumeMount %q of container %q\", mount.Name, container.Name) } } hostPath, cleanupAction, err = subpather.PrepareSafeSubpath(subpath.Subpath{ VolumeMountIndex: i, Path: hostPath, VolumeName: vol.InnerVolumeSpecName, VolumePath: volumePath, PodDir: podDir, ContainerName: container.Name, }) if err != nil { // Don't pass detailed error back to the user because it could give information about host filesystem klog.Errorf(\"failed to prepare subPath for volumeMount %q of container %q: %v\", mount.Name, container.Name, err) return nil, cleanupAction, fmt.Errorf(\"failed to prepare subPath for volumeMount %q of container %q\", mount.Name, container.Name) } } // Docker Volume Mounts fail on Windows if it is not of the form C:/ if volumeutil.IsWindowsLocalPath(runtime.GOOS, hostPath) { hostPath = volumeutil.MakeAbsolutePath(runtime.GOOS, hostPath) } containerPath := mount.MountPath // IsAbs returns false for UNC path/SMB shares/named pipes in Windows. So check for those specifically and skip MakeAbsolutePath if !volumeutil.IsWindowsUNCPath(runtime.GOOS, containerPath) && !filepath.IsAbs(containerPath) { containerPath = volumeutil.MakeAbsolutePath(runtime.GOOS, containerPath) } propagation, err := translateMountPropagation(mount.MountPropagation) if err != nil { return nil, cleanupAction, err } klog.V(5).Infof(\"Pod %q container %q mount %q has propagation %q\", format.Pod(pod), container.Name, mount.Name, propagation) mustMountRO := vol.Mounter.GetAttributes().ReadOnly mounts = append(mounts, kubecontainer.Mount{ Name: mount.Name, ContainerPath: containerPath, HostPath: hostPath, ReadOnly: mount.ReadOnly || mustMountRO, SELinuxRelabel: relabelVolume, Propagation: propagation, }) } if mountEtcHostsFile { hostAliases := pod.Spec.HostAliases hostsMount, err := makeHostsMount(podDir, podIPs, hostName, hostDomain, hostAliases, pod.Spec.HostNetwork) if err != nil { return nil, cleanupAction, err } mounts = append(mounts, *hostsMount) } return mounts, cleanupAction, nil } 总结 该阶段主要做以下操作： 生成创建容器所需配置 根据镜像名称，调用容器运行时，获取运行容器启动命令的用户 检测运行容器启动命令的用户判是否违反pod安全上下文设置（runAsNonRoot: true时，不允许容器以root用户启动） 生成日志目录（格式为: /var/log/pods/__/） 针对windows平台，定义额外配置 定义容器内的环境变量 组装配置项并返回 参考文章 Raw Block Volume 支持进入 Beta Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/03创建容器.html":{"url":"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/03创建容器.html","title":"03创建容器","keywords":"","body":"生成容器引用信息 基于kubernetes v1.18.6，关于基于windows平台运行kubelet的相关代码逻辑不作解析。 概述 kubelet通过以下四个步骤，来启动pod容器： 拉取镜像 创建容器 启动容器 执行容器启动后的钩子 其中创建容器又分为以下子步骤： 设置容器重启次数 生成创建容器所需配置 创建容器 预启动容器 生成容器引用信息 本文主要解析创建容器/创建容器阶段kubelet所做工作，首先我们先看下创建容器阶段的代码逻辑 func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { ... containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) ... } 接下来我们分析m.runtimeService.CreateContainer()函数调用 m.runtimeService.CreateContainer()函数解析 流程解析 流程很简单：调用容器运行时创建容器，返回容器ID 入参: 沙箱ID 沙箱配置 容器元数据 返回值: 容器ID、异常 源码解析 kubernetes\\pkg\\kubelet\\remote\\remote_runtime.go func (r *RemoteRuntimeService) CreateContainer(podSandBoxID string, config *runtimeapi.ContainerConfig, sandboxConfig *runtimeapi.PodSandboxConfig) (string, error) { ctx, cancel := getContextWithTimeout(r.timeout) defer cancel() resp, err := r.runtimeClient.CreateContainer(ctx, &runtimeapi.CreateContainerRequest{ PodSandboxId: podSandBoxID, Config: config, SandboxConfig: sandboxConfig, }) if err != nil { klog.Errorf(\"CreateContainer in sandbox %q from runtime service failed: %v\", podSandBoxID, err) return \"\", err } if resp.ContainerId == \"\" { errorMessage := fmt.Sprintf(\"ContainerId is not set for container %q\", config.GetMetadata()) klog.Errorf(\"CreateContainer failed: %s\", errorMessage) return \"\", errors.New(errorMessage) } return resp.ContainerId, nil } 关于沙箱 前文我们了解到: kubelet通过调用容器运行时创建容器，返回容器ID。其中入参中有沙箱相关的参数。 那么沙箱到底是什么？ 沙箱其实是CRI定义的。当运行时为docker时，sandbox实质就是pause容器。 pause容器作为一个pod内其他所有容器的父角色，拥有很多pod级别资源，如： DNS配置、命名空间（主机名、IP地址、端口映射列表），父级控制组（pod内容器控制组的父控制组由pause容器定义）等 关于pause容器介绍，建议您移步The Almighty Pause Container PodSandboxConfig沙箱配置对象数据结构解析 type PodSandboxConfig struct { // Metadata of the sandbox. This information will uniquely identify the // sandbox, and the runtime should leverage this to ensure correct // operation. The runtime may also use this information to improve UX, such // as by constructing a readable name. Metadata *PodSandboxMetadata `protobuf:\"bytes,1,opt,name=metadata,proto3\" json:\"metadata,omitempty\"` // Hostname of the sandbox. Hostname could only be empty when the pod // network namespace is NODE. Hostname string `protobuf:\"bytes,2,opt,name=hostname,proto3\" json:\"hostname,omitempty\"` // Path to the directory on the host in which container log files are // stored. // By default the log of a container going into the LogDirectory will be // hooked up to STDOUT and STDERR. However, the LogDirectory may contain // binary log files with structured logging data from the individual // containers. For example, the files might be newline separated JSON // structured logs, systemd-journald journal files, gRPC trace files, etc. // E.g., // PodSandboxConfig.LogDirectory = `/var/log/pods//` // ContainerConfig.LogPath = `containerName/Instance#.log` // // WARNING: Log management and how kubelet should interface with the // container logs are under active discussion in // https://issues.k8s.io/24677. There *may* be future change of direction // for logging as the discussion carries on. LogDirectory string `protobuf:\"bytes,3,opt,name=log_directory,json=logDirectory,proto3\" json:\"log_directory,omitempty\"` // DNS config for the sandbox. DnsConfig *DNSConfig `protobuf:\"bytes,4,opt,name=dns_config,json=dnsConfig,proto3\" json:\"dns_config,omitempty\"` // Port mappings for the sandbox. PortMappings []*PortMapping `protobuf:\"bytes,5,rep,name=port_mappings,json=portMappings,proto3\" json:\"port_mappings,omitempty\"` // Key-value pairs that may be used to scope and select individual resources. Labels map[string]string `protobuf:\"bytes,6,rep,name=labels,proto3\" json:\"labels,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"` // Unstructured key-value map that may be set by the kubelet to store and // retrieve arbitrary metadata. This will include any annotations set on a // pod through the Kubernetes API. // // Annotations MUST NOT be altered by the runtime; the annotations stored // here MUST be returned in the PodSandboxStatus associated with the pod // this PodSandboxConfig creates. // // In general, in order to preserve a well-defined interface between the // kubelet and the container runtime, annotations SHOULD NOT influence // runtime behaviour. // // Annotations can also be useful for runtime authors to experiment with // new features that are opaque to the Kubernetes APIs (both user-facing // and the CRI). Whenever possible, however, runtime authors SHOULD // consider proposing new typed fields for any new features instead. Annotations map[string]string `protobuf:\"bytes,7,rep,name=annotations,proto3\" json:\"annotations,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"` // Optional configurations specific to Linux hosts. Linux *LinuxPodSandboxConfig `protobuf:\"bytes,8,opt,name=linux,proto3\" json:\"linux,omitempty\"` XXX_NoUnkeyedLiteral struct{} `json:\"-\"` XXX_sizecache int32 `json:\"-\"` } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/04预启动容器.html":{"url":"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/04预启动容器.html","title":"04预启动容器","keywords":"","body":"预启动容器 基于kubernetes v1.18.6，关于基于windows平台运行kubelet的相关代码逻辑不作解析。 概述 kubelet通过以下四个步骤，来启动pod容器： 拉取镜像 创建容器 启动容器 执行容器启动后的钩子 其中创建容器又分为以下子步骤： 设置容器重启次数 生成创建容器所需配置 创建容器 预启动容器 生成容器引用信息 本文主要解析创建容器/预启动容器阶段kubelet所做工作，首先我们先看下预启动容器阶段的代码逻辑 预启动逻辑分析 该阶段没有动作发生，也是针对启动关联配置的赋值 预启动容器入口函数源码 kubernetes\\pkg\\kubelet\\kuberuntime\\kuberuntime_container.go func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { ... containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return s.Message(), ErrCreateContainer } err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Internal PreStartContainer hook failed: %v\", s.Message()) return s.Message(), ErrPreStartHook } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.CreatedContainer, fmt.Sprintf(\"Created container %s\", container.Name)) ... 透过源码我们发现，m.internalLifecycle.PreStartContainer(pod, container, containerID)为预启动容器步骤主要逻辑 该阶段主要设置pod容器的cpu管理策略与拓扑管理策略。 cpu管理策略分为: none: 默认策略，表示现有的调度行为。 static: 该策略针对具有整数型CPU requests的Guaranteed Pod，它允许该类Pod中的容器访问节点上的独占CPU资源。 这种独占性是使用cpuset cgroup 控制器 来实现的 关于cpu管理策略请参考控制节点上的 CPU 管理策略 关于拓扑管理策略请参考控制节点上的拓扑管理策略 预启动容器源码实现 func (i *internalContainerLifecycleImpl) PreStartContainer(pod *v1.Pod, container *v1.Container, containerID string) error { if i.cpuManager != nil { err := i.cpuManager.AddContainer(pod, container, containerID) if err != nil { return err } } if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.TopologyManager) { err := i.topologyManager.AddContainer(pod, containerID) if err != nil { return err } } return nil } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/05生成容器引用信息.html":{"url":"2.容器/k8s/core/kubelet/创建pod/05启动容器/02创建容器/05生成容器引用信息.html","title":"05生成容器引用信息","keywords":"","body":"生成容器引用信息 基于kubernetes v1.18.6，关于基于windows平台运行kubelet的相关代码逻辑不作解析。 概述 kubelet通过以下四个步骤，来启动pod容器： 拉取镜像 创建容器 启动容器 执行容器启动后的钩子 其中创建容器又分为以下子步骤： 设置容器重启次数 生成创建容器所需配置 创建容器 预启动容器 生成容器引用信息 本文主要解析创建容器/生成容器引用信息阶段kubelet所做工作，首先我们先看下生成容器引用信息阶段的代码逻辑 源码解析 流程很简单：生成容器引用信息并赋值给kubeGenericRuntimeManager.containerRefManager func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { ... ref, err := kubecontainer.GenerateContainerRef(pod, container) ... if ref != nil { m.containerRefManager.SetRef(kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, }, ref) } ... } 容器引用解析 容器引用是什么？ 本质就是一个map类型集合，以容器ID+运行时类型为key作为索引，以容器的信息（所属pod的uid、名称、命名空间，以及容器在pod内的域等信息）为value RefManager对象解析 RefManager管理容器的引用，是一个线程安全对象，调用者不需要锁。引用用于报告事件，如创建、失败等。 结构体数据结构 type RefManager struct { sync.RWMutex containerIDToRef map[ContainerID]*v1.ObjectReference } ContainerID对象解析 ContainerID是容器引用对象的key，ContainerID为一个结构体类型对象，ContainerID根据运行时（如docker、containered）类型与容器id生成 type ContainerID struct { // The type of the container runtime. e.g. 'docker'. Type string // The identification of the container, this is comsumable by // the underlying container runtime. (Note that the container // runtime interface still takes the whole struct as input). ID string } v1.ObjectReference对象解析 通过以下字段描述容器索引属性，除FieldPath字段，其他字段均取自所属Pod（pod是k8s下最小调度资源） type ObjectReference struct { Kind string `json:\"kind,omitempty\" protobuf:\"bytes,1,opt,name=kind\"` Namespace string `json:\"namespace,omitempty\" protobuf:\"bytes,2,opt,name=namespace\"` Name string `json:\"name,omitempty\" protobuf:\"bytes,3,opt,name=name\"` UID types.UID `json:\"uid,omitempty\" protobuf:\"bytes,4,opt,name=uid,casttype=k8s.io/apimachinery/pkg/types.UID\"` APIVersion string `json:\"apiVersion,omitempty\" protobuf:\"bytes,5,opt,name=apiVersion\"` ResourceVersion string `json:\"resourceVersion,omitempty\" protobuf:\"bytes,6,opt,name=resourceVersion\"` FieldPath string `json:\"fieldPath,omitempty\" protobuf:\"bytes,7,opt,name=fieldPath\"` } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/05启动容器/01拉取镜像.html":{"url":"2.容器/k8s/core/kubelet/创建pod/05启动容器/01拉取镜像.html","title":"01拉取镜像","keywords":"","body":"kubelet启动容器之拉取镜像 概览 kubelet通过以下四个步骤，来启动pod容器： 拉取镜像 创建容器 启动容器 执行容器启动后的钩子 kubelet启动容器的第一步便是拉取镜像，核心源码如下： 启动Pod容器之拉取镜像 kubernetes\\pkg\\kubelet\\kuberuntime\\kuberuntime_container.go func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { container := spec.container // Step 1: pull the image. imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets, podSandboxConfig) if err != nil { s, _ := grpcstatus.FromError(err) m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", s.Message()) return msg, err } ... } 主要逻辑便是调用EnsureImageExists函数，接下来我们对m.imagePuller.EnsureImageExists()函数深入分析 函数入参及返回值解析 入参解析 pod *v1.Pod入参: 当前容器所属pod实例 container *v1.Container: 容器实例（包含镜像名称等元数据信息） pullSecrets []v1.Secret: 拉取镜像所需的凭据 podSandboxConfig *runtimeapi.PodSandboxConfig: pod沙箱配置实例（包含主机名、dns、日志目录等） 返回值解析 返回值一镜像像的引用(镜像摘要或ID，如harbor.wl.io/library/redis:5.0.12): 返回本地存储中容器的镜像引用(摘要或ID)，如果镜像不在本地存储中返回\"\"。 返回值二镜像拉取信息：失败/成功及原因 返回值三异常：拉取镜像过程中产生的异常（如拉取镜像网络不可达） 设置镜像tag默认值 当容器镜像不存在tag时，会设置缺省tag值: latest func (m *imageManager) EnsureImageExists(pod *v1.Pod, container *v1.Container, pullSecrets []v1.Secret, podSandboxConfig *runtimeapi.PodSandboxConfig) (string, string, error) { ... // If the image contains no tag or digest, a default tag should be applied. // 为没有摘要或tag的镜像设置默认tag: latest image, err := applyDefaultImageTag(container.Image) if err != nil { msg := fmt.Sprintf(\"Failed to apply default image tag %q: %v\", container.Image, err) m.logIt(ref, v1.EventTypeWarning, events.FailedToInspectImage, logPrefix, msg, klog.Warning) return \"\", msg, ErrInvalidImageName } ... } 例如：下面spec.containers[0].image只声明了镜像名称，未声明tag $ cat 查看pod事件，发现并没有因为未指定镜像tag而中断 $ kubectl describe pod nginx-with-no-tag ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 119s default-scheduler Successfully assigned default/nginx-with-no-tag to node69 Normal Pulling 118s kubelet, node69 Pulling image \"nginx\" Normal Pulled 74s kubelet, node69 Successfully pulled image \"nginx\" Normal Created 73s kubelet, node69 Created container nginx Normal Started 73s kubelet, node69 Started container nginx 查询本地镜像列表 $ docker images|grep nginx nginx latest ea335eea17ab 44 hours ago 141MB 拉取镜像 1.判断是否需要拉取镜像 以下情况需要拉取镜像： 容器的镜像拉取策略为Always 容器的镜像拉取策略为IfNotPresent，并且本地没有该镜像 如果需要拉取镜像继续后续流程，如果不需要拉取镜像直接返回，并根据以下场景返回对应信息: 容器的镜像拉取策略为Never，并且本地存在该镜像，则返回: 镜像引用信息（即镜像名称，如harbor.wl.io/library/redis:5.0.12） 容器的镜像拉取策略为Never，并且本地不存在该镜像，则返回异常: Container image xxx is not present with pull policy of Never 源码实现 func (m *imageManager) EnsureImageExists(pod *v1.Pod, container *v1.Container, pullSecrets []v1.Secret, podSandboxConfig *runtimeapi.PodSandboxConfig) (string, string, error) { ... spec := kubecontainer.ImageSpec{Image: image} imageRef, err := m.imageService.GetImageRef(spec) if err != nil { msg := fmt.Sprintf(\"Failed to inspect image %q: %v\", container.Image, err) m.logIt(ref, v1.EventTypeWarning, events.FailedToInspectImage, logPrefix, msg, klog.Warning) return \"\", msg, ErrImageInspect } present := imageRef != \"\" // 当容器的镜像拉取策略为Never时只用本地的镜像而非从镜像库拉取 if !shouldPullImage(container, present) { if present { msg := fmt.Sprintf(\"Container image %q already present on machine\", container.Image) m.logIt(ref, v1.EventTypeNormal, events.PulledImage, logPrefix, msg, klog.Info) return imageRef, \"\", nil } msg := fmt.Sprintf(\"Container image %q is not present with pull policy of Never\", container.Image) m.logIt(ref, v1.EventTypeWarning, events.ErrImageNeverPullPolicy, logPrefix, msg, klog.Warning) return \"\", msg, ErrImageNeverPull } ... } 2.判断是否超过镜像拉取时间 --image-pull-progress-deadline值，默认一分钟。 若在此截止日期前未进行镜像拉取，则镜像拉取将被取消。这个特定于docker的标志只在容器运行时被设置为docker时有效 func (m *imageManager) EnsureImageExists(pod *v1.Pod, container *v1.Container, pullSecrets []v1.Secret, podSandboxConfig *runtimeapi.PodSandboxConfig) (string, string, error) { ... backOffKey := fmt.Sprintf(\"%s_%s\", pod.UID, container.Image) if m.backOff.IsInBackOffSinceUpdate(backOffKey, m.backOff.Clock.Now()) { msg := fmt.Sprintf(\"Back-off pulling image %q\", container.Image) m.logIt(ref, v1.EventTypeNormal, events.BackOffPullImage, logPrefix, msg, klog.Info) return \"\", msg, ErrImagePullBackOff } ... } 3.拉取镜像操作 调用容器运行时拉取镜像，并开启垃圾回收。 func (m *imageManager) EnsureImageExists(pod *v1.Pod, container *v1.Container, pullSecrets []v1.Secret, podSandboxConfig *runtimeapi.PodSandboxConfig) (string, string, error) { ... m.logIt(ref, v1.EventTypeNormal, events.PullingImage, logPrefix, fmt.Sprintf(\"Pulling image %q\", container.Image), klog.Info) pullChan := make(chan pullResult) m.puller.pullImage(spec, pullSecrets, pullChan, podSandboxConfig) imagePullResult := Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/05启动容器/03启动容器.html":{"url":"2.容器/k8s/core/kubelet/创建pod/05启动容器/03启动容器.html","title":"03启动容器","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/05启动容器/04执行容器启动后的钩子.html":{"url":"2.容器/k8s/core/kubelet/创建pod/05启动容器/04执行容器启动后的钩子.html","title":"04执行容器启动后的钩子","keywords":"","body":"运行容器启动后的钩子 比如：调用注册中心注册自己 概述 kubelet通过以下四个步骤，来启动pod容器： 拉取镜像 创建容器 启动容器 执行容器启动后的钩子 本文主要解析执行容器启动后的钩子阶段kubelet所做工作，对应pod的配置声明为: apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler > /usr/share/message\"] Kubernetes在容器创建后立即发送postStart事件。 然而，postStart处理函数的调用不保证在容器的入口点（entrypoint） 之前执行。 postStart处理函数与容器的代码是异步执行的，但Kubernetes的容器管理逻辑会一直阻塞等待postStart处理函数执行完毕。 只有postStart处理函数执行完毕，容器的状态才会变成RUNNING。 接下来我们对下述执行容器启动后的钩子阶段的代码逻辑进行解析: 源码实现 func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { ... if container.Lifecycle != nil && container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { m.recordContainerEvent(pod, container, kubeContainerID.ID, v1.EventTypeWarning, events.FailedPostStartHook, msg) if err := m.killContainer(pod, kubeContainerID, container.Name, \"FailedPostStartHook\", nil); err != nil { klog.Errorf(\"Failed to kill container %q(id=%q) in pod %q: %v, %v\", container.Name, kubeContainerID.String(), format.Pod(pod), ErrPostStartHook, err) } return msg, fmt.Errorf(\"%s: %v\", ErrPostStartHook, handlerErr) } } return \"\", nil 流程解析 判断pod是否配置了postStart钩子，若未配置跳过该逻辑 调用postStart钩子处理函数，执行成功返回空异常，执行失败调用pre-stop钩子处理函数并kill掉容器 postStart钩子处理函数实现 当postStart钩子为命令类型时，容器内执行postStart钩子指令，若执行结果非空退出码返回异常 当postStart钩子为http请求类型时，按http://host:port/path格式发送请求，返回请求响应结果 kubernetes\\pkg\\kubelet\\lifecycle\\handlers.go func (hr *HandlerRunner) Run(containerID kubecontainer.ContainerID, pod *v1.Pod, container *v1.Container, handler *v1.Handler) (string, error) { switch { case handler.Exec != nil: var msg string // TODO(tallclair): Pass a proper timeout value. // 容器内执行`postStart`钩子指令，若执行结果非空退出码返回异常 output, err := hr.commandRunner.RunInContainer(containerID, handler.Exec.Command, 0) if err != nil { msg = fmt.Sprintf(\"Exec lifecycle hook (%v) for Container %q in Pod %q failed - error: %v, message: %q\", handler.Exec.Command, container.Name, format.Pod(pod), err, string(output)) klog.V(1).Infof(msg) } return msg, err case handler.HTTPGet != nil: msg, err := hr.runHTTPHandler(pod, container, handler) if err != nil { msg = fmt.Sprintf(\"Http lifecycle hook (%s) for Container %q in Pod %q failed - error: %v, message: %q\", handler.HTTPGet.Path, container.Name, format.Pod(pod), err, msg) klog.V(1).Infof(msg) } return msg, err default: err := fmt.Errorf(\"invalid handler: %v\", handler) msg := fmt.Sprintf(\"Cannot run handler: %v\", err) klog.Errorf(msg) return msg, err } } postStart钩子处理函数执行失败后的逻辑 postStart钩子处理函数执行失败后，将执行preStop钩子处理函数。等待preStop事件处理程序结束或者Pod的--grace-period超时，删除容器 kubernetes\\pkg\\kubelet\\kuberuntime\\kuberuntime_container.go func (m *kubeGenericRuntimeManager) killContainer(pod *v1.Pod, containerID kubecontainer.ContainerID, containerName string, message string, gracePeriodOverride *int64) error { var containerSpec *v1.Container if pod != nil { if containerSpec = kubecontainer.GetContainerSpec(pod, containerName); containerSpec == nil { return fmt.Errorf(\"failed to get containerSpec %q(id=%q) in pod %q when killing container for reason %q\", containerName, containerID.String(), format.Pod(pod), message) } } else { // Restore necessary information if one of the specs is nil. restoredPod, restoredContainer, err := m.restoreSpecsFromContainerLabels(containerID) if err != nil { return err } pod, containerSpec = restoredPod, restoredContainer } // From this point, pod and container must be non-nil. gracePeriod := int64(minimumGracePeriodInSeconds) switch { case pod.DeletionGracePeriodSeconds != nil: gracePeriod = *pod.DeletionGracePeriodSeconds case pod.Spec.TerminationGracePeriodSeconds != nil: gracePeriod = *pod.Spec.TerminationGracePeriodSeconds } if len(message) == 0 { message = fmt.Sprintf(\"Stopping container %s\", containerSpec.Name) } m.recordContainerEvent(pod, containerSpec, containerID.ID, v1.EventTypeNormal, events.KillingContainer, message) // Run internal pre-stop lifecycle hook if err := m.internalLifecycle.PreStopContainer(containerID.ID); err != nil { return err } // Run the pre-stop lifecycle hooks if applicable and if there is enough time to run it if containerSpec.Lifecycle != nil && containerSpec.Lifecycle.PreStop != nil && gracePeriod > 0 { gracePeriod = gracePeriod - m.executePreStopHook(pod, containerID, containerSpec, gracePeriod) } // always give containers a minimal shutdown window to avoid unnecessary SIGKILLs if gracePeriod Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/创建pod沙箱/pod容器日志管理.html":{"url":"2.容器/k8s/core/kubelet/创建pod/创建pod沙箱/pod容器日志管理.html","title":"pod容器日志管理","keywords":"","body":"pod容器日志管理 通过本文，你将了解kubelet是如何管理pod内的容器日志的。 定义pod日志目录 kubelet定义pod日志路径为： /var/log/pods/__ 通过一个例子来验证上面的路径规则 default命名空间下存在一个名为stakater-reloader-598f958967-ddkl7的pod $ kubectl get pod NAME READY STATUS RESTARTS AGE stakater-reloader-598f958967-ddkl7 1/1 Running 7 117d 获取其uid（metadata.uid字段） $ kubectl get pod stakater-reloader-598f958967-ddkl7 -o yaml|grep uid k:{\"uid\":\"ea8b9161-bee3-4bf6-a4e3-65fe256e3771\"}: f:uid: {} uid: ea8b9161-bee3-4bf6-a4e3-65fe256e3771 uid: 2681cb7f-daa4-4620-bb60-1d449709181c 跟据上面信息拼装，我们获取了该pod的日志目录 $ ls /var/log/pods/default_stakater-reloader-598f958967-ddkl7_2681cb7f-daa4-4620-bb60-1d449709181c stakater-reloader 证明上面规则正确 关于源码的实现 kubernetes/pkg/kubelet/kuberuntime/kuberuntime_sandbox.go $ func (m *kubeGenericRuntimeManager) generatePodSandboxConfig(pod *v1.Pod, attempt uint32) (*runtimeapi.PodSandboxConfig, error) { ... logDir := BuildPodLogsDirectory(pod.Namespace, pod.Name, pod.UID) podSandboxConfig.LogDirectory = logDir ... } pod日志目录结构 思考一: pod日志目录下日志文件生成规则 上文我们了解到，kubelet定义pod日志路径为： /var/log/pods/__ 我们观察下其路径结构： $ ls -R /var/log/pods/default_stakater-reloader-598f958967-ddkl7_2681cb7f-daa4-4620-bb60-1d449709181c /var/log/pods/default_stakater-reloader-598f958967-ddkl7_2681cb7f-daa4-4620-bb60-1d449709181c: stakater-reloader /var/log/pods/default_stakater-reloader-598f958967-ddkl7_2681cb7f-daa4-4620-bb60-1d449709181c/stakater-reloader: 1.log 6.log 7.log 其中stakater-reloader是根据容器名称生成的目录 其中1.log 6.log 7.log是根据容器重启次数生成日志文件，格式为: .log 我们看下源码实现: kubernetes/pkg/kubelet/kuberuntime/kuberuntime_container.go ... func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { ... // 1. 生成容器配置 // 其中日志目录规则为： containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, podIPs, target) ... // 2. 拼接容器日志目录 // 其中podSandboxConfig.LogDirectory = /var/log/pods/__ // containerConfig.LogPath = /容器重启次数.log containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) // only create legacy symlink if containerLog path exists (or the error is not IsNotExist). // Because if containerLog path does not exist, only dandling legacySymlink is created. // This dangling legacySymlink is later removed by container gc, so it does not make sense // to create it in the first place. it happens when journald logging driver is used with docker. // 3. 建立链接 if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) { if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil { klog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\", legacySymlink, containerID, containerLog, err) } } ... } kubernetes/pkg/kubelet/kuberuntime/kuberuntime_container.go func (m *kubeGenericRuntimeManager) generateContainerConfig(container *v1.Container, pod *v1.Pod, restartCount int, podIP, imageRef string, podIPs []string, nsTarget *kubecontainer.ContainerID) (*runtimeapi.ContainerConfig, func(), error) { opts, cleanupAction, err := m.runtimeHelper.GenerateRunContainerOptions(pod, container, podIP, podIPs) ... // 拼接容器日志文件路径 containerLogsPath := buildContainerLogsPath(container.Name, restartCount) ... } 其中restartCount重启次数这个值我们可以通过以下方式获取: $ kubectl get pod stakater-reloader-598f958967-ddkl7 -o yaml ... status: ... containerStatuses: - containerID: docker://c73e273fd2886df631739b03fd71a8216054549b11c651a7c8c38ce902342332 ... restartCount: 7 ... 思考二: 日志文件软链接规则 通过观察我们可以发现，这个目录下的日志文件是软链接类型文件: $ ls -l /var/log/pods/default_stakater-reloader-598f958967-ddkl7_2681cb7f-daa4-4620-bb60-1d449709181c/stakater-reloader lrwxrwxrwx 1 root root 165 Jul 31 18:24 1.log -> /var/lib/docker/containers/cc75d79b3aef49f739b03f5fa7379f75c69944cbeb0a4555d3833a26ebfe06b4/cc75d79b3aef49f739b03f5fa7379f75c69944cbeb0a4555d3833a26ebfe06b4-json.log lrwxrwxrwx 1 root root 165 Oct 23 21:56 6.log -> /var/lib/docker/containers/e9bc97e0cdd1e4258f17bb18db976ac371860103b861529cb690005ada97d153/e9bc97e0cdd1e4258f17bb18db976ac371860103b861529cb690005ada97d153-json.log lrwxrwxrwx 1 root root 165 Nov 6 16:08 7.log -> /var/lib/docker/containers/c73e273fd2886df631739b03fd71a8216054549b11c651a7c8c38ce902342332/c73e273fd2886df631739b03fd71a8216054549b11c651a7c8c38ce902342332-json.log 其实是kubelet在启动容器后，生成的链接。 通过样例，我们不难发现链接匹配的规则如下：(适用于docker运行时，且为json日志插件) /var/log/pods/__//重启重启次数.log 指向 /var/lib/docker/containers//-json.log 其中容器id我们可以通过以下方式获取 $ kubectl get pod stakater-reloader-598f958967-zbn2g -o yaml|grep -e \"- containerID\" - containerID: docker://fed0bdb0183c8bcfd1c91090d96e3c594c588de94f89f68ff24a8fe7f940e50e 源码实现 日志文件的软链接设置逻辑实际发生于容器启动后： /var/log/pods/__//重启重启次数.log 指向 /var/lib/docker/containers//-json.log kubelet启动容器核心源码：kubernetes/pkg/kubelet/dockershim/docker_container.go func (ds *dockerService) StartContainer(_ context.Context, r *runtimeapi.StartContainerRequest) (*runtimeapi.StartContainerResponse, error) { err := ds.client.StartContainer(r.ContainerId) // Create container log symlink for all containers (including failed ones). if linkError := ds.createContainerLogSymlink(r.ContainerId); linkError != nil { // Do not stop the container if we failed to create symlink because: // 1. This is not a critical failure. // 2. We don't have enough information to properly stop container here. // Kubelet will surface this error to user via an event. return nil, linkError } if err != nil { err = transformStartContainerError(err) return nil, fmt.Errorf(\"failed to start container %q: %v\", r.ContainerId, err) } return &runtimeapi.StartContainerResponse{}, nil } 其中ds.createContainerLogSymlink(r.ContainerId)便是创建日志文件软链接的逻辑，我们接下来对其进行深入分析 容器日志链接创建流程解析 创建容器日志软链接，主要分为三个步骤: 根据容器ID，解析容器的真实日志路径，对应返回值realPath $ docker inspect cff000d9fc5e -f \"{{ .LogPath }}\" /var/lib/docker/containers/cff000d9fc5edad5a8b042b8879fedd1d7de978b928c522a53c11b3217c220df/cff000d9fc5edad5a8b042b8879fedd1d7de978b928c522a53c11b3217c220df-json.log 根据容器ID，解析容器的pod日志路径标签，对应返回值path $ docker inspect cff000d9fc5e -f '{{ index .Config.Labels \"io.kubernetes.container.logpath\" }}' /var/log/pods/istio-system_istio-ingressgateway-c694cfd-cgk2n_c6ea97b6-d858-4c4c-8d83-17477599aebf/istio-proxy/4.log 根据前两步获取的path与realPath创建文件链接，等同于 $ ln -s $realPath $path 源码解析其实现 其中path, realPath, err := ds.getContainerLogPath(containerID)为获取容器信息逻辑，关于返回值解析: path: 容器的Config.Labels[\"io.kubernetes.container.logpath\"]字段，该容器所属pod日志路径（实质为真实日志路径的软链接） realPath: 该容器的真实日志路径 kubernetes/pkg/kubelet/dockershim/docker_container.go源码 // createContainerLogSymlink creates the symlink for docker container log. func (ds *dockerService) createContainerLogSymlink(containerID string) error { path, realPath, err := ds.getContainerLogPath(containerID) if err != nil { return fmt.Errorf(\"failed to get container %q log path: %v\", containerID, err) } if path == \"\" { klog.V(5).Infof(\"Container %s log path isn't specified, will not create the symlink\", containerID) return nil } if realPath != \"\" { // Only create the symlink when container log path is specified and log file exists. // Delete possibly existing file first if err = ds.os.Remove(path); err == nil { klog.Warningf(\"Deleted previously existing symlink file: %q\", path) } if err = ds.os.Symlink(realPath, path); err != nil { return fmt.Errorf(\"failed to create symbolic link %q to the container log file %q for container %q: %v\", path, realPath, containerID, err) } } else { supported, err := ds.IsCRISupportedLogDriver() if err != nil { klog.Warningf(\"Failed to check supported logging driver by CRI: %v\", err) return nil } if supported { klog.Warningf(\"Cannot create symbolic link because container log file doesn't exist!\") } else { klog.V(5).Infof(\"Unsupported logging driver by CRI\") } } return nil } 其中函数getContainerLogPath()源码如下: func (ds *dockerService) getContainerLogPath(containerID string) (string, string, error) { info, err := ds.client.InspectContainer(containerID) if err != nil { return \"\", \"\", fmt.Errorf(\"failed to inspect container %q: %v\", containerID, err) } return info.Config.Labels[containerLogPathLabelKey], info.LogPath, nil } 其中函数createContainerLogSymlink()源码如下: func (ds *dockerService) getContainerLogPath(containerID string) (string, string, error) { info, err := ds.client.InspectContainer(containerID) if err != nil { return \"\", \"\", fmt.Errorf(\"failed to inspect container %q: %v\", containerID, err) } return info.Config.Labels[containerLogPathLabelKey], info.LogPath, nil } ds.os.Symlink(realPath, path)为调用系统接口，创建日志文件软链接 总结 通过上述分析，我们可以得出以下结论（docker运行时下）: k8s下容器日志目录为：/var/log/pods/__//重启重启次数.log，并且是文件 /var/lib/docker/containers//-json.log的链接。 pod会按容器的重启次数对应保留日志，具体保留个数应该与GC策略有关 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/01计算沙箱与容器的变化.html":{"url":"2.容器/k8s/core/kubelet/创建pod/01计算沙箱与容器的变化.html","title":"01计算沙箱与容器的变化","keywords":"","body":"计算沙箱与容器的变化 该阶段主要检测沙箱容器（pause）与其他容器（初始化容器、临时容器、常规容器）的变化，也就是pod期望运行状态与实际运行状态。 当对pod的操作为创建时，显然不存在实际运行状态。 podActions是一个记录pod期望运行状态与实际运行状态的对象 当新建pod时，以下字段取值如下： KillPod: 是否停止pod所有容器（pause容器、初始化容器、临时容器、常规容器），该值为true CreateSandbox: 是否创建沙箱，该值为true Attempt: 尝试创建pod沙箱次数，该值为0、 ContainersToStart: 一般容器数组，，该值对应pod的spec.containers[x]数组 NextInitContainerToStart: 下一个将启动的初始化容器，该值对应pod的spec.initContainers[0] // podActions keeps information what to do for a pod. type podActions struct { // Stop all running (regular, init and ephemeral) containers and the sandbox for the pod. KillPod bool // Whether need to create a new sandbox. If needed to kill pod and create // a new pod sandbox, all init containers need to be purged (i.e., removed). CreateSandbox bool // The id of existing sandbox. It is used for starting containers in ContainersToStart. SandboxID string // The attempt number of creating sandboxes for the pod. Attempt uint32 // The next init container to start. NextInitContainerToStart *v1.Container // ContainersToStart keeps a list of indexes for the containers to start, // where the index is the index of the specific container in the pod spec ( // pod.Spec.Containers. ContainersToStart []int // ContainersToKill keeps a map of containers that need to be killed, note that // the key is the container ID of the container, while // the value contains necessary information to kill a container. ContainersToKill map[kubecontainer.ContainerID]containerToKillInfo // EphemeralContainersToStart is a list of indexes for the ephemeral containers to start, // where the index is the index of the specific container in pod.Spec.EphemeralContainers. EphemeralContainersToStart []int } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/创建pod/04创建pod沙箱.html":{"url":"2.容器/k8s/core/kubelet/创建pod/04创建pod沙箱.html","title":"04创建pod沙箱","keywords":"","body":"创建pod沙箱 这里的pod沙箱其实就是pod pause容器 pod沙箱创建流程 流程解析 pod沙箱创建流程，主要分为以下步骤: 生成沙箱容器配置，配置包含： 沙箱元数据（基于元数据生成沙箱唯一标识） pod沙箱名称（取值pod名称） pod沙箱命名空间（取值pod命名空间） uid（取值pod uid） pod沙箱主机名（一般取值pod名称） pod沙箱日志目录 pod沙箱DNS配置（DNS server、search domain） pod沙箱端口映射列表（遍历pod下容器端口映射获得） pod沙箱标签集合 pod定义标签: metadata.labels字段 kubelet注入标签: io.kubernetes.pod.name、io.kubernetes.pod.namespace、io.kubernetes.pod.uid pod沙箱注释集合: metadata.annonations字段 linux相关配置: 系统调用、SELinuxOptions等 创建pod沙箱日志目录(/var/log/pods/__) 获取容器运行时 根据pod容器运行时、pod沙箱配置创建pod沙箱 源码解析 kubernetes\\pkg\\kubelet\\kuberuntime\\kuberuntime_sandbox.go func (m *kubeGenericRuntimeManager) createPodSandbox(pod *v1.Pod, attempt uint32) (string, string, error) { podSandboxConfig, err := m.generatePodSandboxConfig(pod, attempt) if err != nil { message := fmt.Sprintf(\"GeneratePodSandboxConfig for pod %q failed: %v\", format.Pod(pod), err) klog.Error(message) return \"\", message, err } // Create pod logs directory err = m.osInterface.MkdirAll(podSandboxConfig.LogDirectory, 0755) if err != nil { message := fmt.Sprintf(\"Create pod log directory for pod %q failed: %v\", format.Pod(pod), err) klog.Errorf(message) return \"\", message, err } runtimeHandler := \"\" if utilfeature.DefaultFeatureGate.Enabled(features.RuntimeClass) && m.runtimeClassManager != nil { runtimeHandler, err = m.runtimeClassManager.LookupRuntimeHandler(pod.Spec.RuntimeClassName) if err != nil { message := fmt.Sprintf(\"CreatePodSandbox for pod %q failed: %v\", format.Pod(pod), err) return \"\", message, err } if runtimeHandler != \"\" { klog.V(2).Infof(\"Running pod %s with RuntimeHandler %q\", format.Pod(pod), runtimeHandler) } } podSandBoxID, err := m.runtimeService.RunPodSandbox(podSandboxConfig, runtimeHandler) if err != nil { message := fmt.Sprintf(\"CreatePodSandbox for pod %q failed: %v\", format.Pod(pod), err) klog.Error(message) return \"\", message, err } return podSandBoxID, \"\", nil } 从上面代码中我们发现有个RuntimeClass特性，这个特性是干嘛的？ RuntimeClass是什么？ 背景介绍 Kubernetes最初是为了支持在Linux主机上运行本机应用程序的Docker容器而创建的。 从Kubernetes 1.3中的rkt开始，更多的运行时间开始涌现，这导致了容器运行时接口（Container Runtime Interface）（CRI）的开发。 从那时起，备用运行时集合越来越大：为了加强工作负载隔离，Kata Containers和gVisor等项目被发起，并且Kubernetes对Windows的支持正在稳步发展。 由于存在诸多针对不同用例的运行时，集群对混合运行时的需求变得明晰起来。但是，所有这些不同的容器运行方式都带来了一系列新问题要处理： 用户如何知道哪些运行时可用，并为其工作负载选择运行时？ 我们如何确保将Pod被调度到支持所需运行时的节点上？ 哪些运行时支持哪些功能，以及我们如何向用户显示不兼容性？ 我们如何考虑运行时的各种资源开销？ RuntimeClass旨在解决这些问题。 什么场景需要多个运行时？ 举个例子，有一个开放的云平台向外部用户提供容器服务，平台上运行有两种容器，一种是云平台管理用的容器（可信的），一种是用户部署的业务容器（不可信）。 在这种场景下，我们希望使用runc运行可信容器（弱隔离但性能好），用runv运行不可信容器（强隔离安全性好）。 值得注意的是：RuntimeClass是Pod级别的概念(即同一pod内容器的运行时必须相同) 为什么RuntimeClass是Pod级别的概念？ Kubernetes资源模型期望Pod中的容器之间可以共享某些资源。如果Pod由具有不同资源模型的不同容器组成，支持必要水平的资源共享变得非常具有挑战性。 例如，要跨VM边界支持本地回路（localhost）接口非常困难，但这是Pod中两个容器之间通信的通用模型。 如何使用RuntimeClass 在Kubernetes worker节点配置CRI shim 例如CRI-O运行时的配置，需要在文件/etc/crio/crio.conf定义runtime的handler_name [crio.runtime.runtimes.${HANDLER_NAME}] runtime_path = \"${PATH_TO_BINARY}\" 创建RuntimeClass资源对象； $ cat 在pod中指定RuntimeClass apiVersion: v1 kind: Pod metadata: name: mypod spec: runtimeClassName: myclass # ... docker运行时下创建启动pod沙箱的具体流程 上文中我们已经拥有了创建pod沙箱所需的配置，接下来我们针对docker运行时下，创建启动pod沙箱的流程做进一步分析 docker运行时下，pod沙箱实质是由一个包含该pod的网络名称空间的容器实现的。 创建启动流程如下： 拉取pod沙箱所需镜像（即pause容器镜像，默认k8s.gcr.io/pause:3.2） 根据配置调用运行时创建沙箱容器（即pause容器） 创建沙箱容器检查端点（pod下容器的端接口映射列表） 启动沙箱容器，如果启动失败将被回收。启动成功后重写docker生成的/etc/resolv.conf文件 设置沙箱容器网络: 如果pod共享主机网络命名空间，则跳过后续流程直接返回。此时创建启动pod沙箱流程结束。 源码解析 kubernetes\\pkg\\kubelet\\dockershim\\docker_sandbox.go func (ds *dockerService) RunPodSandbox(ctx context.Context, r *runtimeapi.RunPodSandboxRequest) (*runtimeapi.RunPodSandboxResponse, error) { config := r.GetConfig() // Step 1: Pull the image for the sandbox. image := defaultSandboxImage podSandboxImage := ds.podSandboxImage if len(podSandboxImage) != 0 { image = podSandboxImage } // NOTE: To use a custom sandbox image in a private repository, users need to configure the nodes with credentials properly. // see: http://kubernetes.io/docs/user-guide/images/#configuring-nodes-to-authenticate-to-a-private-repository // Only pull sandbox image when it's not present - v1.PullIfNotPresent. if err := ensureSandboxImageExists(ds.client, image); err != nil { return nil, err } // Step 2: Create the sandbox container. if r.GetRuntimeHandler() != \"\" && r.GetRuntimeHandler() != runtimeName { return nil, fmt.Errorf(\"RuntimeHandler %q not supported\", r.GetRuntimeHandler()) } createConfig, err := ds.makeSandboxDockerConfig(config, image) if err != nil { return nil, fmt.Errorf(\"failed to make sandbox docker config for pod %q: %v\", config.Metadata.Name, err) } createResp, err := ds.client.CreateContainer(*createConfig) if err != nil { createResp, err = recoverFromCreationConflictIfNeeded(ds.client, *createConfig, err) } if err != nil || createResp == nil { return nil, fmt.Errorf(\"failed to create a sandbox for pod %q: %v\", config.Metadata.Name, err) } resp := &runtimeapi.RunPodSandboxResponse{PodSandboxId: createResp.ID} ds.setNetworkReady(createResp.ID, false) defer func(e *error) { // Set networking ready depending on the error return of // the parent function if *e == nil { ds.setNetworkReady(createResp.ID, true) } }(&err) // Step 3: Create Sandbox Checkpoint. if err = ds.checkpointManager.CreateCheckpoint(createResp.ID, constructPodSandboxCheckpoint(config)); err != nil { return nil, err } // Step 4: Start the sandbox container. // Assume kubelet's garbage collector would remove the sandbox later, if // startContainer failed. err = ds.client.StartContainer(createResp.ID) if err != nil { return nil, fmt.Errorf(\"failed to start sandbox container for pod %q: %v\", config.Metadata.Name, err) } // Rewrite resolv.conf file generated by docker. // NOTE: cluster dns settings aren't passed anymore to docker api in all cases, // not only for pods with host network: the resolver conf will be overwritten // after sandbox creation to override docker's behaviour. This resolv.conf // file is shared by all containers of the same pod, and needs to be modified // only once per pod. if dnsConfig := config.GetDnsConfig(); dnsConfig != nil { containerInfo, err := ds.client.InspectContainer(createResp.ID) if err != nil { return nil, fmt.Errorf(\"failed to inspect sandbox container for pod %q: %v\", config.Metadata.Name, err) } if err := rewriteResolvFile(containerInfo.ResolvConfPath, dnsConfig.Servers, dnsConfig.Searches, dnsConfig.Options); err != nil { return nil, fmt.Errorf(\"rewrite resolv.conf failed for pod %q: %v\", config.Metadata.Name, err) } } // Do not invoke network plugins if in hostNetwork mode. if config.GetLinux().GetSecurityContext().GetNamespaceOptions().GetNetwork() == runtimeapi.NamespaceMode_NODE { return resp, nil } // Step 5: Setup networking for the sandbox. // All pod networking is setup by a CNI plugin discovered at startup time. // This plugin assigns the pod ip, sets up routes inside the sandbox, // creates interfaces etc. In theory, its jurisdiction ends with pod // sandbox networking, but it might insert iptables rules or open ports // on the host as well, to satisfy parts of the pod spec that aren't // recognized by the CNI standard yet. cID := kubecontainer.BuildContainerID(runtimeName, createResp.ID) networkOptions := make(map[string]string) if dnsConfig := config.GetDnsConfig(); dnsConfig != nil { // Build DNS options. dnsOption, err := json.Marshal(dnsConfig) if err != nil { return nil, fmt.Errorf(\"failed to marshal dns config for pod %q: %v\", config.Metadata.Name, err) } networkOptions[\"dns\"] = string(dnsOption) } err = ds.network.SetUpPod(config.GetMetadata().Namespace, config.GetMetadata().Name, cID, config.Annotations, networkOptions) if err != nil { errList := []error{fmt.Errorf(\"failed to set up sandbox container %q network for pod %q: %v\", createResp.ID, config.Metadata.Name, err)} // Ensure network resources are cleaned up even if the plugin // succeeded but an error happened between that success and here. err = ds.network.TearDownPod(config.GetMetadata().Namespace, config.GetMetadata().Name, cID) if err != nil { errList = append(errList, fmt.Errorf(\"failed to clean up sandbox container %q network for pod %q: %v\", createResp.ID, config.Metadata.Name, err)) } err = ds.client.StopContainer(createResp.ID, defaultSandboxGracePeriod) if err != nil { errList = append(errList, fmt.Errorf(\"failed to stop sandbox container %q for pod %q: %v\", createResp.ID, config.Metadata.Name, err)) } return resp, utilerrors.NewAggregate(errList) } return resp, nil } 设置沙箱容器网络流程解析: 调用CNI进行pod网络配置 kubernetes下容器网络由CNI管理，而非容器运行时。 CNI负责分配pod ip，在沙箱中设置路由，创建接口等。理论上，它的管辖范围仅限于pod沙箱网络，但它也可能在主机上插入iptables规则或开放端口，以满足CNI标准还不认可的pod规范的部分。 参考文献 Kubernetes v1.12: RuntimeClass 简介 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/":{"url":"2.容器/k8s/core/kubelet/启动/","title":"启动","keywords":"","body":"kubelet启动流程解析 基于kubernetes v1.18.6 内容概览: 本文介绍kubelet的常用启动方式、启动参数介绍、kubelet配置初始化流程 源码地址 kubelet 启动流程分析 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/启动模块/":{"url":"2.容器/k8s/core/kubelet/启动/启动模块/","title":"启动模块","keywords":"","body":"kubelet模块 1.PLEG模块 PLEG，即Pod Lifecycle Event Generator。 其维护着存储Pod信息的cache，从运行时获取容器的信息，并根据前后两次信息对比， 生成对应的PodLifecycleEvent，通过eventChannel发送到kubelet syncLoop进行消费，最终由kubelet syncPod完成Pod的同步，维护着用户的期望。 CAdvisor模块 集成在Kubelet中的容器监控工具，用于收集本节点和容器的监控信息。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/启动模块/启动不依赖容器运行时模块/01启动镜像管理器.html":{"url":"2.容器/k8s/core/kubelet/启动/启动模块/启动不依赖容器运行时模块/01启动镜像管理器.html","title":"01启动镜像管理器","keywords":"","body":"启动镜像管理器 基于kubernetes v1.18.6 概述 镜像管理器包含运行时相关接口，拥有对镜像、pod操作的能力。 主要功能如下： 协助gc管理器对镜像垃圾回收（提供镜像列表） 协助节点状态管理器记录当前节点镜像（imageCache） 镜像管理器数据结构 type realImageGCManager struct { // Container runtime runtime container.Runtime // Records of images and their use. imageRecords map[string]*imageRecord imageRecordsLock sync.Mutex // The image garbage collection policy in use. policy ImageGCPolicy // statsProvider provides stats used during image garbage collection. statsProvider StatsProvider // Recorder for Kubernetes events. recorder record.EventRecorder // Reference to this node. nodeRef *v1.ObjectReference // Track initialization initialized bool // imageCache is the cache of latest image list. imageCache imageCache // sandbox image exempted from GC sandboxImage string } 工作流程解析 镜像管理器是一个kubelet子模块，在kubelet启动流程内启动。 工作流程解析 镜像管理器启动时会fork两个goroutine进行周期性调用，主要维护以下两个对象： realImageGCManager.imageRecords: 记录镜像列表，以及镜像是否处于被使用状态。（五分钟更新一次，作为GC时清理镜像的依据） realImageGCManager.imageCache: 用于记录节点状态（镜像列表缓存，三十秒更新一次。避免频繁调用运行时获取） 显然这两个goroutine是可以合并的，源码确实也加了todo标识（// TODO(random-liu): Merge this with the previous loop.） imageRecords对象解析 imageRecords是一个map类型对象，key为镜像id，value为imageRecord对象。 imageRecord对象记录了镜像的状态，包含GC所需要的信息： firstDetected: 记录镜像被检测时间 lastUsed: 记录镜像上次被使用时间 size: 镜像大小 imageCache对象解析 imageCache是一个线程安全的对象，images是一个按镜像大小降序的镜像列表切片。 type imageCache struct { // sync.Mutex is the mutex protects the image cache. sync.Mutex // images is the image cache. images []container.Image } 源码实现 func (im *realImageGCManager) Start() { // 每5分钟执行： // go wait.Until(func() { // Initial detection make detected time \"unknown\" in the past. var ts time.Time if im.initialized { ts = time.Now() } // 仅为了测试是否可以监控到节点镜像 _, err := im.detectImages(ts) if err != nil { klog.Warningf(\"[imageGCManager] Failed to monitor images: %v\", err) } else { im.initialized = true } }, 5*time.Minute, wait.NeverStop) // Start a goroutine periodically updates image cache. // TODO(random-liu): Merge this with the previous loop. go wait.Until(func() { images, err := im.runtime.ListImages() if err != nil { klog.Warningf(\"[imageGCManager] Failed to update image list: %v\", err) } else { im.imageCache.set(images) } }, 30*time.Second, wait.NeverStop) } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/启动模块/启动不依赖容器运行时模块/02启动证书管理器.html":{"url":"2.容器/k8s/core/kubelet/启动/启动模块/启动不依赖容器运行时模块/02启动证书管理器.html","title":"02启动证书管理器","keywords":"","body":"启动证书管理器 概述 启动阶段如下图（红色部分） 证书管理器是做什么的？ 当证书到期时，通过向kube-apiserver请求新证书，自动轮转kubelet客户端证书 。 关联启动配置/标识 建议通过配置文件（一般为：/var/lib/kubelet/config.yaml）添加rotateCertificates: true项开启 kubelet证书目录（--cert-dir指定） $ pwd /var/lib/kubelet/pki $ ls kubelet-client-2021-08-23-16-32-25.pem kubelet-client-current.pem kubelet.crt kubelet.key 解析 流程解析 源码实现 kubernetes\\vendor\\k8s.io\\client-go\\util\\certificate\\certificate_manager.go // Start will start the background work of rotating the certificates. func (m *manager) Start() { // Certificate rotation depends on access to the API server certificate // signing API, so don't start the certificate manager if we don't have a // client. if m.clientFn == nil { klog.V(2).Infof(\"Certificate rotation is not enabled, no connection to the apiserver.\") return } klog.V(2).Infof(\"Certificate rotation is enabled.\") templateChanged := make(chan struct{}) go wait.Until(func() { deadline := m.nextRotationDeadline() if sleepInterval := deadline.Sub(m.now()); sleepInterval > 0 { klog.V(2).Infof(\"Waiting %v for next certificate rotation\", sleepInterval) timer := time.NewTimer(sleepInterval) defer timer.Stop() select { case Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/启动模块/启动不依赖容器运行时模块/03启动oomWather.html":{"url":"2.容器/k8s/core/kubelet/启动/启动模块/启动不依赖容器运行时模块/03启动oomWather.html","title":"03启动oomWather","keywords":"","body":"启动oomWatcher 概述 oomWatcher是一个kubelet子模块，在kubelet启动流程内启动。 通过与cAdvisor模块交互采集主机oom进程信息，启动阶段如下图（红色部分） oomWatcher是做什么的？ oomWatcher记录系统oom并记录到节点的event 解析 流程解析 oomWatcher模块在启动的时候会fork出两个goroutine，一个作为oom事件生产者，一个作为oom事件消费者。 两个goroutine采用channel进行数据传输。 // Start watches for system oom's and records an event for every system oom encountered. func (ow *realWatcher) Start(ref *v1.ObjectReference) error { outStream := make(chan *oomparser.OomInstance, 10) // cAdvisor模块 从/dev/kmsg读取数据解析出oom容器信息：宿主机pid、时间戳、容器名称、容器内进程名称 go ow.oomStreamer.StreamOoms(outStream) go func() { defer runtime.HandleCrash() for event := range outStream { if event.ContainerName == recordEventContainerName { klog.V(1).Infof(\"Got sys oom event: %v\", event) eventMsg := \"System OOM encountered\" if event.ProcessName != \"\" && event.Pid != 0 { eventMsg = fmt.Sprintf(\"%s, victim process: %s, pid: %d\", eventMsg, event.ProcessName, event.Pid) } ow.recorder.Eventf(ref, v1.EventTypeWarning, systemOOMEvent, eventMsg) } } klog.Errorf(\"Unexpectedly stopped receiving OOM notifications\") }() return nil } 数据源采集 cAdvisor模块从/dev/kmsg读取数据解析出oom容器信息：pid、oom时间戳、容器名称、容器内进程名称等。其中字段值如下： 优先级: 6 序列号: 4029 时间戳: 2021308979 信息: [ pid ] uid tgid total_vm rss pgtables_bytes swapents oom_score_adj name $ cat /dev/kmsg ... 6,4029,2021308979,-;[ pid ] uid tgid total_vm rss pgtables_bytes swapents oom_score_adj name ... 数据处理 数据过滤 过滤出oom信息，过滤规则：正则匹配包含invoked oom-killer:字符串的行。 随后利用正则匹配+字符串分割生成对应对象 4,3994,2021308860,-;java invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=999 数据解析 获取进程pid: 正则匹配+字符串分割 获取容器名称: 正则匹配+字符串分割 // StreamOoms writes to a provided a stream of OomInstance objects representing // OOM events that are found in the logs. // It will block and should be called from a goroutine. func (self *OomParser) StreamOoms(outStream chan 数据源消费 cAdvisor模块生产出的oom进程信息，最终被记录至节点的event，可通过以下方式获取相关信息。 $ kubectl describe node ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning SystemOOM 13m kubelet, node1 System OOM encountered, victim process: nginx, pid: 60867 Warning SystemOOM 13m kubelet, node1 System OOM encountered, victim process: nginx, pid: 61438 Warning SystemOOM 13m kubelet, node1 System OOM encountered, victim process: nginx, pid: 61746 Warning SystemOOM 12m kubelet, node1 System OOM encountered, victim process: runc:[2:INIT], pid: 71899 Warning SystemOOM 12m kubelet, node1 System OOM encountered, victim process: nginx, pid: 82162 Warning SystemOOM 11m kubelet, node1 System OOM encountered, victim process: nginx, pid: 96195 Warning SystemOOM 10m kubelet, node1 System OOM encountered, victim process: runc:[2:INIT], pid: 12707 Warning SystemOOM 7m33s kubelet, node1 System OOM encountered, victim process: nginx, pid: 87657 Warning SystemOOM 2m32s kubelet, node1 System OOM encountered, victim process: runc:[2:INIT], pid: 98756 ... Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/启动模块/启动不依赖容器运行时模块/TODO_04启动资源分析器.html":{"url":"2.容器/k8s/core/kubelet/启动/启动模块/启动不依赖容器运行时模块/TODO_04启动资源分析器.html","title":"TODO_04启动资源分析器","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/启动模块/启动不依赖容器运行时模块/TODO_README.html":{"url":"2.容器/k8s/core/kubelet/启动/启动模块/启动不依赖容器运行时模块/TODO_README.html","title":"TODO_README","keywords":"","body":"启动不依赖运行时模块 启动内容及阶段如下图： 除初始化必要目录外，主要启动以下四个模块: 镜像GC管理器: 管理镜像垃圾回收 证书管理器: 管理证书轮换 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/启动模块/TODO_PLEG模块.html":{"url":"2.容器/k8s/core/kubelet/启动/启动模块/TODO_PLEG模块.html","title":"TODO_PLEG模块","keywords":"","body":"PLEG模块解析 基于kubernetes v1.18.6 PLEG(Pod Lifecycle Event Generator) 通过CRI接口轮询容器状态，然后与内存中的容器状态做比对，并发送相应事件。 PLEG是kubelet的核心模块,PLEG会周期性调用container runtime获取本节点containers/sandboxes的信息， 并与自身维护的pods cache信息进行对比，生成对应的PodLifecycleEvent， 然后输出到eventChannel中，通过eventChannel发送到kubelet syncLoop进行消费， 然后由kubelet syncPod来触发pod同步处理过程，最终达到用户的期望状态。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/启动模块/TODO_pod状态管理器.html":{"url":"2.容器/k8s/core/kubelet/启动/启动模块/TODO_pod状态管理器.html","title":"TODO_pod状态管理器","keywords":"","body":"启动pod状态管理器 基于kubernetes v1.18.6 概述 pod管理器主要用来将本地pod状态信息同步到apiserver，statusManage并不会主动监控pod的状态，而是提供接口供其他manager进行调用。 对于pod状态的变更会推入名为podStatusChannel的通道，statusManage在启动的时候会开辟一个goroutine，用于循环处理podStatusChannel内pod状态变更对象。 statusManage对podStatusChannel内对象的处理分为两种方式： 方式一: sync()。按顺序逐一处理podStatusChannel内对象，先进先出 方式二: batch()。每10s批量处理 两种方式同步进行 状态管理器数据结构 kubeClient: 用于和apiserver交互，查询/更新pod状态 podManager: 用于管理本地pod缓存信息（避免频繁与apiserver交互） podStatuses: 缓存本地pod状态，map类型，key为pod的uid，value为pod的status字段内容。 podStatusesLock: 线程锁 podStatusChannel: 存放pod状态变更事件channel，该通道为缓冲通道，缓冲1000个podStatusSyncRequest对象。 apiStatusVersions: 维护最新的pod status版本号，每更新一次会加1 podDeletionSafety: 安全删除pod的接口 type manager struct { kubeClient clientset.Interface podManager kubepod.Manager // Map from pod UID to sync status of the corresponding pod. podStatuses map[types.UID]versionedPodStatus podStatusesLock sync.RWMutex podStatusChannel chan podStatusSyncRequest // Map from (mirror) pod UID to latest status version successfully sent to the API server. // apiStatusVersions must only be accessed from the sync thread. apiStatusVersions map[kubetypes.MirrorPodUID]uint64 podDeletionSafety PodDeletionSafetyProvider } 状态管理器初始化阶段 在初始化kubelet实例的时候初始化 func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, crOptions *config.ContainerRuntimeOptions, containerRuntime string, hostnameOverride string, nodeIP string, providerID string, cloudProvider string, certDirectory string, rootDirectory string, registerNode bool, registerWithTaints []api.Taint, allowedUnsafeSysctls []string, experimentalMounterPath string, experimentalKernelMemcgNotification bool, experimentalCheckNodeCapabilitiesBeforeMount bool, experimentalNodeAllocatableIgnoreEvictionThreshold bool, minimumGCAge metav1.Duration, maxPerPodContainerCount int32, maxContainerCount int32, masterServiceNamespace string, registerSchedulable bool, keepTerminatedPodVolumes bool, nodeLabels map[string]string, seccompProfileRoot string, bootstrapCheckpointPath string, nodeStatusMaxImages int32) (*Kubelet, error) { ... klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet) ... } sync()处理流程解析 当一个pod状态变更被推入podStatusChannel后，且未到达定时器设置时间（10s间隔），将交由sync()函数处理。 首先我们了解下podStatusChannel存放的对象数据结构： podUID: pod的uid status: pod状态变更对象 type podStatusSyncRequest struct { podUID types.UID status versionedPodStatus } status: pod的status字段(可通过kubectl get pod -n -o yaml查看) version: pod状态变更版本计数，每变更一次对应加一。 podName: pod名称 podNamespace: pod所属命名空间 type versionedPodStatus struct { status v1.PodStatus // Monotonically increasing version number (per pod). version uint64 // Pod name & namespace, for sending updates to API server. podName string podNamespace string } 接下来我们来分析下处理流程: 判断pod是否需要更新。判断方式如下（顺序执行判断逻辑）: 根据pod uid从apiStatusVersions获取pod实例，若获取失败（如：第一次创建时并没有存储pod状态信息）则需要更新 根据pod uid从apiStatusVersions获取pod实例，若获取的pod状态版本（statusManager.apiStatusVersions[]）小于podStatusSyncRequest对象的status.version版本，则需要更新 根据pod uid从podManager获取pod实例，若获取失败（如已经被删除）则不需要更新 上述情况均不满足会调用canBeDeleted()函数。canBeDeleted()函数判断如下： pod不存在DeletionTimestamp字段或pod类型为镜像类型pod，不需要更新 上述情况均不满足，说明pod处于删除状态。调用PodResourcesAreReclaimed()函数，判断是否可以安全删除，返回PodResourcesAreReclaimed()函数返回值 PodResourcesAreReclaimed()函数判断以下状态的pod是否已被安全删除（pod可能处于删除中状态，但未删除完毕）: pod处于terminated状态，但仍有container处于running状态，返回false pod处于terminated状态，但无法从podCache缓存对象中获取运行时信息，返回false pod处于terminated状态，但仍有container未被清理完毕，返回false pod处于terminated状态，但仍有卷未被清理完毕，返回false pod处于terminated状态，但pod cgroup沙盒未被清理完毕，返回false 从apiserver获取pod实例（入参命名空间、pod名称），若获取不到（可能已被删除），说明不需要同步Pod状态，跳出对当前pod处理流程。 对比podStatusSyncRequest.podUID与从apiserver查询到的pod uid是否相同，如不相同说明pod可能被删除重建，则不需要同步Pod状态，跳出对当前pod处理流程。 调用apiserver同步pod最新的status。同步之前比对oldPodStatus与newPodStatus差异，若存在差异调用api-server对pod状态进行更新，并将返回的pod作为newPod，如不存在差异将不会调用api-server进行更新。其中 oldPodStatus: 根据pod归属命名空间、pod名称从apiserver查询到的pod实例的status值 newPodStatus: 从podStatusChannel通道传递来的需要更新状态的pod实例的status值。 调用canBeDeleted()函数（删除pod事件触发的修改pod状态会走该逻辑）。canBeDeleted()函数判断如下： newPod不存在DeletionTimestamp字段或newPod类型为镜像类型pod，返回false 上述情况均不满足，说明newPod处于删除状态。调用PodResourcesAreReclaimed()函数，判断是否可以安全删除，返回PodResourcesAreReclaimed()函数返回值 PodResourcesAreReclaimed()函数判断以下状态的newPod是否已被安全删除（pod可能处于删除中状态，但未删除完毕）: newPod处于terminated状态，但仍有container处于running状态，返回false newPod处于terminated状态，但无法从podCache缓存对象中获取运行时信息，返回false newPod处于terminated状态，但仍有container未被清理完毕，返回false newPod处于terminated状态，但仍有卷未被清理完毕，返回false newPod处于terminated状态，但pod cgroup沙盒未被清理完毕，返回false 当newPod可以被安全删除，调用apiserver对newPod执行删除操作，删除成功后将newPod从statusManager.podStatuses（该对象缓存pod状态信息）中删除 核心源码 func (m *manager) syncPod(uid types.UID, status versionedPodStatus) { if !m.needsUpdate(uid, status) { klog.V(1).Infof(\"Status for pod %q is up-to-date; skipping\", uid) return } // TODO: make me easier to express from client code pod, err := m.kubeClient.CoreV1().Pods(status.podNamespace).Get(context.TODO(), status.podName, metav1.GetOptions{}) if errors.IsNotFound(err) { klog.V(3).Infof(\"Pod %q does not exist on the server\", format.PodDesc(status.podName, status.podNamespace, uid)) // If the Pod is deleted the status will be cleared in // RemoveOrphanedStatuses, so we just ignore the update here. return } if err != nil { klog.Warningf(\"Failed to get status for pod %q: %v\", format.PodDesc(status.podName, status.podNamespace, uid), err) return } // 获取pod真实uid（针对static类型pod的uid需要做转换） translatedUID := m.podManager.TranslatePodUID(pod.UID) // Type convert original uid just for the purpose of comparison. if len(translatedUID) > 0 && translatedUID != kubetypes.ResolvedPodUID(uid) { klog.V(2).Infof(\"Pod %q was deleted and then recreated, skipping status update; old UID %q, new UID %q\", format.Pod(pod), uid, translatedUID) m.deletePodStatus(uid) return } oldStatus := pod.Status.DeepCopy() newPod, patchBytes, unchanged, err := statusutil.PatchPodStatus(m.kubeClient, pod.Namespace, pod.Name, pod.UID, *oldStatus, mergePodStatus(*oldStatus, status.status)) klog.V(3).Infof(\"Patch status for pod %q with %q\", format.Pod(pod), patchBytes) if err != nil { klog.Warningf(\"Failed to update status for pod %q: %v\", format.Pod(pod), err) return } if unchanged { klog.V(3).Infof(\"Status for pod %q is up-to-date: (%d)\", format.Pod(pod), status.version) } else { klog.V(3).Infof(\"Status for pod %q updated successfully: (%d, %+v)\", format.Pod(pod), status.version, status.status) pod = newPod } m.apiStatusVersions[kubetypes.MirrorPodUID(pod.UID)] = status.version // We don't handle graceful deletion of mirror pods. if m.canBeDeleted(pod, status.status) { deleteOptions := metav1.DeleteOptions{ GracePeriodSeconds: new(int64), // Use the pod UID as the precondition for deletion to prevent deleting a // newly created pod with the same name and namespace. Preconditions: metav1.NewUIDPreconditions(string(pod.UID)), } err = m.kubeClient.CoreV1().Pods(pod.Namespace).Delete(context.TODO(), pod.Name, deleteOptions) if err != nil { klog.Warningf(\"Failed to delete status for pod %q: %v\", format.Pod(pod), err) return } klog.V(3).Infof(\"Pod %q fully terminated and removed from etcd\", format.Pod(pod)) m.deletePodStatus(uid) } } syncBatch()处理流程解析 syncBatch()主要是将statusManager.podStatuses中的数据与statusManager.apiStatusVersions和statusManager.podManager中的数据进行对比是否一致，若不一致则以statusManager.podStatuses中的数据为准同步至apiserver。 statusManager.podStatuses statusManager.podManager statusManager.apiStatusVersions: 维护最新的pod status版本号，map类型集合，key为pod uid，value为pod status Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/启动模块/TODO_确认防火墙规则.html":{"url":"2.容器/k8s/core/kubelet/启动/启动模块/TODO_确认防火墙规则.html","title":"TODO_确认防火墙规则","keywords":"","body":"iptables 基础概念 本文讨论内容需具备一定iptables基础： iptables详解（1）：iptables概念 iptables详解（12）：iptables动作总结之一 iptables详解（13）：iptables动作总结之二 常用指令 查询链是否存在 $ iptables -t nat -L 模块解析 该模块需指定--make-iptables-util-chains=true开启，默认开启。 func (kl *Kubelet) Run(updates 内部逻辑非常简单：初始化kubernetes定义的iptables链、规则。 并周期性（每分钟）对这些iptables链、规则检查变更， func (kl *Kubelet) initNetworkUtil() { kl.syncNetworkUtil() go kl.iptClient.Monitor(utiliptables.Chain(\"KUBE-KUBELET-CANARY\"), []utiliptables.Table{utiliptables.TableMangle, utiliptables.TableNAT, utiliptables.TableFilter}, kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop) } iptables配置初始化 确认防火墙链是否存在 确认链是否存在，确认方式为执行创建命令: $ iptables -N -t 创建成功返回true，若链已存在也返回true func (runner *runner) EnsureChain(table Table, chain Chain) (bool, error) { fullArgs := makeFullArgs(table, chain) runner.mu.Lock() defer runner.mu.Unlock() /* $ iptables -t nat -N KUBE-KUBELET-CANARY iptables: Chain already exists. */ out, err := runner.run(opCreateChain, fullArgs) if err != nil { if ee, ok := err.(utilexec.ExitError); ok { if ee.Exited() && ee.ExitStatus() == 1 { return true, nil } } return false, fmt.Errorf(\"error creating chain %q: %v: %s\", chain, err, out) } return false, nil } 创建的链如下： nat/KUBE-MARK-DROP: 对于未能匹配到跳转规则的traffic set mark 0x8000，有此标记的数据包会在filter表drop掉 nat/KUBE-MARK-MASQ: 对于符合条件的包set mark 0x4000, 有此标记的数据包会在KUBE-POSTROUTING chain中统一做MASQUERADE（动态SNAT） nat/KUBE-POSTROUTING: 该链对打上了0x4000标记的报文进行SNAT转换 filter/KUBE-FIREWALL: 该链对所有标记了0x8000的报文进行丢弃 规则的创建 确认防火墙规则，检测未存在进行创建： $ iptables -C 具体路由方式由kube-proxy管理，这里不作过深讨论。 参考文献 浅谈 kubernetes service 那些事 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/节点状态/上报节点状态.html":{"url":"2.容器/k8s/core/kubelet/启动/节点状态/上报节点状态.html","title":"上报节点状态","keywords":"","body":"kubelet上报节点状态机制 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/01启动方式.html":{"url":"2.容器/k8s/core/kubelet/启动/01启动方式.html","title":"01启动方式","keywords":"","body":"kubelet启动方式分析 我们一般以systemd系统守护进程的方式启动kubelet $ systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Sat 2021-11-06 16:05:42 CST; 4 days ago Docs: http://kubernetes.io/docs/ Process: 2153 ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service (code=exited, status=0/SUCCESS) Process: 2150 ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/systemd/system.slice/kubelet.service (code=exited, status=0/SUCCESS) Process: 2143 ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/memory/system.slice/kubelet.service (code=exited, status=0/SUCCESS) Process: 2111 ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service (code=exited, status=0/SUCCESS) Process: 2105 ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpuacct/system.slice/kubelet.service (code=exited, status=0/SUCCESS) Process: 2061 ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpu/system.slice/kubelet.service (code=exited, status=0/SUCCESS) Main PID: 2168 (kubelet) Tasks: 230 Memory: 867.4M CGroup: /system.slice/kubelet.service └─2168 /usr/local/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.co... Nov 11 10:20:57 node1 kubelet[2168]: W1111 10:20:57.208900 2168 volume_linux.go:49] Setting volume ownership for /var/lib/kubelet/pods/5a15... Nov 11 10:20:57 node1 kubelet[2168]: W1111 10:20:57.210533 2168 volume_linux.go:49] Setting volume ownership for /var/lib/kubelet/pods/5a15... Nov 11 10:20:57 node1 kubelet[2168]: W1111 10:20:57.210833 2168 volume_linux.go:49] Setting volume ownership for /var/lib/kubelet/pods/5a15... Nov 11 10:20:57 node1 kubelet[2168]: E1111 10:20:57.767389 2168 summary_sys_containers.go:47] Failed to get system container stats for \"/sy... Nov 11 10:20:58 node1 kubelet[2168]: I1111 10:20:58.064091 2168 topology_manager.go:219] [topologymanager] RemoveContainer - Contai...8291c7ef Nov 11 10:20:58 node1 kubelet[2168]: I1111 10:20:58.064592 2168 topology_manager.go:219] [topologymanager] RemoveContainer - Contai...78e11ac7 Nov 11 10:20:58 node1 kubelet[2168]: E1111 10:20:58.074045 2168 pod_workers.go:191] Error syncing pod 2f3115c0-22d2-4094-a467-f543cd34da3f ... Nov 11 10:20:58 node1 kubelet[2168]: E1111 10:20:58.082305 2168 pod_workers.go:191] Error syncing pod ea9d254d-3d05-4b69-b1ff-c986454078d8 ... Nov 11 10:20:58 node1 kubelet[2168]: E1111 10:20:58.085689 2168 kuberuntime_manager.go:801] container start failed: CreateContainer...ot found Nov 11 10:20:58 node1 kubelet[2168]: E1111 10:20:58.085713 2168 pod_workers.go:191] Error syncing pod ececebcf-daeb-45e1-8516-06a95d706293 ... Hint: Some lines were ellipsized, use -l to show in full. 我们看下service配置 $ cat /etc/systemd/system/kubelet.service [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=http://kubernetes.io/docs/ [Service] ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpu/system.slice/kubelet.service ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpuacct/system.slice/kubelet.service ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/memory/system.slice/kubelet.service ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/systemd/system.slice/kubelet.service ExecStartPre=/usr/bin/mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service ExecStart=/usr/local/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target 其中启动部分执行了/usr/local/bin/kubelet二进制文件，启动的参数我们暂且不关注。 /usr/local/bin/kubelet二进制文件是由kubernetes源码编译而来，接下来让我们从kubelet源码角度分析它的启动流程 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/02初始化kubelet指令.html":{"url":"2.容器/k8s/core/kubelet/启动/02初始化kubelet指令.html","title":"02初始化kubelet指令","keywords":"","body":"主函数入口 启动主函数位于: kubernetes/cmd/kubelet/kubelet.go package main import ( \"math/rand\" \"os\" \"time\" \"k8s.io/component-base/logs\" _ \"k8s.io/component-base/metrics/prometheus/restclient\" _ \"k8s.io/component-base/metrics/prometheus/version\" // for version metric registration \"k8s.io/kubernetes/cmd/kubelet/app\" ) func main() { rand.Seed(time.Now().UnixNano()) // 初始化kubelet指令 command := app.NewKubeletCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { os.Exit(1) } } rand.Seed(time.Now().UnixNano()): 定义了全局随机数种子 command := app.NewKubeletCommand(): 根据启动参数初始化了kubelet指令 logs.InitLogs(): 初始化日志控制器 err := command.Execute(): 执行启动流程 接下来我们来分析kubelet初始化流程 kubelet指令初始化解析 源码位置: kubernetes/cmd/kubelet/app/server.go // NewKubeletCommand creates a *cobra.Command object with default parameters func NewKubeletCommand() *cobra.Command { cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) cleanFlagSet.SetNormalizeFunc(cliflag.WordSepNormalizeFunc) kubeletFlags := options.NewKubeletFlags() kubeletConfig, err := options.NewKubeletConfiguration() // programmer error if err != nil { klog.Fatal(err) } cmd := &cobra.Command{ Use: componentKubelet, Long: `The kubelet is the primary \"node agent\" that runs on each node. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider. The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object that describes a pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms (primarily through the apiserver) and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes. Other than from an PodSpec from the apiserver, there are three ways that a container manifest can be provided to the Kubelet. File: Path passed as a flag on the command line. Files under this path will be monitored periodically for updates. The monitoring period is 20s by default and is configurable via a flag. HTTP endpoint: HTTP endpoint passed as a parameter on the command line. This endpoint is checked every 20 seconds (also configurable with a flag). HTTP server: The kubelet can also listen for HTTP and respond to a simple API (underspec'd currently) to submit a new manifest.`, // The Kubelet has special flag parsing requirements to enforce flag precedence rules, // so we do all our parsing manually in Run, below. // DisableFlagParsing=true provides the full set of flags passed to the kubelet in the // `args` arg to Run, without Cobra's interference. DisableFlagParsing: true, Run: func(cmd *cobra.Command, args []string) { // initial flag parse, since we disable cobra's flag parsing if err := cleanFlagSet.Parse(args); err != nil { cmd.Usage() klog.Fatal(err) } // check if there are non-flag arguments in the command line cmds := cleanFlagSet.Args() if len(cmds) > 0 { cmd.Usage() klog.Fatalf(\"unknown command: %s\", cmds[0]) } // short-circuit on help help, err := cleanFlagSet.GetBool(\"help\") if err != nil { klog.Fatal(`\"help\" flag is non-bool, programmer error, please correct`) } if help { cmd.Help() return } // short-circuit on verflag verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cleanFlagSet) // set feature gates from initial flags-based config if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { klog.Fatal(err) } // validate the initial KubeletFlags if err := options.ValidateKubeletFlags(kubeletFlags); err != nil { klog.Fatal(err) } if kubeletFlags.ContainerRuntime == \"remote\" && cleanFlagSet.Changed(\"pod-infra-container-image\") { klog.Warning(\"Warning: For remote container runtime, --pod-infra-container-image is ignored in kubelet, which should be set in that remote runtime instead\") } // load kubelet config file, if provided if configFile := kubeletFlags.KubeletConfigFile; len(configFile) > 0 { kubeletConfig, err = loadConfigFile(configFile) if err != nil { klog.Fatal(err) } // We must enforce flag precedence by re-parsing the command line into the new object. // This is necessary to preserve backwards-compatibility across binary upgrades. // See issue #56171 for more details. if err := kubeletConfigFlagPrecedence(kubeletConfig, args); err != nil { klog.Fatal(err) } // update feature gates based on new config if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { klog.Fatal(err) } } // We always validate the local configuration (command line + config file). // This is the default \"last-known-good\" config for dynamic config, and must always remain valid. if err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil { klog.Fatal(err) } // use dynamic kubelet config, if enabled var kubeletConfigController *dynamickubeletconfig.Controller if dynamicConfigDir := kubeletFlags.DynamicConfigDir.Value(); len(dynamicConfigDir) > 0 { var dynamicKubeletConfig *kubeletconfiginternal.KubeletConfiguration dynamicKubeletConfig, kubeletConfigController, err = BootstrapKubeletConfigController(dynamicConfigDir, func(kc *kubeletconfiginternal.KubeletConfiguration) error { // Here, we enforce flag precedence inside the controller, prior to the controller's validation sequence, // so that we get a complete validation at the same point where we can decide to reject dynamic config. // This fixes the flag-precedence component of issue #63305. // See issue #56171 for general details on flag precedence. return kubeletConfigFlagPrecedence(kc, args) }) if err != nil { klog.Fatal(err) } // If we should just use our existing, local config, the controller will return a nil config if dynamicKubeletConfig != nil { kubeletConfig = dynamicKubeletConfig // Note: flag precedence was already enforced in the controller, prior to validation, // by our above transform function. Now we simply update feature gates from the new config. if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { klog.Fatal(err) } } } // construct a KubeletServer from kubeletFlags and kubeletConfig kubeletServer := &options.KubeletServer{ KubeletFlags: *kubeletFlags, KubeletConfiguration: *kubeletConfig, } // use kubeletServer to construct the default KubeletDeps kubeletDeps, err := UnsecuredDependencies(kubeletServer, utilfeature.DefaultFeatureGate) if err != nil { klog.Fatal(err) } // add the kubelet config controller to kubeletDeps kubeletDeps.KubeletConfigController = kubeletConfigController // set up stopCh here in order to be reused by kubelet and docker shim stopCh := genericapiserver.SetupSignalHandler() // start the experimental docker shim, if enabled if kubeletServer.KubeletFlags.ExperimentalDockershim { if err := RunDockershim(&kubeletServer.KubeletFlags, kubeletConfig, stopCh); err != nil { klog.Fatal(err) } return } // run the kubelet klog.V(5).Infof(\"KubeletConfiguration: %#v\", kubeletServer.KubeletConfiguration) if err := Run(kubeletServer, kubeletDeps, utilfeature.DefaultFeatureGate, stopCh); err != nil { klog.Fatal(err) } }, } // keep cleanFlagSet separate, so Cobra doesn't pollute it with the global flags kubeletFlags.AddFlags(cleanFlagSet) options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig) options.AddGlobalFlags(cleanFlagSet) cleanFlagSet.BoolP(\"help\", \"h\", false, fmt.Sprintf(\"help for %s\", cmd.Name())) // ugly, but necessary, because Cobra's default UsageFunc and HelpFunc pollute the flagset with global flags const usageFmt = \"Usage:\\n %s\\n\\nFlags:\\n%s\" cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) }) return cmd } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/03标识初始化.html":{"url":"2.容器/k8s/core/kubelet/启动/03标识初始化.html","title":"03标识初始化","keywords":"","body":"kubelet相关标识解析 kubelet对于标识定义这块的代码读起来有点乱，定义了两个标识集合: func NewKubeletCommand() *cobra.Command { cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) cleanFlagSet.SetNormalizeFunc(cliflag.WordSepNormalizeFunc) kubeletFlags := options.NewKubeletFlags() ... } cleanFlagSet: 全部标识(--help、--version等全局标识 + kubelet标识) kubeletFlags: kubelet标识 针对这一设计官方有提到 // keep cleanFlagSet separate, so Cobra doesn't pollute it with the global flags kubeletFlags.AddFlags(cleanFlagSet) options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig) options.AddGlobalFlags(cleanFlagSet) cleanFlagSet.BoolP(\"help\", \"h\", false, fmt.Sprintf(\"help for %s\", cmd.Name())) 所有与标识相关的内容如下: // NewKubeletCommand creates a *cobra.Command object with default parameters func NewKubeletCommand() *cobra.Command { cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) cleanFlagSet.SetNormalizeFunc(cliflag.WordSepNormalizeFunc) kubeletFlags := options.NewKubeletFlags() kubeletConfig, err := options.NewKubeletConfiguration() // programmer error if err != nil { klog.Fatal(err) } ... // keep cleanFlagSet separate, so Cobra doesn't pollute it with the global flags kubeletFlags.AddFlags(cleanFlagSet) options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig) options.AddGlobalFlags(cleanFlagSet) cleanFlagSet.BoolP(\"help\", \"h\", false, fmt.Sprintf(\"help for %s\", cmd.Name())) ... return cmd } 接下来我们逐步分析，标识初始化流程 1.cleanFlagSet初始化 初始化kubelet指令集时会先初始化一部分标识（这里全局标识集合为cleanFlagSet），并赋予默认值。 cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) cleanFlagSet.SetNormalizeFunc(cliflag.WordSepNormalizeFunc) kubeletFlags := options.NewKubeletFlags() kubeletConfig, err := options.NewKubeletConfiguration() // programmer error if err != nil { klog.Fatal(err) } 其中以下部分定义了一个名为cleanFlagSet标识集合，并具有自动将标识中_转换为-能力 cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) cleanFlagSet.SetNormalizeFunc(cliflag.WordSepNormalizeFunc) kubeletFlags := options.NewKubeletFlags()则初始化了kubelet标识集合并赋予默认值 2.添加cleanFlagSet为全局标识 上文我们分析了，kubelet指令在初始化阶段，初始化了一个全局标识对象cleanFlagSet。 它是如何被设置为全局标识的呢？其实是通过以下代码实现： // NewKubeletCommand creates a *cobra.Command object with default parameters func NewKubeletCommand() *cobra.Command { // keep cleanFlagSet separate, so Cobra doesn't pollute it with the global flags ... options.AddGlobalFlags(cleanFlagSet) ... return cmd } 3.kubeletFlags初始化 上文我们了解到，为了更好的管理标识，kubelet定义了两个标识集合: cleanFlagSet: 全局标识(配置文件路径、容器运行时、证书路径) kubeletFlags: kubelet标识（kubelet相关标识，包含cleanFlagSet内的标识） 接下来我们分析下kubelet标识集合的初始化流程: 首先通过调用options.NewKubeletFlags()初始化kubeletFlags对象 func NewKubeletCommand() *cobra.Command { ... kubeletFlags := options.NewKubeletFlags() ... } kubernetes/cmd/kubelet/app/options/options.go func NewKubeletFlags() *KubeletFlags { remoteRuntimeEndpoint := \"\" if runtime.GOOS == \"linux\" { remoteRuntimeEndpoint = \"unix:///var/run/dockershim.sock\" } else if runtime.GOOS == \"windows\" { remoteRuntimeEndpoint = \"npipe:////./pipe/dockershim\" } return &KubeletFlags{ EnableServer: true, ContainerRuntimeOptions: *NewContainerRuntimeOptions(), CertDirectory: \"/var/lib/kubelet/pki\", RootDirectory: defaultRootDir, MasterServiceNamespace: metav1.NamespaceDefault, MaxContainerCount: -1, MaxPerPodContainerCount: 1, MinimumGCAge: metav1.Duration{Duration: 0}, NonMasqueradeCIDR: \"10.0.0.0/8\", RegisterSchedulable: true, ExperimentalKernelMemcgNotification: false, RemoteRuntimeEndpoint: remoteRuntimeEndpoint, NodeLabels: make(map[string]string), VolumePluginDir: \"/usr/libexec/kubernetes/kubelet-plugins/volume/exec/\", RegisterNode: true, SeccompProfileRoot: filepath.Join(defaultRootDir, \"seccomp\"), // prior to the introduction of this flag, there was a hardcoded cap of 50 images NodeStatusMaxImages: 50, EnableCAdvisorJSONEndpoints: false, } } 这里我们创建了一个存放全局标识的对象,即cleanFlagSet，针对其初始化的默认值我们做下简要分析： EnableServer: 开启kubelet服务端 ContainerRuntimeOptions: 定义了容器运行时选项 func NewContainerRuntimeOptions() *config.ContainerRuntimeOptions { dockerEndpoint := \"\" if runtime.GOOS != \"windows\" { dockerEndpoint = \"unix:///var/run/docker.sock\" } return &config.ContainerRuntimeOptions{ ContainerRuntime: kubetypes.DockerContainerRuntime, RedirectContainerStreaming: false, DockerEndpoint: dockerEndpoint, DockershimRootDirectory: \"/var/lib/dockershim\", PodSandboxImage: defaultPodSandboxImage, ImagePullProgressDeadline: metav1.Duration{Duration: 1 * time.Minute}, ExperimentalDockershim: false, //Alpha feature CNIBinDir: \"/opt/cni/bin\", CNIConfDir: \"/etc/cni/net.d\", CNICacheDir: \"/var/lib/cni/cache\", } } &config.ContainerRuntimeOptions会初始化一部分字段，并非所有。 ContainerRuntime: 默认使用docker容器运行时（--container-runtime） RedirectContainerStreaming: 重定向容器流，默认false（--container-runtime） 当为true时: kubelet将返回一个HTTP重定向到apiserver，apiserver将直接访问容器运行时。 虽然这样的话性能会有所提升，但安全方面又有了隐患（apiserver与容器运行时间无身份认证） 当为false时: kubelet将会代理apiserver与容器运行时间的流数据，虽然性能上有些损耗，但也提供了安全保障。 DockerEndpoint: 定义了docker socket路径(unix:///var/run/docker.sock)用于与docker间通信 DockershimRootDirectory: dockershim根目录，默认为/var/lib/dockershim，用于集成测试（例如: OpenShift） PodSandboxImage: pod沙箱镜像，默认k8s.gcr.io/pause:3.2 ImagePullProgressDeadline: 镜像拉取超时时间，默认1分钟，超过1分钟如果镜像未拉取成功将取消进行镜像拉取。 ExperimentalDockershim: 是否开启dockershim only模式，默认false CNIBinDir: CNI二进制目录 CNIConfDir: CNI配置文件目录 CNICacheDir: CNI缓存目录 CertDirectory: 存放TLS证书的目录，默认为/var/lib/kubelet/pki。如果指定了tlsCertFile与tlsPrivateKeyFile，该标识会被忽略。 RootDirectory: 存放kubelet文件 (卷挂载，配置等)的目录，默认/var/lib/kubelet MasterServiceNamespace: 已移除标识。注入到pod中的kubernetes master服务的命名空间，默认default MaxContainerCount: 已移除标识。当前节点最大容器数量，默认无限制。 MaxPerPodContainerCount: 已移除标识。每一个容器最多在系统中保存的最大已经停止的实例数量，默认为1 MinimumGCAge: 已移除该标识。 NonMasqueradeCIDR: 已移除该标识。 RegisterSchedulable: 已移除该标识。 ExperimentalKernelMemcgNotification: 默认false。如果启用，kubelet将与内核memcg通知集成，以确定是否越过内存回收阈值，而不是轮询。 RemoteRuntimeEndpoint: 运行时服务端点 unix系统: unix:///var/run/dockershim.sock windows系统: npipe:////./pipe/dockershim NodeLabels: 注册节点至集群时，提供的节点标签集合(map[string]string) VolumePluginDir: 存放第三方卷插件的目录，默认/usr/libexec/kubernetes/kubelet-plugins/volume/exec/ RegisterNode: 启用自动注册apiserver，默认true SeccompProfileRoot: 存放seccomp配置文件的目录，默认为/var/lib/kubelet/seccomp NodeStatusMaxImages: 限制了node.status.images中上报的映像数量，这是一个实验性的短期标志，用于帮助实现节点可伸缩性。默认50 EnableCAdvisorJSONEndpoints: 启用一些将在未来版本中删除的cAdvisor端点，默认false 4.kubelet配置项初始化 该阶段初始化kubelet配置对象，用以绑定解析启动标识 kubeletConfig, err := options.NewKubeletConfiguration() 5.标识绑定 我们知道，程序想到使用命令行传入进来的参数时，需要定义一个变量去接收解析，以便后续使用。 如果标识比较多的情况，定义一个结构体来存放变量是个很好的选择。接下来我们分析下kubelet是如何将标识与结构体绑定的 func NewKubeletCommand() *cobra.Command { ... // keep cleanFlagSet separate, so Cobra doesn't pollute it with the global flags kubeletFlags.AddFlags(cleanFlagSet) options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig) options.AddGlobalFlags(cleanFlagSet) cleanFlagSet.BoolP(\"help\", \"h\", false, fmt.Sprintf(\"help for %s\", cmd.Name())) ... return cmd 我们先分析下kubeletFlags.AddFlags(cleanFlagSet)调用: func (f *KubeletFlags) AddFlags(mainfs *pflag.FlagSet) { fs := pflag.NewFlagSet(\"\", pflag.ExitOnError) defer func() { // Unhide deprecated flags. We want deprecated flags to show in Kubelet help. // We have some hidden flags, but we might as well unhide these when they are deprecated, // as silently deprecating and removing (even hidden) things is unkind to people who use them. fs.VisitAll(func(f *pflag.Flag) { if len(f.Deprecated) > 0 { f.Hidden = false } }) mainfs.AddFlagSet(fs) }() f.ContainerRuntimeOptions.AddFlags(fs) f.addOSFlags(fs) fs.StringVar(&f.KubeletConfigFile, \"config\", f.KubeletConfigFile, \"The Kubelet will load its initial configuration from this file. The path may be absolute or relative; relative paths start at the Kubelet's current working directory. Omit this flag to use the built-in default configuration values. Command-line flags override configuration from this file.\") fs.StringVar(&f.KubeConfig, \"kubeconfig\", f.KubeConfig, \"Path to a kubeconfig file, specifying how to connect to the API server. Providing --kubeconfig enables API server mode, omitting --kubeconfig enables standalone mode.\") fs.StringVar(&f.BootstrapKubeconfig, \"bootstrap-kubeconfig\", f.BootstrapKubeconfig, \"Path to a kubeconfig file that will be used to get client certificate for kubelet. \"+ \"If the file specified by --kubeconfig does not exist, the bootstrap kubeconfig is used to request a client certificate from the API server. \"+ \"On success, a kubeconfig file referencing the generated client certificate and key is written to the path specified by --kubeconfig. \"+ \"The client certificate and key file will be stored in the directory pointed by --cert-dir.\") fs.BoolVar(&f.ReallyCrashForTesting, \"really-crash-for-testing\", f.ReallyCrashForTesting, \"If true, when panics occur crash. Intended for testing.\") fs.Float64Var(&f.ChaosChance, \"chaos-chance\", f.ChaosChance, \"If > 0.0, introduce random client errors and latency. Intended for testing.\") fs.BoolVar(&f.RunOnce, \"runonce\", f.RunOnce, \"If true, exit after spawning pods from static pod files or remote urls. Exclusive with --enable-server\") fs.BoolVar(&f.EnableServer, \"enable-server\", f.EnableServer, \"Enable the Kubelet's server\") fs.StringVar(&f.HostnameOverride, \"hostname-override\", f.HostnameOverride, \"If non-empty, will use this string as identification instead of the actual hostname. If --cloud-provider is set, the cloud provider determines the name of the node (consult cloud provider documentation to determine if and how the hostname is used).\") fs.StringVar(&f.NodeIP, \"node-ip\", f.NodeIP, \"IP address of the node. If set, kubelet will use this IP address for the node. If unset, kubelet will use the node's default IPv4 address, if any, or its default IPv6 address if it has no IPv4 addresses. You can pass `::` to make it prefer the default IPv6 address rather than the default IPv4 address.\") fs.StringVar(&f.ProviderID, \"provider-id\", f.ProviderID, \"Unique identifier for identifying the node in a machine database, i.e cloudprovider\") fs.StringVar(&f.CertDirectory, \"cert-dir\", f.CertDirectory, \"The directory where the TLS certs are located. \"+ \"If --tls-cert-file and --tls-private-key-file are provided, this flag will be ignored.\") fs.StringVar(&f.CloudProvider, \"cloud-provider\", f.CloudProvider, \"The provider for cloud services. Specify empty string for running with no cloud provider. If set, the cloud provider determines the name of the node (consult cloud provider documentation to determine if and how the hostname is used).\") fs.StringVar(&f.CloudConfigFile, \"cloud-config\", f.CloudConfigFile, \"The path to the cloud provider configuration file. Empty string for no configuration file.\") fs.StringVar(&f.RootDirectory, \"root-dir\", f.RootDirectory, \"Directory path for managing kubelet files (volume mounts,etc).\") fs.Var(&f.DynamicConfigDir, \"dynamic-config-dir\", \"The Kubelet will use this directory for checkpointing downloaded configurations and tracking configuration health. The Kubelet will create this directory if it does not already exist. The path may be absolute or relative; relative paths start at the Kubelet's current working directory. Providing this flag enables dynamic Kubelet configuration. The DynamicKubeletConfig feature gate must be enabled to pass this flag; this gate currently defaults to true because the feature is beta.\") fs.BoolVar(&f.RegisterNode, \"register-node\", f.RegisterNode, \"Register the node with the apiserver. If --kubeconfig is not provided, this flag is irrelevant, as the Kubelet won't have an apiserver to register with.\") fs.Var(utiltaints.NewTaintsVar(&f.RegisterWithTaints), \"register-with-taints\", \"Register the node with the given list of taints (comma separated \\\"=:\\\"). No-op if register-node is false.\") // EXPERIMENTAL FLAGS fs.StringVar(&f.ExperimentalMounterPath, \"experimental-mounter-path\", f.ExperimentalMounterPath, \"[Experimental] Path of mounter binary. Leave empty to use the default mount.\") fs.BoolVar(&f.ExperimentalKernelMemcgNotification, \"experimental-kernel-memcg-notification\", f.ExperimentalKernelMemcgNotification, \"If enabled, the kubelet will integrate with the kernel memcg notification to determine if memory eviction thresholds are crossed rather than polling.\") fs.StringVar(&f.RemoteRuntimeEndpoint, \"container-runtime-endpoint\", f.RemoteRuntimeEndpoint, \"[Experimental] The endpoint of remote runtime service. Currently unix socket endpoint is supported on Linux, while npipe and tcp endpoints are supported on windows. Examples:'unix:///var/run/dockershim.sock', 'npipe:////./pipe/dockershim'\") fs.StringVar(&f.RemoteImageEndpoint, \"image-service-endpoint\", f.RemoteImageEndpoint, \"[Experimental] The endpoint of remote image service. If not specified, it will be the same with container-runtime-endpoint by default. Currently unix socket endpoint is supported on Linux, while npipe and tcp endpoints are supported on windows. Examples:'unix:///var/run/dockershim.sock', 'npipe:////./pipe/dockershim'\") fs.BoolVar(&f.ExperimentalCheckNodeCapabilitiesBeforeMount, \"experimental-check-node-capabilities-before-mount\", f.ExperimentalCheckNodeCapabilitiesBeforeMount, \"[Experimental] if set true, the kubelet will check the underlying node for required components (binaries, etc.) before performing the mount\") fs.BoolVar(&f.ExperimentalNodeAllocatableIgnoreEvictionThreshold, \"experimental-allocatable-ignore-eviction\", f.ExperimentalNodeAllocatableIgnoreEvictionThreshold, \"When set to 'true', Hard Eviction Thresholds will be ignored while calculating Node Allocatable. See https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ for more details. [default=false]\") bindableNodeLabels := cliflag.ConfigurationMap(f.NodeLabels) fs.Var(&bindableNodeLabels, \"node-labels\", fmt.Sprintf(\" Labels to add when registering the node in the cluster. Labels must be key=value pairs separated by ','. Labels in the 'kubernetes.io' namespace must begin with an allowed prefix (%s) or be in the specifically allowed set (%s)\", strings.Join(kubeletapis.KubeletLabelNamespaces(), \", \"), strings.Join(kubeletapis.KubeletLabels(), \", \"))) fs.StringVar(&f.VolumePluginDir, \"volume-plugin-dir\", f.VolumePluginDir, \"The full path of the directory in which to search for additional third party volume plugins\") fs.StringVar(&f.LockFilePath, \"lock-file\", f.LockFilePath, \" The path to file for kubelet to use as a lock file.\") fs.BoolVar(&f.ExitOnLockContention, \"exit-on-lock-contention\", f.ExitOnLockContention, \"Whether kubelet should exit upon lock-file contention.\") fs.StringVar(&f.SeccompProfileRoot, \"seccomp-profile-root\", f.SeccompProfileRoot, \" Directory path for seccomp profiles.\") fs.StringVar(&f.BootstrapCheckpointPath, \"bootstrap-checkpoint-path\", f.BootstrapCheckpointPath, \" Path to the directory where the checkpoints are stored\") fs.Int32Var(&f.NodeStatusMaxImages, \"node-status-max-images\", f.NodeStatusMaxImages, \" The maximum number of images to report in Node.Status.Images. If -1 is specified, no cap will be applied.\") // DEPRECATED FLAGS fs.StringVar(&f.BootstrapKubeconfig, \"experimental-bootstrap-kubeconfig\", f.BootstrapKubeconfig, \"\") fs.MarkDeprecated(\"experimental-bootstrap-kubeconfig\", \"Use --bootstrap-kubeconfig\") fs.DurationVar(&f.MinimumGCAge.Duration, \"minimum-container-ttl-duration\", f.MinimumGCAge.Duration, \"Minimum age for a finished container before it is garbage collected. Examples: '300ms', '10s' or '2h45m'\") fs.MarkDeprecated(\"minimum-container-ttl-duration\", \"Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.\") fs.Int32Var(&f.MaxPerPodContainerCount, \"maximum-dead-containers-per-container\", f.MaxPerPodContainerCount, \"Maximum number of old instances to retain per container. Each container takes up some disk space.\") fs.MarkDeprecated(\"maximum-dead-containers-per-container\", \"Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.\") fs.Int32Var(&f.MaxContainerCount, \"maximum-dead-containers\", f.MaxContainerCount, \"Maximum number of old instances of containers to retain globally. Each container takes up some disk space. To disable, set to a negative number.\") fs.MarkDeprecated(\"maximum-dead-containers\", \"Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.\") fs.StringVar(&f.MasterServiceNamespace, \"master-service-namespace\", f.MasterServiceNamespace, \"The namespace from which the kubernetes master services should be injected into pods\") fs.MarkDeprecated(\"master-service-namespace\", \"This flag will be removed in a future version.\") fs.BoolVar(&f.RegisterSchedulable, \"register-schedulable\", f.RegisterSchedulable, \"Register the node as schedulable. Won't have any effect if register-node is false.\") fs.MarkDeprecated(\"register-schedulable\", \"will be removed in a future version\") fs.StringVar(&f.NonMasqueradeCIDR, \"non-masquerade-cidr\", f.NonMasqueradeCIDR, \"Traffic to IPs outside this range will use IP masquerade. Set to '0.0.0.0/0' to never masquerade.\") fs.MarkDeprecated(\"non-masquerade-cidr\", \"will be removed in a future version\") fs.BoolVar(&f.KeepTerminatedPodVolumes, \"keep-terminated-pod-volumes\", f.KeepTerminatedPodVolumes, \"Keep terminated pod volumes mounted to the node after the pod terminates. Can be useful for debugging volume related issues.\") fs.MarkDeprecated(\"keep-terminated-pod-volumes\", \"will be removed in a future version\") fs.BoolVar(&f.EnableCAdvisorJSONEndpoints, \"enable-cadvisor-json-endpoints\", f.EnableCAdvisorJSONEndpoints, \"Enable cAdvisor json /spec and /stats/* endpoints.\") fs.MarkDeprecated(\"enable-cadvisor-json-endpoints\", \"will be removed in a future version\") } 函数大致流程如下: kubelet标识绑定ContainerRuntimeOptions结构体内字段 kubelet添加系统标识(windows) kubeletFlags标识赋值给cleanFlagSet options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig)调用解析 func NewKubeletCommand() *cobra.Command { ... options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig) options.AddGlobalFlags(cleanFlagSet) ... return cmd } AddKubeletConfigFlags()函数主体内容: // AddKubeletConfigFlags adds flags for a specific kubeletconfig.KubeletConfiguration to the specified FlagSet func AddKubeletConfigFlags(mainfs *pflag.FlagSet, c *kubeletconfig.KubeletConfiguration) { fs := pflag.NewFlagSet(\"\", pflag.ExitOnError) defer func() { // All KubeletConfiguration flags are now deprecated, and any new flags that point to // KubeletConfiguration fields are deprecated-on-creation. When removing flags at the end // of their deprecation period, be careful to check that they have *actually* been deprecated // members of the KubeletConfiguration for the entire deprecation period: // e.g. if a flag was added after this deprecation function, it may not be at the end // of its lifetime yet, even if the rest are. deprecated := \"This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.\" fs.VisitAll(func(f *pflag.Flag) { f.Deprecated = deprecated }) mainfs.AddFlagSet(fs) }() fs.BoolVar(&c.FailSwapOn, \"fail-swap-on\", c.FailSwapOn, \"Makes the Kubelet fail to start if swap is enabled on the node. \") fs.StringVar(&c.StaticPodPath, \"pod-manifest-path\", c.StaticPodPath, \"Path to the directory containing static pod files to run, or the path to a single static pod file. Files starting with dots will be ignored.\") fs.DurationVar(&c.SyncFrequency.Duration, \"sync-frequency\", c.SyncFrequency.Duration, \"Max period between synchronizing running containers and config\") fs.DurationVar(&c.FileCheckFrequency.Duration, \"file-check-frequency\", c.FileCheckFrequency.Duration, \"Duration between checking config files for new data\") fs.DurationVar(&c.HTTPCheckFrequency.Duration, \"http-check-frequency\", c.HTTPCheckFrequency.Duration, \"Duration between checking http for new data\") fs.StringVar(&c.StaticPodURL, \"manifest-url\", c.StaticPodURL, \"URL for accessing additional Pod specifications to run\") fs.Var(cliflag.NewColonSeparatedMultimapStringString(&c.StaticPodURLHeader), \"manifest-url-header\", \"Comma-separated list of HTTP headers to use when accessing the url provided to --manifest-url. Multiple headers with the same name will be added in the same order provided. This flag can be repeatedly invoked. For example: `--manifest-url-header 'a:hello,b:again,c:world' --manifest-url-header 'b:beautiful'`\") fs.Var(utilflag.IPVar{Val: &c.Address}, \"address\", \"The IP address for the Kubelet to serve on (set to `0.0.0.0` for all IPv4 interfaces and `::` for all IPv6 interfaces)\") fs.Int32Var(&c.Port, \"port\", c.Port, \"The port for the Kubelet to serve on.\") fs.Int32Var(&c.ReadOnlyPort, \"read-only-port\", c.ReadOnlyPort, \"The read-only port for the Kubelet to serve on with no authentication/authorization (set to 0 to disable)\") // Authentication fs.BoolVar(&c.Authentication.Anonymous.Enabled, \"anonymous-auth\", c.Authentication.Anonymous.Enabled, \"\"+ \"Enables anonymous requests to the Kubelet server. Requests that are not rejected by another \"+ \"authentication method are treated as anonymous requests. Anonymous requests have a username \"+ \"of system:anonymous, and a group name of system:unauthenticated.\") fs.BoolVar(&c.Authentication.Webhook.Enabled, \"authentication-token-webhook\", c.Authentication.Webhook.Enabled, \"\"+ \"Use the TokenReview API to determine authentication for bearer tokens.\") fs.DurationVar(&c.Authentication.Webhook.CacheTTL.Duration, \"authentication-token-webhook-cache-ttl\", c.Authentication.Webhook.CacheTTL.Duration, \"\"+ \"The duration to cache responses from the webhook token authenticator.\") fs.StringVar(&c.Authentication.X509.ClientCAFile, \"client-ca-file\", c.Authentication.X509.ClientCAFile, \"\"+ \"If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file \"+ \"is authenticated with an identity corresponding to the CommonName of the client certificate.\") // Authorization fs.StringVar((*string)(&c.Authorization.Mode), \"authorization-mode\", string(c.Authorization.Mode), \"\"+ \"Authorization mode for Kubelet server. Valid options are AlwaysAllow or Webhook. \"+ \"Webhook mode uses the SubjectAccessReview API to determine authorization.\") fs.DurationVar(&c.Authorization.Webhook.CacheAuthorizedTTL.Duration, \"authorization-webhook-cache-authorized-ttl\", c.Authorization.Webhook.CacheAuthorizedTTL.Duration, \"\"+ \"The duration to cache 'authorized' responses from the webhook authorizer.\") fs.DurationVar(&c.Authorization.Webhook.CacheUnauthorizedTTL.Duration, \"authorization-webhook-cache-unauthorized-ttl\", c.Authorization.Webhook.CacheUnauthorizedTTL.Duration, \"\"+ \"The duration to cache 'unauthorized' responses from the webhook authorizer.\") fs.StringVar(&c.TLSCertFile, \"tls-cert-file\", c.TLSCertFile, \"\"+ \"File containing x509 Certificate used for serving HTTPS (with intermediate certs, if any, concatenated after server cert). \"+ \"If --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key \"+ \"are generated for the public address and saved to the directory passed to --cert-dir.\") fs.StringVar(&c.TLSPrivateKeyFile, \"tls-private-key-file\", c.TLSPrivateKeyFile, \"File containing x509 private key matching --tls-cert-file.\") fs.BoolVar(&c.ServerTLSBootstrap, \"rotate-server-certificates\", c.ServerTLSBootstrap, \"Auto-request and rotate the kubelet serving certificates by requesting new certificates from the kube-apiserver when the certificate expiration approaches. Requires the RotateKubeletServerCertificate feature gate to be enabled, and approval of the submitted CertificateSigningRequest objects.\") tlsCipherPossibleValues := cliflag.TLSCipherPossibleValues() fs.StringSliceVar(&c.TLSCipherSuites, \"tls-cipher-suites\", c.TLSCipherSuites, \"Comma-separated list of cipher suites for the server. \"+ \"If omitted, the default Go cipher suites will be used. \"+ \"Possible values: \"+strings.Join(tlsCipherPossibleValues, \",\")) tlsPossibleVersions := cliflag.TLSPossibleVersions() fs.StringVar(&c.TLSMinVersion, \"tls-min-version\", c.TLSMinVersion, \"Minimum TLS version supported. \"+ \"Possible values: \"+strings.Join(tlsPossibleVersions, \", \")) fs.BoolVar(&c.RotateCertificates, \"rotate-certificates\", c.RotateCertificates, \" Auto rotate the kubelet client certificates by requesting new certificates from the kube-apiserver when the certificate expiration approaches.\") fs.Int32Var(&c.RegistryPullQPS, \"registry-qps\", c.RegistryPullQPS, \"If > 0, limit registry pull QPS to this value. If 0, unlimited.\") fs.Int32Var(&c.RegistryBurst, \"registry-burst\", c.RegistryBurst, \"Maximum size of a bursty pulls, temporarily allows pulls to burst to this number, while still not exceeding registry-qps. Only used if --registry-qps > 0\") fs.Int32Var(&c.EventRecordQPS, \"event-qps\", c.EventRecordQPS, \"If > 0, limit event creations per second to this value. If 0, unlimited.\") fs.Int32Var(&c.EventBurst, \"event-burst\", c.EventBurst, \"Maximum size of a bursty event records, temporarily allows event records to burst to this number, while still not exceeding event-qps. Only used if --event-qps > 0\") fs.BoolVar(&c.EnableDebuggingHandlers, \"enable-debugging-handlers\", c.EnableDebuggingHandlers, \"Enables server endpoints for log collection and local running of containers and commands\") fs.BoolVar(&c.EnableContentionProfiling, \"contention-profiling\", c.EnableContentionProfiling, \"Enable lock contention profiling, if profiling is enabled\") fs.Int32Var(&c.HealthzPort, \"healthz-port\", c.HealthzPort, \"The port of the localhost healthz endpoint (set to 0 to disable)\") fs.Var(utilflag.IPVar{Val: &c.HealthzBindAddress}, \"healthz-bind-address\", \"The IP address for the healthz server to serve on (set to `0.0.0.0` for all IPv4 interfaces and `::` for all IPv6 interfaces)\") fs.Int32Var(&c.OOMScoreAdj, \"oom-score-adj\", c.OOMScoreAdj, \"The oom-score-adj value for kubelet process. Values must be within the range [-1000, 1000]\") fs.StringVar(&c.ClusterDomain, \"cluster-domain\", c.ClusterDomain, \"Domain for this cluster. If set, kubelet will configure all containers to search this domain in addition to the host's search domains\") fs.StringSliceVar(&c.ClusterDNS, \"cluster-dns\", c.ClusterDNS, \"Comma-separated list of DNS server IP address. This value is used for containers DNS server in case of Pods with \\\"dnsPolicy=ClusterFirst\\\". Note: all DNS servers appearing in the list MUST serve the same set of records otherwise name resolution within the cluster may not work correctly. There is no guarantee as to which DNS server may be contacted for name resolution.\") fs.DurationVar(&c.StreamingConnectionIdleTimeout.Duration, \"streaming-connection-idle-timeout\", c.StreamingConnectionIdleTimeout.Duration, \"Maximum time a streaming connection can be idle before the connection is automatically closed. 0 indicates no timeout. Example: '5m'\") fs.DurationVar(&c.NodeStatusUpdateFrequency.Duration, \"node-status-update-frequency\", c.NodeStatusUpdateFrequency.Duration, \"Specifies how often kubelet posts node status to master. Note: be cautious when changing the constant, it must work with nodeMonitorGracePeriod in nodecontroller.\") fs.DurationVar(&c.ImageMinimumGCAge.Duration, \"minimum-image-ttl-duration\", c.ImageMinimumGCAge.Duration, \"Minimum age for an unused image before it is garbage collected. Examples: '300ms', '10s' or '2h45m'.\") fs.Int32Var(&c.ImageGCHighThresholdPercent, \"image-gc-high-threshold\", c.ImageGCHighThresholdPercent, \"The percent of disk usage after which image garbage collection is always run. Values must be within the range [0, 100], To disable image garbage collection, set to 100. \") fs.Int32Var(&c.ImageGCLowThresholdPercent, \"image-gc-low-threshold\", c.ImageGCLowThresholdPercent, \"The percent of disk usage before which image garbage collection is never run. Lowest disk usage to garbage collect to. Values must be within the range [0, 100] and should not be larger than that of --image-gc-high-threshold.\") fs.DurationVar(&c.VolumeStatsAggPeriod.Duration, \"volume-stats-agg-period\", c.VolumeStatsAggPeriod.Duration, \"Specifies interval for kubelet to calculate and cache the volume disk usage for all pods and volumes. To disable volume calculations, set to 0.\") fs.Var(cliflag.NewMapStringBool(&c.FeatureGates), \"feature-gates\", \"A set of key=value pairs that describe feature gates for alpha/experimental features. \"+ \"Options are:\\n\"+strings.Join(utilfeature.DefaultFeatureGate.KnownFeatures(), \"\\n\")) fs.StringVar(&c.KubeletCgroups, \"kubelet-cgroups\", c.KubeletCgroups, \"Optional absolute name of cgroups to create and run the Kubelet in.\") fs.StringVar(&c.SystemCgroups, \"system-cgroups\", c.SystemCgroups, \"Optional absolute name of cgroups in which to place all non-kernel processes that are not already inside a cgroup under `/`. Empty for no container. Rolling back the flag requires a reboot.\") fs.BoolVar(&c.CgroupsPerQOS, \"cgroups-per-qos\", c.CgroupsPerQOS, \"Enable creation of QoS cgroup hierarchy, if true top level QoS and pod cgroups are created.\") fs.StringVar(&c.CgroupDriver, \"cgroup-driver\", c.CgroupDriver, \"Driver that the kubelet uses to manipulate cgroups on the host. Possible values: 'cgroupfs', 'systemd'\") fs.StringVar(&c.CgroupRoot, \"cgroup-root\", c.CgroupRoot, \"Optional root cgroup to use for pods. This is handled by the container runtime on a best effort basis. Default: '', which means use the container runtime default.\") fs.StringVar(&c.CPUManagerPolicy, \"cpu-manager-policy\", c.CPUManagerPolicy, \"CPU Manager policy to use. Possible values: 'none', 'static'. Default: 'none'\") fs.DurationVar(&c.CPUManagerReconcilePeriod.Duration, \"cpu-manager-reconcile-period\", c.CPUManagerReconcilePeriod.Duration, \" CPU Manager reconciliation period. Examples: '10s', or '1m'. If not supplied, defaults to `NodeStatusUpdateFrequency`\") fs.Var(cliflag.NewMapStringString(&c.QOSReserved), \"qos-reserved\", \" A set of ResourceName=Percentage (e.g. memory=50%) pairs that describe how pod resource requests are reserved at the QoS level. Currently only memory is supported. Requires the QOSReserved feature gate to be enabled.\") fs.StringVar(&c.TopologyManagerPolicy, \"topology-manager-policy\", c.TopologyManagerPolicy, \"Topology Manager policy to use. Possible values: 'none', 'best-effort', 'restricted', 'single-numa-node'.\") fs.DurationVar(&c.RuntimeRequestTimeout.Duration, \"runtime-request-timeout\", c.RuntimeRequestTimeout.Duration, \"Timeout of all runtime requests except long running request - pull, logs, exec and attach. When timeout exceeded, kubelet will cancel the request, throw out an error and retry later.\") fs.StringVar(&c.HairpinMode, \"hairpin-mode\", c.HairpinMode, \"How should the kubelet setup hairpin NAT. This allows endpoints of a Service to loadbalance back to themselves if they should try to access their own Service. Valid values are \\\"promiscuous-bridge\\\", \\\"hairpin-veth\\\" and \\\"none\\\".\") fs.Int32Var(&c.MaxPods, \"max-pods\", c.MaxPods, \"Number of Pods that can run on this Kubelet.\") fs.StringVar(&c.PodCIDR, \"pod-cidr\", c.PodCIDR, \"The CIDR to use for pod IP addresses, only used in standalone mode. In cluster mode, this is obtained from the master. For IPv6, the maximum number of IP's allocated is 65536\") fs.Int64Var(&c.PodPidsLimit, \"pod-max-pids\", c.PodPidsLimit, \"Set the maximum number of processes per pod. If -1, the kubelet defaults to the node allocatable pid capacity.\") fs.StringVar(&c.ResolverConfig, \"resolv-conf\", c.ResolverConfig, \"Resolver configuration file used as the basis for the container DNS resolution configuration.\") fs.BoolVar(&c.CPUCFSQuota, \"cpu-cfs-quota\", c.CPUCFSQuota, \"Enable CPU CFS quota enforcement for containers that specify CPU limits\") fs.DurationVar(&c.CPUCFSQuotaPeriod.Duration, \"cpu-cfs-quota-period\", c.CPUCFSQuotaPeriod.Duration, \"Sets CPU CFS quota period value, cpu.cfs_period_us, defaults to Linux Kernel default\") fs.BoolVar(&c.EnableControllerAttachDetach, \"enable-controller-attach-detach\", c.EnableControllerAttachDetach, \"Enables the Attach/Detach controller to manage attachment/detachment of volumes scheduled to this node, and disables kubelet from executing any attach/detach operations\") fs.BoolVar(&c.MakeIPTablesUtilChains, \"make-iptables-util-chains\", c.MakeIPTablesUtilChains, \"If true, kubelet will ensure iptables utility rules are present on host.\") fs.Int32Var(&c.IPTablesMasqueradeBit, \"iptables-masquerade-bit\", c.IPTablesMasqueradeBit, \"The bit of the fwmark space to mark packets for SNAT. Must be within the range [0, 31]. Please match this parameter with corresponding parameter in kube-proxy.\") fs.Int32Var(&c.IPTablesDropBit, \"iptables-drop-bit\", c.IPTablesDropBit, \"The bit of the fwmark space to mark packets for dropping. Must be within the range [0, 31].\") fs.StringVar(&c.ContainerLogMaxSize, \"container-log-max-size\", c.ContainerLogMaxSize, \" Set the maximum size (e.g. 10Mi) of container log file before it is rotated. This flag can only be used with --container-runtime=remote.\") fs.Int32Var(&c.ContainerLogMaxFiles, \"container-log-max-files\", c.ContainerLogMaxFiles, \" Set the maximum number of container log files that can be present for a container. The number must be >= 2. This flag can only be used with --container-runtime=remote.\") fs.StringSliceVar(&c.AllowedUnsafeSysctls, \"allowed-unsafe-sysctls\", c.AllowedUnsafeSysctls, \"Comma-separated whitelist of unsafe sysctls or unsafe sysctl patterns (ending in *). Use these at your own risk.\") // Flags intended for testing, not recommended used in production environments. fs.Int64Var(&c.MaxOpenFiles, \"max-open-files\", c.MaxOpenFiles, \"Number of files that can be opened by Kubelet process.\") fs.StringVar(&c.ContentType, \"kube-api-content-type\", c.ContentType, \"Content type of requests sent to apiserver.\") fs.Int32Var(&c.KubeAPIQPS, \"kube-api-qps\", c.KubeAPIQPS, \"QPS to use while talking with kubernetes apiserver\") fs.Int32Var(&c.KubeAPIBurst, \"kube-api-burst\", c.KubeAPIBurst, \"Burst to use while talking with kubernetes apiserver\") fs.BoolVar(&c.SerializeImagePulls, \"serialize-image-pulls\", c.SerializeImagePulls, \"Pull images one at a time. We recommend *not* changing the default value on nodes that run docker daemon with version 该调用做了两件事情： 将kubeletconfig.KubeletConfiguration结构体字段与标识绑定 为cleanFlagSet添加kubelet配置项相关的标识（如：--max-open-files对应kubeletconfig.KubeletConfiguration结构体中MaxOpenFiles字段） 最终通过以下调用，为cleanFlagSet添加--help等标识 func NewKubeletCommand() *cobra.Command { ... options.AddGlobalFlags(cleanFlagSet) cleanFlagSet.BoolP(\"help\", \"h\", false, fmt.Sprintf(\"help for %s\", cmd.Name())) // ugly, but necessary, because Cobra's default UsageFunc and HelpFunc pollute the flagset with global flags const usageFmt = \"Usage:\\n %s\\n\\nFlags:\\n%s\" cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) }) return cmd } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/04kubelet指令解析.html":{"url":"2.容器/k8s/core/kubelet/启动/04kubelet指令解析.html","title":"04kubelet指令解析","keywords":"","body":"kubelet指令解析 完整的指令定义: cmd := &cobra.Command{ Use: componentKubelet, Long: `The kubelet is the primary \"node agent\" that runs on each node. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider. The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object that describes a pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms (primarily through the apiserver) and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes. Other than from an PodSpec from the apiserver, there are three ways that a container manifest can be provided to the Kubelet. File: Path passed as a flag on the command line. Files under this path will be monitored periodically for updates. The monitoring period is 20s by default and is configurable via a flag. HTTP endpoint: HTTP endpoint passed as a parameter on the command line. This endpoint is checked every 20 seconds (also configurable with a flag). HTTP server: The kubelet can also listen for HTTP and respond to a simple API (underspec'd currently) to submit a new manifest.`, // The Kubelet has special flag parsing requirements to enforce flag precedence rules, // so we do all our parsing manually in Run, below. // DisableFlagParsing=true provides the full set of flags passed to the kubelet in the // `args` arg to Run, without Cobra's interference. DisableFlagParsing: true, Run: func(cmd *cobra.Command, args []string) { // initial flag parse, since we disable cobra's flag parsing if err := cleanFlagSet.Parse(args); err != nil { cmd.Usage() klog.Fatal(err) } // check if there are non-flag arguments in the command line cmds := cleanFlagSet.Args() if len(cmds) > 0 { cmd.Usage() klog.Fatalf(\"unknown command: %s\", cmds[0]) } // short-circuit on help help, err := cleanFlagSet.GetBool(\"help\") if err != nil { klog.Fatal(`\"help\" flag is non-bool, programmer error, please correct`) } if help { cmd.Help() return } // short-circuit on verflag verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cleanFlagSet) // set feature gates from initial flags-based config if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { klog.Fatal(err) } // validate the initial KubeletFlags if err := options.ValidateKubeletFlags(kubeletFlags); err != nil { klog.Fatal(err) } if kubeletFlags.ContainerRuntime == \"remote\" && cleanFlagSet.Changed(\"pod-infra-container-image\") { klog.Warning(\"Warning: For remote container runtime, --pod-infra-container-image is ignored in kubelet, which should be set in that remote runtime instead\") } // load kubelet config file, if provided if configFile := kubeletFlags.KubeletConfigFile; len(configFile) > 0 { kubeletConfig, err = loadConfigFile(configFile) if err != nil { klog.Fatal(err) } // We must enforce flag precedence by re-parsing the command line into the new object. // This is necessary to preserve backwards-compatibility across binary upgrades. // See issue #56171 for more details. if err := kubeletConfigFlagPrecedence(kubeletConfig, args); err != nil { klog.Fatal(err) } // update feature gates based on new config if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { klog.Fatal(err) } } // We always validate the local configuration (command line + config file). // This is the default \"last-known-good\" config for dynamic config, and must always remain valid. if err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil { klog.Fatal(err) } // use dynamic kubelet config, if enabled var kubeletConfigController *dynamickubeletconfig.Controller if dynamicConfigDir := kubeletFlags.DynamicConfigDir.Value(); len(dynamicConfigDir) > 0 { var dynamicKubeletConfig *kubeletconfiginternal.KubeletConfiguration dynamicKubeletConfig, kubeletConfigController, err = BootstrapKubeletConfigController(dynamicConfigDir, func(kc *kubeletconfiginternal.KubeletConfiguration) error { // Here, we enforce flag precedence inside the controller, prior to the controller's validation sequence, // so that we get a complete validation at the same point where we can decide to reject dynamic config. // This fixes the flag-precedence component of issue #63305. // See issue #56171 for general details on flag precedence. return kubeletConfigFlagPrecedence(kc, args) }) if err != nil { klog.Fatal(err) } // If we should just use our existing, local config, the controller will return a nil config if dynamicKubeletConfig != nil { kubeletConfig = dynamicKubeletConfig // Note: flag precedence was already enforced in the controller, prior to validation, // by our above transform function. Now we simply update feature gates from the new config. if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { klog.Fatal(err) } } } // construct a KubeletServer from kubeletFlags and kubeletConfig kubeletServer := &options.KubeletServer{ KubeletFlags: *kubeletFlags, KubeletConfiguration: *kubeletConfig, } // use kubeletServer to construct the default KubeletDeps kubeletDeps, err := UnsecuredDependencies(kubeletServer, utilfeature.DefaultFeatureGate) if err != nil { klog.Fatal(err) } // add the kubelet config controller to kubeletDeps kubeletDeps.KubeletConfigController = kubeletConfigController // set up stopCh here in order to be reused by kubelet and docker shim stopCh := genericapiserver.SetupSignalHandler() // start the experimental docker shim, if enabled if kubeletServer.KubeletFlags.ExperimentalDockershim { if err := RunDockershim(&kubeletServer.KubeletFlags, kubeletConfig, stopCh); err != nil { klog.Fatal(err) } return } // run the kubelet klog.V(5).Infof(\"KubeletConfiguration: %#v\", kubeletServer.KubeletConfiguration) if err := Run(kubeletServer, kubeletDeps, utilfeature.DefaultFeatureGate, stopCh); err != nil { klog.Fatal(err) } }, } // keep cleanFlagSet separate, so Cobra doesn't pollute it with the global flags kubeletFlags.AddFlags(cleanFlagSet) options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig) options.AddGlobalFlags(cleanFlagSet) cleanFlagSet.BoolP(\"help\", \"h\", false, fmt.Sprintf(\"help for %s\", cmd.Name())) // ugly, but necessary, because Cobra's default UsageFunc and HelpFunc pollute the flagset with global flags const usageFmt = \"Usage:\\n %s\\n\\nFlags:\\n%s\" cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) }) return cmd } 这里我们只探讨Run部分内容，这部分内容为实际运行时逻辑，其他部分内容可参考cobra 内容较多，我们按步骤进行拆解分析 1.命令行参数解析 标识解析 if err := cleanFlagSet.Parse(args); err != nil { cmd.Usage() klog.Fatal(err) } 第一步解析命令行的入参（如: --kubeconfig、--config），如果解析阶段出现异常（通常为标识名或值错误）， 调用cmd.Usage()输出可选标识，退出启动。 子命令解析 kubelet无子命令（如：kubectl apply中的apply为kubectl的子命令），若解析出含有子命令，调用cmd.Usage()输出可选标识，退出启动。 // check if there are non-flag arguments in the command line cmds := cleanFlagSet.Args() if len(cmds) > 0 { cmd.Usage() klog.Fatalf(\"unknown command: %s\", cmds[0]) } 判断是否为--help或-h 如果为help标识，输出以下内容，退出启动流程 $ kubelet -h The kubelet is the primary \"node agent\" that runs on each node. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider. The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object that describes a pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms (primarily through the apiserver) and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes. Other than from an PodSpec from the apiserver, there are three ways that a container manifest can be provided to the Kubelet. File: Path passed as a flag on the command line. Files under this path will be monitored periodically for updates. The monitoring period is 20s by default and is configurable via a flag. HTTP endpoint: HTTP endpoint passed as a parameter on the command line. This endpoint is checked every 20 seconds (also configurable with a flag). HTTP server: The kubelet can also listen for HTTP and respond to a simple API (underspec'd currently) to submit a new manifest. Usage: kubelet [flags] Flags: --add-dir-header If true, adds the file directory to the header --address 0.0.0.0 The IP address for the Kubelet to serve on (set to 0.0.0.0 for all IPv4 interfaces and `::` for all IPv6 interfaces) (default 0.0.0.0) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.) --allowed-unsafe-sysctls strings Comma-separated whitelist of unsafe sysctls or unsafe sysctl patterns (ending in *). Use these at your own risk. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.) --alsologtostderr log to standard error as well as files --anonymous-auth Enables anonymous requests to the Kubelet server. Requests that are not rejected by another authentication method are treated as anonymous requests. Anonymous requests have a username of system:anonymous, and a group name of system:unauthenticated. (default true) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.) --application-metrics-count-limit int Max number of application metrics to store (per container) (default 100) (DEPRECATED: This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.) --authentication-token-webhook Use the TokenReview API to determine authentication for bearer tokens. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.) --authentication-token-webhook-cache-ttl duration The duration to cache responses from the webhook token authenticator. (default 2m0s) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.) --authorization-mode string Authorization mode for Kubelet server. Valid options are AlwaysAllow or Webhook. Webhook mode uses the SubjectAccessReview API to determine authorization. (default \"AlwaysAllow\") (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.) ... 判断是否为--version或-v 如果为version标识，输出以下内容，退出启动流程 $ kubelet --version Kubernetes v1.18.6 2.设置门控特性 // set feature gates from initial flags-based config if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { klog.Fatal(err) } 根据门控特性标识--feature-gates设置门控特性，特性包含如下： --feature-gates mapStringBool A set of key=value pairs that describe feature gates for alpha/experimental features. Options are: APIListChunking=true|false (BETA - default=true) APIPriorityAndFairness=true|false (ALPHA - default=false) APIResponseCompression=true|false (BETA - default=true) AllAlpha=true|false (ALPHA - default=false) AllBeta=true|false (BETA - default=false) AllowInsecureBackendProxy=true|false (BETA - default=true) AnyVolumeDataSource=true|false (ALPHA - default=false) AppArmor=true|false (BETA - default=true) BalanceAttachedNodeVolumes=true|false (ALPHA - default=false) BoundServiceAccountTokenVolume=true|false (ALPHA - default=false) CPUManager=true|false (BETA - default=true) CRIContainerLogRotation=true|false (BETA - default=true) CSIInlineVolume=true|false (BETA - default=true) CSIMigration=true|false (BETA - default=true) CSIMigrationAWS=true|false (BETA - default=false) CSIMigrationAWSComplete=true|false (ALPHA - default=false) CSIMigrationAzureDisk=true|false (ALPHA - default=false) CSIMigrationAzureDiskComplete=true|false (ALPHA - default=false) CSIMigrationAzureFile=true|false (ALPHA - default=false) CSIMigrationAzureFileComplete=true|false (ALPHA - default=false) CSIMigrationGCE=true|false (BETA - default=false) CSIMigrationGCEComplete=true|false (ALPHA - default=false) CSIMigrationOpenStack=true|false (BETA - default=false) CSIMigrationOpenStackComplete=true|false (ALPHA - default=false) ConfigurableFSGroupPolicy=true|false (ALPHA - default=false) CustomCPUCFSQuotaPeriod=true|false (ALPHA - default=false) DefaultIngressClass=true|false (BETA - default=true) DevicePlugins=true|false (BETA - default=true) DryRun=true|false (BETA - default=true) DynamicAuditing=true|false (ALPHA - default=false) DynamicKubeletConfig=true|false (BETA - default=true) EndpointSlice=true|false (BETA - default=true) EndpointSliceProxying=true|false (ALPHA - default=false) EphemeralContainers=true|false (ALPHA - default=false) EvenPodsSpread=true|false (BETA - default=true) ExpandCSIVolumes=true|false (BETA - default=true) ExpandInUsePersistentVolumes=true|false (BETA - default=true) ExpandPersistentVolumes=true|false (BETA - default=true) ExperimentalHostUserNamespaceDefaulting=true|false (BETA - default=false) HPAScaleToZero=true|false (ALPHA - default=false) HugePageStorageMediumSize=true|false (ALPHA - default=false) HyperVContainer=true|false (ALPHA - default=false) IPv6DualStack=true|false (ALPHA - default=false) ImmutableEphemeralVolumes=true|false (ALPHA - default=false) KubeletPodResources=true|false (BETA - default=true) LegacyNodeRoleBehavior=true|false (ALPHA - default=true) LocalStorageCapacityIsolation=true|false (BETA - default=true) LocalStorageCapacityIsolationFSQuotaMonitoring=true|false (ALPHA - default=false) NodeDisruptionExclusion=true|false (ALPHA - default=false) NonPreemptingPriority=true|false (ALPHA - default=false) PodDisruptionBudget=true|false (BETA - default=true) PodOverhead=true|false (BETA - default=true) ProcMountType=true|false (ALPHA - default=false) QOSReserved=true|false (ALPHA - default=false) RemainingItemCount=true|false (BETA - default=true) RemoveSelfLink=true|false (ALPHA - default=false) ResourceLimitsPriorityFunction=true|false (ALPHA - default=false) RotateKubeletClientCertificate=true|false (BETA - default=true) RotateKubeletServerCertificate=true|false (BETA - default=true) RunAsGroup=true|false (BETA - default=true) RuntimeClass=true|false (BETA - default=true) SCTPSupport=true|false (ALPHA - default=false) SelectorIndex=true|false (ALPHA - default=false) ServerSideApply=true|false (BETA - default=true) ServiceAccountIssuerDiscovery=true|false (ALPHA - default=false) ServiceAppProtocol=true|false (ALPHA - default=false) ServiceNodeExclusion=true|false (ALPHA - default=false) ServiceTopology=true|false (ALPHA - default=false) StartupProbe=true|false (BETA - default=true) StorageVersionHash=true|false (BETA - default=true) SupportNodePidsLimit=true|false (BETA - default=true) SupportPodPidsLimit=true|false (BETA - default=true) Sysctls=true|false (BETA - default=true) TTLAfterFinished=true|false (ALPHA - default=false) TokenRequest=true|false (BETA - default=true) TokenRequestProjection=true|false (BETA - default=true) TopologyManager=true|false (BETA - default=true) ValidateProxyRedirects=true|false (BETA - default=true) VolumeSnapshotDataSource=true|false (BETA - default=true) WinDSR=true|false (ALPHA - default=false) WinOverlay=true|false (ALPHA - default=false) (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.) 3.标识合法性检测 // validate the initial KubeletFlags if err := options.ValidateKubeletFlags(kubeletFlags); err != nil { klog.Fatal(err) } 检测内容如下： 如果指定了--dynamic-config-dir标识，却未开启DynamicKubeletConfig特性门控，返回异常退出启动流程 --node-status-max-images标识值不能小于-1 --node-labels节点标签合法性检测 如果运行时标识值为remote，并且--pod-infra-container-image值非空，会输出警告信息: Warning: For remote container runtime, --pod-infra-container-image is ignored in kubelet, which should be set in that remote runtime instead 4.配置导入 // load kubelet config file, if provided if configFile := kubeletFlags.KubeletConfigFile; len(configFile) > 0 { kubeletConfig, err = loadConfigFile(configFile) if err != nil { klog.Fatal(err) } // We must enforce flag precedence by re-parsing the command line into the new object. // This is necessary to preserve backwards-compatibility across binary upgrades. // See issue #56171 for more details. if err := kubeletConfigFlagPrecedence(kubeletConfig, args); err != nil { klog.Fatal(err) } // update feature gates based on new config if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { klog.Fatal(err) } } 若--config值非空，加载配置文件内容 具体流程如下： 加载文件内容，序列化 在解析完--config配置后重新解析命令行标识，避免配置混乱。若配置文件内与命令标识存在相同配置属性，命令行优先级高于配置文件 配置文件内如果存在特新门控配置，则会追加赋值 再次检测配置的合法性，检测内容包参考kubernetes/pkg/kubelet/apis/config/validation/validation.go // ValidateKubeletConfiguration validates `kc` and returns an error if it is invalid func ValidateKubeletConfiguration(kc *kubeletconfig.KubeletConfiguration) error { allErrors := []error{} // Make a local copy of the global feature gates and combine it with the gates set by this configuration. // This allows us to validate the config against the set of gates it will actually run against. localFeatureGate := utilfeature.DefaultFeatureGate.DeepCopy() if err := localFeatureGate.SetFromMap(kc.FeatureGates); err != nil { return err } if kc.NodeLeaseDurationSeconds 0 { allErrors = append(allErrors, fmt.Errorf(\"invalid configuration: EnforceNodeAllocatable (--enforce-node-allocatable) is not supported unless CgroupsPerQOS (--cgroups-per-qos) feature is turned on\")) } if kc.SystemCgroups != \"\" && kc.CgroupRoot == \"\" { allErrors = append(allErrors, fmt.Errorf(\"invalid configuration: SystemCgroups (--system-cgroups) was specified and CgroupRoot (--cgroup-root) was not specified\")) } if kc.EventBurst = kc.ImageGCHighThresholdPercent { allErrors = append(allErrors, fmt.Errorf(\"invalid configuration: ImageGCLowThresholdPercent (--image-gc-low-threshold) %v must be less than ImageGCHighThresholdPercent (--image-gc-high-threshold) %v\", kc.ImageGCLowThresholdPercent, kc.ImageGCHighThresholdPercent)) } if utilvalidation.IsInRange(int(kc.IPTablesDropBit), 0, 31) != nil { allErrors = append(allErrors, fmt.Errorf(\"invalid configuration: IPTablesDropBit (--iptables-drop-bit) %v must be between 0 and 31, inclusive\", kc.IPTablesDropBit)) } if utilvalidation.IsInRange(int(kc.IPTablesMasqueradeBit), 0, 31) != nil { allErrors = append(allErrors, fmt.Errorf(\"invalid configuration: IPTablesMasqueradeBit (--iptables-masquerade-bit) %v must be between 0 and 31, inclusive\", kc.IPTablesMasqueradeBit)) } if kc.KubeAPIBurst 1 { allErrors = append(allErrors, fmt.Errorf(\"invalid configuration: EnforceNodeAllocatable (--enforce-node-allocatable) may not contain additional enforcements when '%s' is specified\", kubetypes.NodeAllocatableNoneKey)) } default: allErrors = append(allErrors, fmt.Errorf(\"invalid configuration: option %q specified for EnforceNodeAllocatable (--enforce-node-allocatable). Valid options are %q, %q, %q, or %q\", val, kubetypes.NodeAllocatableEnforcementKey, kubetypes.SystemReservedEnforcementKey, kubetypes.KubeReservedEnforcementKey, kubetypes.NodeAllocatableNoneKey)) } } switch kc.HairpinMode { case kubeletconfig.HairpinNone: case kubeletconfig.HairpinVeth: case kubeletconfig.PromiscuousBridge: default: allErrors = append(allErrors, fmt.Errorf(\"invalid configuration: option %q specified for HairpinMode (--hairpin-mode). Valid options are %q, %q or %q\", kc.HairpinMode, kubeletconfig.HairpinNone, kubeletconfig.HairpinVeth, kubeletconfig.PromiscuousBridge)) } if kc.ReservedSystemCPUs != \"\" { // --reserved-cpus does not support --system-reserved-cgroup or --kube-reserved-cgroup if kc.SystemReservedCgroup != \"\" || kc.KubeReservedCgroup != \"\" { allErrors = append(allErrors, fmt.Errorf(\"can't use --reserved-cpus with --system-reserved-cgroup or --kube-reserved-cgroup\")) } if _, err := cpuset.Parse(kc.ReservedSystemCPUs); err != nil { allErrors = append(allErrors, fmt.Errorf(\"unable to parse --reserved-cpus, error: %v\", err)) } } if err := validateKubeletOSConfiguration(kc); err != nil { allErrors = append(allErrors, err) } allErrors = append(allErrors, metrics.ValidateShowHiddenMetricsVersion(kc.ShowHiddenMetricsForVersion)...) return utilerrors.NewAggregate(allErrors) } 若--dynamic-config-dir值非空，kubelet将使用此目录对下载的配置进行检查点和跟踪配置运行状况。 如果该目录不存在，kubelet将创建该目录。路径可以是绝对的，也可以是相对的。 相对路径从kubelet的当前工作目录开始。提供此标志将启用动态kubelet配置。必须启用DynamicKubeletConfig特性门才能通过该标志;这个门目前默认为true，因为该功能处于beta版。 关于kubelet动态配置解析请参考Dynamic Kubelet Configuration 同时定义--dynamic-config-dir与--config时，kubelet会--dynamic-config-dir中的动态配置。--config的配置不会生效 5.启动dockershim 当指定开启dockershim only模式时，才会走这个启动逻辑 // start the experimental docker shim, if enabled if kubeletServer.KubeletFlags.ExperimentalDockershim { if err := RunDockershim(&kubeletServer.KubeletFlags, kubeletConfig, stopCh); err != nil { klog.Fatal(err) } return } 一般用于测试容器运行时，官方考虑后续提供独立的二进制文件，我们暂且不做讨论。 6.启动server前的准备 经过前几个步骤的准备，我们已经获取了kubelet的配置参数，接下来我们分析启动前还需要做哪些准备： // construct a KubeletServer from kubeletFlags and kubeletConfig kubeletServer := &options.KubeletServer{ KubeletFlags: *kubeletFlags, KubeletConfiguration: *kubeletConfig, } // use kubeletServer to construct the default KubeletDeps kubeletDeps, err := UnsecuredDependencies(kubeletServer, utilfeature.DefaultFeatureGate) if err != nil { klog.Fatal(err) } // add the kubelet config controller to kubeletDeps kubeletDeps.KubeletConfigController = kubeletConfigController // set up stopCh here in order to be reused by kubelet and docker shim stopCh := genericapiserver.SetupSignalHandler() // start the experimental docker shim, if enabled if kubeletServer.KubeletFlags.ExperimentalDockershim { if err := RunDockershim(&kubeletServer.KubeletFlags, kubeletConfig, stopCh); err != nil { klog.Fatal(err) } return } // run the kubelet klog.V(5).Infof(\"KubeletConfiguration: %#v\", kubeletServer.KubeletConfiguration) if err := Run(kubeletServer, kubeletDeps, utilfeature.DefaultFeatureGate, stopCh); err != nil { klog.Fatal(err) } 透过源码我们了解了最后的Run函数执行启动的逻辑，而他需要四个参数： 参数一*options.KubeletServer: 包含kubelet启动所需的配置项与标识集合 参数二*kubelet.Dependencies: 实质是一个注入依赖的容器--在运行时构造的对象，这些对象是运行Kubelet所必需的。(如：想操作容器时，得需要实现容器运行时接口) 参数三featuregate.FeatureGate: 特性门控列表，决定开启/关闭那些特性 参数四: 用于作为主程序退出的信号通知其他各协程进行相关的退出操作 参数一、三、四很好理解，我们着重分析下*kubelet.Dependencies这个入参对象 kubelet.Dependencies结构体解析 kubelet初始化阶段，通过KubeletServer、DefaultFeatureGate入参实例化 // use kubeletServer to construct the default KubeletDeps kubeletDeps, err := UnsecuredDependencies(kubeletServer, utilfeature.DefaultFeatureGate) if err != nil { klog.Fatal(err) } // add the kubelet config controller to kubeletDeps kubeletDeps.KubeletConfigController = kubeletConfigController 我们看下kubelet.Dependencies结构体属性: type Dependencies struct { Options []Option // Injected Dependencies Auth server.AuthInterface CAdvisorInterface cadvisor.Interface Cloud cloudprovider.Interface ContainerManager cm.ContainerManager DockerClientConfig *dockershim.ClientConfig EventClient v1core.EventsGetter HeartbeatClient clientset.Interface OnHeartbeatFailure func() KubeClient clientset.Interface Mounter mount.Interface HostUtil hostutil.HostUtils OOMAdjuster *oom.OOMAdjuster OSInterface kubecontainer.OSInterface PodConfig *config.PodConfig Recorder record.EventRecorder Subpather subpath.Interface VolumePlugins []volume.VolumePlugin DynamicPluginProber volume.DynamicPluginProber TLSOptions *server.TLSOptions KubeletConfigController *kubeletconfig.Controller RemoteRuntimeService internalapi.RuntimeService RemoteImageService internalapi.ImageManagerService criHandler http.Handler dockerLegacyService dockershim.DockerLegacyService // remove it after cadvisor.UsingLegacyCadvisorStats dropped. useLegacyCadvisorStats bool } 我们看到kubelet.Dependencies其实是一个接口集合，包括对卷、容器运行时、kube-apiserver等操作的接口。针对这些接口的使用场景及实现我们不做过多讨论。 至此为止，所有的启动前准备工作均已完成（配置项，运行时所需接口），接下来我们将分析kubelet实际运行流程(Run函数) Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/05启动流程解析.html":{"url":"2.容器/k8s/core/kubelet/启动/05启动流程解析.html","title":"05启动流程解析","keywords":"","body":"概述 前文我们介绍了kubelet参数初始化，接下来我们分析下kubelet服务实际启动逻辑。 整体流程大致如下： 设置全局门控特性 kubelet参数合法性检测 注册当前配置至/configz端点 检查kubelet启动模式是否为standalone模式 检测kubeDeps是否为空，为空则初始化 获取主机名称，用于初始化事件记录器 standalone模式下关闭所有客户端连接 初始化身份认证接口 初始化cgroups 初始化cAdvisor 初始化事件记录器，用于向kubelet端事件 初始化容器管理器 检测是否以root用户运行kubelet 为kubelet进程设置OOM分数 容器运行时初始化 启动kubelet 如果开启动态配置，则监听动态配置中的配置变化 开启/healthz端点 通知init进程kubelet服务启动完毕 针对上述步骤我们接下来逐一分析，针对部分内容（容器运行时启动流程、kubelet启动流程）不做过多讨论，后续篇幅再做分析。 部分通用内容（如cAdvisor，/configz端点、/healthz端点，OOM分数等），后续篇幅讨论。 函数调用 if err := Run(kubeletServer, kubeletDeps, utilfeature.DefaultFeatureGate, stopCh); err != nil { klog.Fatal(err) } func Run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, featureGate featuregate.FeatureGate, stopCh 最终启动逻辑为run函数 func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, featureGate featuregate.FeatureGate, stopCh 0 { metrics.SetShowHidden() } // About to get clients and such, detect standaloneMode standaloneMode := true if len(s.KubeConfig) > 0 { standaloneMode = false } if kubeDeps == nil { kubeDeps, err = UnsecuredDependencies(s, featureGate) if err != nil { return err } } if kubeDeps.Cloud == nil { if !cloudprovider.IsExternal(s.CloudProvider) { cloud, err := cloudprovider.InitCloudProvider(s.CloudProvider, s.CloudConfigFile) if err != nil { return err } if cloud == nil { klog.V(2).Infof(\"No cloud provider specified: %q from the config file: %q\\n\", s.CloudProvider, s.CloudConfigFile) } else { klog.V(2).Infof(\"Successfully initialized cloud provider: %q from the config file: %q\\n\", s.CloudProvider, s.CloudConfigFile) } kubeDeps.Cloud = cloud } } hostName, err := nodeutil.GetHostname(s.HostnameOverride) if err != nil { return err } nodeName, err := getNodeName(kubeDeps.Cloud, hostName) if err != nil { return err } // if in standalone mode, indicate as much by setting all clients to nil switch { case standaloneMode: kubeDeps.KubeClient = nil kubeDeps.EventClient = nil kubeDeps.HeartbeatClient = nil klog.Warningf(\"standalone mode, no API client\") case kubeDeps.KubeClient == nil, kubeDeps.EventClient == nil, kubeDeps.HeartbeatClient == nil: clientConfig, closeAllConns, err := buildKubeletClientConfig(s, nodeName) if err != nil { return err } if closeAllConns == nil { return errors.New(\"closeAllConns must be a valid function other than nil\") } kubeDeps.OnHeartbeatFailure = closeAllConns kubeDeps.KubeClient, err = clientset.NewForConfig(clientConfig) if err != nil { return fmt.Errorf(\"failed to initialize kubelet client: %v\", err) } // make a separate client for events eventClientConfig := *clientConfig eventClientConfig.QPS = float32(s.EventRecordQPS) eventClientConfig.Burst = int(s.EventBurst) kubeDeps.EventClient, err = v1core.NewForConfig(&eventClientConfig) if err != nil { return fmt.Errorf(\"failed to initialize kubelet event client: %v\", err) } // make a separate client for heartbeat with throttling disabled and a timeout attached heartbeatClientConfig := *clientConfig heartbeatClientConfig.Timeout = s.KubeletConfiguration.NodeStatusUpdateFrequency.Duration // The timeout is the minimum of the lease duration and status update frequency leaseTimeout := time.Duration(s.KubeletConfiguration.NodeLeaseDurationSeconds) * time.Second if heartbeatClientConfig.Timeout > leaseTimeout { heartbeatClientConfig.Timeout = leaseTimeout } heartbeatClientConfig.QPS = float32(-1) kubeDeps.HeartbeatClient, err = clientset.NewForConfig(&heartbeatClientConfig) if err != nil { return fmt.Errorf(\"failed to initialize kubelet heartbeat client: %v\", err) } } if kubeDeps.Auth == nil { auth, runAuthenticatorCAReload, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration) if err != nil { return err } kubeDeps.Auth = auth runAuthenticatorCAReload(stopCh) } var cgroupRoots []string cgroupRoots = append(cgroupRoots, cm.NodeAllocatableRoot(s.CgroupRoot, s.CgroupDriver)) kubeletCgroup, err := cm.GetKubeletContainer(s.KubeletCgroups) if err != nil { klog.Warningf(\"failed to get the kubelet's cgroup: %v. Kubelet system container metrics may be missing.\", err) } else if kubeletCgroup != \"\" { cgroupRoots = append(cgroupRoots, kubeletCgroup) } runtimeCgroup, err := cm.GetRuntimeContainer(s.ContainerRuntime, s.RuntimeCgroups) if err != nil { klog.Warningf(\"failed to get the container runtime's cgroup: %v. Runtime system container metrics may be missing.\", err) } else if runtimeCgroup != \"\" { // RuntimeCgroups is optional, so ignore if it isn't specified cgroupRoots = append(cgroupRoots, runtimeCgroup) } if s.SystemCgroups != \"\" { // SystemCgroups is optional, so ignore if it isn't specified cgroupRoots = append(cgroupRoots, s.SystemCgroups) } if kubeDeps.CAdvisorInterface == nil { imageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint) kubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cgroupRoots, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint)) if err != nil { return err } } // Setup event recorder if required. makeEventRecorder(kubeDeps, nodeName) if kubeDeps.ContainerManager == nil { if s.CgroupsPerQOS && s.CgroupRoot == \"\" { klog.Info(\"--cgroups-per-qos enabled, but --cgroup-root was not specified. defaulting to /\") s.CgroupRoot = \"/\" } var reservedSystemCPUs cpuset.CPUSet var errParse error if s.ReservedSystemCPUs != \"\" { reservedSystemCPUs, errParse = cpuset.Parse(s.ReservedSystemCPUs) if errParse != nil { // invalid cpu list is provided, set reservedSystemCPUs to empty, so it won't overwrite kubeReserved/systemReserved klog.Infof(\"Invalid ReservedSystemCPUs \\\"%s\\\"\", s.ReservedSystemCPUs) return errParse } // is it safe do use CAdvisor here ?? machineInfo, err := kubeDeps.CAdvisorInterface.MachineInfo() if err != nil { // if can't use CAdvisor here, fall back to non-explicit cpu list behavor klog.Warning(\"Failed to get MachineInfo, set reservedSystemCPUs to empty\") reservedSystemCPUs = cpuset.NewCPUSet() } else { reservedList := reservedSystemCPUs.ToSlice() first := reservedList[0] last := reservedList[len(reservedList)-1] if first = machineInfo.NumCores { // the specified cpuset is outside of the range of what the machine has klog.Infof(\"Invalid cpuset specified by --reserved-cpus\") return fmt.Errorf(\"Invalid cpuset %q specified by --reserved-cpus\", s.ReservedSystemCPUs) } } } else { reservedSystemCPUs = cpuset.NewCPUSet() } if reservedSystemCPUs.Size() > 0 { // at cmd option valication phase it is tested either --system-reserved-cgroup or --kube-reserved-cgroup is specified, so overwrite should be ok klog.Infof(\"Option --reserved-cpus is specified, it will overwrite the cpu setting in KubeReserved=\\\"%v\\\", SystemReserved=\\\"%v\\\".\", s.KubeReserved, s.SystemReserved) if s.KubeReserved != nil { delete(s.KubeReserved, \"cpu\") } if s.SystemReserved == nil { s.SystemReserved = make(map[string]string) } s.SystemReserved[\"cpu\"] = strconv.Itoa(reservedSystemCPUs.Size()) klog.Infof(\"After cpu setting is overwritten, KubeReserved=\\\"%v\\\", SystemReserved=\\\"%v\\\"\", s.KubeReserved, s.SystemReserved) } kubeReserved, err := parseResourceList(s.KubeReserved) if err != nil { return err } systemReserved, err := parseResourceList(s.SystemReserved) if err != nil { return err } var hardEvictionThresholds []evictionapi.Threshold // If the user requested to ignore eviction thresholds, then do not set valid values for hardEvictionThresholds here. if !s.ExperimentalNodeAllocatableIgnoreEvictionThreshold { hardEvictionThresholds, err = eviction.ParseThresholdConfig([]string{}, s.EvictionHard, nil, nil, nil) if err != nil { return err } } experimentalQOSReserved, err := cm.ParseQOSReserved(s.QOSReserved) if err != nil { return err } devicePluginEnabled := utilfeature.DefaultFeatureGate.Enabled(features.DevicePlugins) kubeDeps.ContainerManager, err = cm.NewContainerManager( kubeDeps.Mounter, kubeDeps.CAdvisorInterface, cm.NodeConfig{ RuntimeCgroupsName: s.RuntimeCgroups, SystemCgroupsName: s.SystemCgroups, KubeletCgroupsName: s.KubeletCgroups, ContainerRuntime: s.ContainerRuntime, CgroupsPerQOS: s.CgroupsPerQOS, CgroupRoot: s.CgroupRoot, CgroupDriver: s.CgroupDriver, KubeletRootDir: s.RootDirectory, ProtectKernelDefaults: s.ProtectKernelDefaults, NodeAllocatableConfig: cm.NodeAllocatableConfig{ KubeReservedCgroupName: s.KubeReservedCgroup, SystemReservedCgroupName: s.SystemReservedCgroup, EnforceNodeAllocatable: sets.NewString(s.EnforceNodeAllocatable...), KubeReserved: kubeReserved, SystemReserved: systemReserved, ReservedSystemCPUs: reservedSystemCPUs, HardEvictionThresholds: hardEvictionThresholds, }, QOSReserved: *experimentalQOSReserved, ExperimentalCPUManagerPolicy: s.CPUManagerPolicy, ExperimentalCPUManagerReconcilePeriod: s.CPUManagerReconcilePeriod.Duration, ExperimentalPodPidsLimit: s.PodPidsLimit, EnforceCPULimits: s.CPUCFSQuota, CPUCFSQuotaPeriod: s.CPUCFSQuotaPeriod.Duration, ExperimentalTopologyManagerPolicy: s.TopologyManagerPolicy, }, s.FailSwapOn, devicePluginEnabled, kubeDeps.Recorder) if err != nil { return err } } if err := checkPermissions(); err != nil { klog.Error(err) } utilruntime.ReallyCrash = s.ReallyCrashForTesting // TODO(vmarmol): Do this through container config. oomAdjuster := kubeDeps.OOMAdjuster if err := oomAdjuster.ApplyOOMScoreAdj(0, int(s.OOMScoreAdj)); err != nil { klog.Warning(err) } err = kubelet.PreInitRuntimeService(&s.KubeletConfiguration, kubeDeps, &s.ContainerRuntimeOptions, s.ContainerRuntime, s.RuntimeCgroups, s.RemoteRuntimeEndpoint, s.RemoteImageEndpoint, s.NonMasqueradeCIDR) if err != nil { return err } if err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil { return err } // If the kubelet config controller is available, and dynamic config is enabled, start the config and status sync loops if utilfeature.DefaultFeatureGate.Enabled(features.DynamicKubeletConfig) && len(s.DynamicConfigDir.Value()) > 0 && kubeDeps.KubeletConfigController != nil && !standaloneMode && !s.RunOnce { if err := kubeDeps.KubeletConfigController.StartSync(kubeDeps.KubeClient, kubeDeps.EventClient, string(nodeName)); err != nil { return err } } if s.HealthzPort > 0 { mux := http.NewServeMux() healthz.InstallHandler(mux) go wait.Until(func() { err := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), mux) if err != nil { klog.Errorf(\"Starting healthz server failed: %v\", err) } }, 5*time.Second, wait.NeverStop) } if s.RunOnce { return nil } // If systemd is used, notify it that we have started go daemon.SdNotify(false, \"READY=1\") select { case 代码量较大，我们按步骤分析 1.设置全局门控特性 err = utilfeature.DefaultMutableFeatureGate.SetFromMap(s.KubeletConfiguration.FeatureGates) if err != nil { return err } 2.kubelet参数合法性检测 if err := options.ValidateKubeletServer(s); err != nil { return err } 检测内容：配置标识及门控特性 3.注册当前配置至/configz端点 err = initConfigz(&s.KubeletConfiguration) if err != nil { klog.Errorf(\"unable to register KubeletConfiguration with configz, error: %v\", err) } 4.检查kubelet启动模式是否为standalone模式 此模式下不会和api-server交互，主要用于kubelet的调试 standaloneMode := true if len(s.KubeConfig) > 0 { standaloneMode = false } 5.检测kubeDeps是否为空，为空则初始化 前文我们讲到，执行Run函数前已经初始化kubeDeps，kubeDeps是一个与运行时各种资源（网络、卷、容器运行时等）交互的接口集合对象。 if kubeDeps == nil { kubeDeps, err = UnsecuredDependencies(s, featureGate) if err != nil { return err } } if kubeDeps.Cloud == nil { if !cloudprovider.IsExternal(s.CloudProvider) { cloud, err := cloudprovider.InitCloudProvider(s.CloudProvider, s.CloudConfigFile) if err != nil { return err } if cloud == nil { klog.V(2).Infof(\"No cloud provider specified: %q from the config file: %q\\n\", s.CloudProvider, s.CloudConfigFile) } else { klog.V(2).Infof(\"Successfully initialized cloud provider: %q from the config file: %q\\n\", s.CloudProvider, s.CloudConfigFile) } kubeDeps.Cloud = cloud } } 6.获取主机名称 用于后续初始化事件记录器 如果指定--cloud-provider，获取云主机节点名称。 如果未指定--cloud-provider，并且指定了--hostname-override，返回--hostname-override值作为主机名 如果未指定--cloud-provider与--hostname-override，返回节点hostnamehostName, err := nodeutil.GetHostname(s.HostnameOverride) if err != nil { return err } nodeName, err := getNodeName(kubeDeps.Cloud, hostName) if err != nil { return err } 7.standalone模式下关闭所有客户端连接 switch { case standaloneMode: kubeDeps.KubeClient = nil kubeDeps.EventClient = nil kubeDeps.HeartbeatClient = nil klog.Warningf(\"standalone mode, no API client\") case kubeDeps.KubeClient == nil, kubeDeps.EventClient == nil, kubeDeps.HeartbeatClient == nil: clientConfig, closeAllConns, err := buildKubeletClientConfig(s, nodeName) if err != nil { return err } if closeAllConns == nil { return errors.New(\"closeAllConns must be a valid function other than nil\") } kubeDeps.OnHeartbeatFailure = closeAllConns kubeDeps.KubeClient, err = clientset.NewForConfig(clientConfig) if err != nil { return fmt.Errorf(\"failed to initialize kubelet client: %v\", err) } // make a separate client for events eventClientConfig := *clientConfig eventClientConfig.QPS = float32(s.EventRecordQPS) eventClientConfig.Burst = int(s.EventBurst) kubeDeps.EventClient, err = v1core.NewForConfig(&eventClientConfig) if err != nil { return fmt.Errorf(\"failed to initialize kubelet event client: %v\", err) } // make a separate client for heartbeat with throttling disabled and a timeout attached heartbeatClientConfig := *clientConfig heartbeatClientConfig.Timeout = s.KubeletConfiguration.NodeStatusUpdateFrequency.Duration // The timeout is the minimum of the lease duration and status update frequency leaseTimeout := time.Duration(s.KubeletConfiguration.NodeLeaseDurationSeconds) * time.Second if heartbeatClientConfig.Timeout > leaseTimeout { heartbeatClientConfig.Timeout = leaseTimeout } heartbeatClientConfig.QPS = float32(-1) kubeDeps.HeartbeatClient, err = clientset.NewForConfig(&heartbeatClientConfig) if err != nil { return fmt.Errorf(\"failed to initialize kubelet heartbeat client: %v\", err) } } 8.初始化身份认证接口 BuildAuth创建一个身份验证器、一个授权器，以及一个与kubelet需要兼容的匹配的授权器属性getter. 它返回一个AuthInterface认证接口，一个运行方法来启动内部控制器(如重新加载证书)和错误。 if kubeDeps.Auth == nil { auth, runAuthenticatorCAReload, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration) if err != nil { return err } kubeDeps.Auth = auth runAuthenticatorCAReload(stopCh) } 9.初始化cgroups 包含如下： kubelet cgroups 容器运行时cgroups 系统cgroups var cgroupRoots []string cgroupRoots = append(cgroupRoots, cm.NodeAllocatableRoot(s.CgroupRoot, s.CgroupDriver)) kubeletCgroup, err := cm.GetKubeletContainer(s.KubeletCgroups) if err != nil { klog.Warningf(\"failed to get the kubelet's cgroup: %v. Kubelet system container metrics may be missing.\", err) } else if kubeletCgroup != \"\" { cgroupRoots = append(cgroupRoots, kubeletCgroup) } runtimeCgroup, err := cm.GetRuntimeContainer(s.ContainerRuntime, s.RuntimeCgroups) if err != nil { klog.Warningf(\"failed to get the container runtime's cgroup: %v. Runtime system container metrics may be missing.\", err) } else if runtimeCgroup != \"\" { // RuntimeCgroups is optional, so ignore if it isn't specified cgroupRoots = append(cgroupRoots, runtimeCgroup) } if s.SystemCgroups != \"\" { // SystemCgroups is optional, so ignore if it isn't specified cgroupRoots = append(cgroupRoots, s.SystemCgroups) } 10.初始化cAdvisor docker容器运行时内置cAdvisor获取容器指标数据 if kubeDeps.CAdvisorInterface == nil { imageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint) kubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cgroupRoots, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint)) if err != nil { return err } } 11.初始化事件记录器，用于向kubelet端事件 // Setup event recorder if required. makeEventRecorder(kubeDeps, nodeName) 事件格式如下： $ kubectl describe pod -n ddd portal-f6f4b4486-grhb7 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Unhealthy 7m49s (x22310 over 7d3h) kubelet, node1 Liveness probe failed: Get http://10.233.90.203:7002/actuator/health: dial tcp 10.233.90.203:7002: connect: connection refused Warning BackOff 2m49s (x27215 over 7d3h) kubelet, node1 Back-off restarting failed container 12.初始化容器管理器 容器管理器主要用来管理容器： 1.如果开启--cgroups-per-qos，并且--cgroup-root未指定，cgroups的根为/。 即启用基于QoS的Cgroup层次结构，所有的Burstable和BestEffort类型pod都在它们特定的顶级QoS cgroup之下。 如： $ ls /sys/fs/cgroup/cpu/kubepods.slice cgroup.clone_children cpuacct.usage_percpu_sys cpu.rt_period_us kubepods-pod347e1023_78aa_4aa6_a1bb_c11e60e995e1.slice cgroup.procs cpuacct.usage_percpu_user cpu.rt_runtime_us kubepods-podafe0da25_4a42_4a71_82c8_afcd7faf3b52.slice cpuacct.stat cpuacct.usage_sys cpu.shares kubepods-pode61df7e6_b184_4c86_bd1e_734c818a4a1f.slice cpuacct.usage cpuacct.usage_user cpu.stat notify_on_release cpuacct.usage_all cpu.cfs_period_us kubepods-besteffort.slice tasks cpuacct.usage_percpu cpu.cfs_quota_us kubepods-burstable.slice 2.--reserved-cpus如果非空，初始化系统CPU预留资源。 当--reserved-cpus被设置时，--system-reserved与--kube-reserved将无效化。初始化逻辑如下： a. 检测--reserved-cpus值合法性，如果非法则置空，避免--system-reserved与--kube-reserved无效化 b. 检测是否可以从CAdvisor中获取主机信息，如果获取不了则置空--reserved-cpus值 c. 检测--reserved-cpus值是否在宿主机CPU核数有效区间，非法则返回异常（如宿主机8核，指令预留12核，大于宿主机CPU实际核数） d. 解析赋值容器管理器其他字段: kubeReserved, err := parseResourceList(s.KubeReserved) if err != nil { return err } systemReserved, err := parseResourceList(s.SystemReserved) if err != nil { return err } var hardEvictionThresholds []evictionapi.Threshold // If the user requested to ignore eviction thresholds, then do not set valid values for hardEvictionThresholds here. if !s.ExperimentalNodeAllocatableIgnoreEvictionThreshold { hardEvictionThresholds, err = eviction.ParseThresholdConfig([]string{}, s.EvictionHard, nil, nil, nil) if err != nil { return err } } experimentalQOSReserved, err := cm.ParseQOSReserved(s.QOSReserved) if err != nil { return err } devicePluginEnabled := utilfeature.DefaultFeatureGate.Enabled(features.DevicePlugins) kubeDeps.ContainerManager, err = cm.NewContainerManager( kubeDeps.Mounter, kubeDeps.CAdvisorInterface, cm.NodeConfig{ RuntimeCgroupsName: s.RuntimeCgroups, SystemCgroupsName: s.SystemCgroups, KubeletCgroupsName: s.KubeletCgroups, ContainerRuntime: s.ContainerRuntime, CgroupsPerQOS: s.CgroupsPerQOS, CgroupRoot: s.CgroupRoot, CgroupDriver: s.CgroupDriver, KubeletRootDir: s.RootDirectory, ProtectKernelDefaults: s.ProtectKernelDefaults, NodeAllocatableConfig: cm.NodeAllocatableConfig{ KubeReservedCgroupName: s.KubeReservedCgroup, SystemReservedCgroupName: s.SystemReservedCgroup, EnforceNodeAllocatable: sets.NewString(s.EnforceNodeAllocatable...), KubeReserved: kubeReserved, SystemReserved: systemReserved, ReservedSystemCPUs: reservedSystemCPUs, HardEvictionThresholds: hardEvictionThresholds, }, QOSReserved: *experimentalQOSReserved, ExperimentalCPUManagerPolicy: s.CPUManagerPolicy, ExperimentalCPUManagerReconcilePeriod: s.CPUManagerReconcilePeriod.Duration, ExperimentalPodPidsLimit: s.PodPidsLimit, EnforceCPULimits: s.CPUCFSQuota, CPUCFSQuotaPeriod: s.CPUCFSQuotaPeriod.Duration, ExperimentalTopologyManagerPolicy: s.TopologyManagerPolicy, }, s.FailSwapOn, devicePluginEnabled, kubeDeps.Recorder) 容器控制器初始化部分源码： if kubeDeps.ContainerManager == nil { if s.CgroupsPerQOS && s.CgroupRoot == \"\" { klog.Info(\"--cgroups-per-qos enabled, but --cgroup-root was not specified. defaulting to /\") s.CgroupRoot = \"/\" } var reservedSystemCPUs cpuset.CPUSet var errParse error if s.ReservedSystemCPUs != \"\" { reservedSystemCPUs, errParse = cpuset.Parse(s.ReservedSystemCPUs) if errParse != nil { // invalid cpu list is provided, set reservedSystemCPUs to empty, so it won't overwrite kubeReserved/systemReserved klog.Infof(\"Invalid ReservedSystemCPUs \\\"%s\\\"\", s.ReservedSystemCPUs) return errParse } // is it safe do use CAdvisor here ?? machineInfo, err := kubeDeps.CAdvisorInterface.MachineInfo() if err != nil { // if can't use CAdvisor here, fall back to non-explicit cpu list behavor klog.Warning(\"Failed to get MachineInfo, set reservedSystemCPUs to empty\") reservedSystemCPUs = cpuset.NewCPUSet() } else { reservedList := reservedSystemCPUs.ToSlice() first := reservedList[0] last := reservedList[len(reservedList)-1] if first = machineInfo.NumCores { // the specified cpuset is outside of the range of what the machine has klog.Infof(\"Invalid cpuset specified by --reserved-cpus\") return fmt.Errorf(\"Invalid cpuset %q specified by --reserved-cpus\", s.ReservedSystemCPUs) } } } else { reservedSystemCPUs = cpuset.NewCPUSet() } if reservedSystemCPUs.Size() > 0 { // at cmd option valication phase it is tested either --system-reserved-cgroup or --kube-reserved-cgroup is specified, so overwrite should be ok klog.Infof(\"Option --reserved-cpus is specified, it will overwrite the cpu setting in KubeReserved=\\\"%v\\\", SystemReserved=\\\"%v\\\".\", s.KubeReserved, s.SystemReserved) if s.KubeReserved != nil { delete(s.KubeReserved, \"cpu\") } if s.SystemReserved == nil { s.SystemReserved = make(map[string]string) } s.SystemReserved[\"cpu\"] = strconv.Itoa(reservedSystemCPUs.Size()) klog.Infof(\"After cpu setting is overwritten, KubeReserved=\\\"%v\\\", SystemReserved=\\\"%v\\\"\", s.KubeReserved, s.SystemReserved) } kubeReserved, err := parseResourceList(s.KubeReserved) if err != nil { return err } systemReserved, err := parseResourceList(s.SystemReserved) if err != nil { return err } var hardEvictionThresholds []evictionapi.Threshold // If the user requested to ignore eviction thresholds, then do not set valid values for hardEvictionThresholds here. if !s.ExperimentalNodeAllocatableIgnoreEvictionThreshold { hardEvictionThresholds, err = eviction.ParseThresholdConfig([]string{}, s.EvictionHard, nil, nil, nil) if err != nil { return err } } experimentalQOSReserved, err := cm.ParseQOSReserved(s.QOSReserved) if err != nil { return err } devicePluginEnabled := utilfeature.DefaultFeatureGate.Enabled(features.DevicePlugins) kubeDeps.ContainerManager, err = cm.NewContainerManager( kubeDeps.Mounter, kubeDeps.CAdvisorInterface, cm.NodeConfig{ RuntimeCgroupsName: s.RuntimeCgroups, SystemCgroupsName: s.SystemCgroups, KubeletCgroupsName: s.KubeletCgroups, ContainerRuntime: s.ContainerRuntime, CgroupsPerQOS: s.CgroupsPerQOS, CgroupRoot: s.CgroupRoot, CgroupDriver: s.CgroupDriver, KubeletRootDir: s.RootDirectory, ProtectKernelDefaults: s.ProtectKernelDefaults, NodeAllocatableConfig: cm.NodeAllocatableConfig{ KubeReservedCgroupName: s.KubeReservedCgroup, SystemReservedCgroupName: s.SystemReservedCgroup, EnforceNodeAllocatable: sets.NewString(s.EnforceNodeAllocatable...), KubeReserved: kubeReserved, SystemReserved: systemReserved, ReservedSystemCPUs: reservedSystemCPUs, HardEvictionThresholds: hardEvictionThresholds, }, QOSReserved: *experimentalQOSReserved, ExperimentalCPUManagerPolicy: s.CPUManagerPolicy, ExperimentalCPUManagerReconcilePeriod: s.CPUManagerReconcilePeriod.Duration, ExperimentalPodPidsLimit: s.PodPidsLimit, EnforceCPULimits: s.CPUCFSQuota, CPUCFSQuotaPeriod: s.CPUCFSQuotaPeriod.Duration, ExperimentalTopologyManagerPolicy: s.TopologyManagerPolicy, }, s.FailSwapOn, devicePluginEnabled, kubeDeps.Recorder) if err != nil { return err } } 13.检测是否以root用户运行kubelet 如果非root用户则返回异常。 if err := checkPermissions(); err != nil { klog.Error(err) } ... func checkPermissions() error { if uid := os.Getuid(); uid != 0 { return fmt.Errorf(\"kubelet needs to run as uid `0`. It is being run as %d\", uid) } // TODO: Check if kubelet is running in the `initial` user namespace. // http://man7.org/linux/man-pages/man7/user_namespaces.7.html return nil } 14.为kubelet进程设置OOM分数 即设置为--oom-score-adj的值，可选区间为[-1000, 1000]，默认值为-999，并且该值越小越不容易被kill掉。 oomAdjuster := kubeDeps.OOMAdjuster if err := oomAdjuster.ApplyOOMScoreAdj(0, int(s.OOMScoreAdj)); err != nil { klog.Warning(err) } 15.容器运行时初始化 当容器运行时为docker时，初始化以下内容： 网络插件名称（一般为cni） CIDR CNI插件配置、缓存、二进制文件目录 MTU 网桥模式 创建启动CRI shim进程，作为连接kubelet与容器运行时间的桥梁 设置是否使用cAdvisor采集容器指标数据 16.启动kubelet if err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil { return err } 启动流程主要如下： a. 初始化事件记录器 hostname, err := nodeutil.GetHostname(kubeServer.HostnameOverride) if err != nil { return err } // Query the cloud provider for our node name, default to hostname if kubeDeps.Cloud == nil nodeName, err := getNodeName(kubeDeps.Cloud, hostname) if err != nil { return err } // Setup event recorder if required. makeEventRecorder(kubeDeps, nodeName) b. kubelet进程开启所有Linux CAP capabilities.Initialize(capabilities.Capabilities{ AllowPrivileged: true, }) c. 初始化kubelet操作操作系统接口方法 if kubeDeps.OSInterface == nil { kubeDeps.OSInterface = kubecontainer.RealOS{} } 接口如下： type OSInterface interface { MkdirAll(path string, perm os.FileMode) error Symlink(oldname string, newname string) error Stat(path string) (os.FileInfo, error) Remove(path string) error RemoveAll(path string) error Create(path string) (*os.File, error) Chmod(path string, perm os.FileMode) error Hostname() (name string, err error) Chtimes(path string, atime time.Time, mtime time.Time) error Pipe() (r *os.File, w *os.File, err error) ReadDir(dirname string) ([]os.FileInfo, error) Glob(pattern string) ([]string, error) Open(name string) (*os.File, error) OpenFile(name string, flag int, perm os.FileMode) (*os.File, error) Rename(oldpath, newpath string) error } d. 创建初始化kubelet服务 初始化逻辑后续我们深入探讨 k, err := createAndInitKubelet(&kubeServer.KubeletConfiguration, kubeDeps, &kubeServer.ContainerRuntimeOptions, kubeServer.ContainerRuntime, kubeServer.HostnameOverride, kubeServer.NodeIP, kubeServer.ProviderID, kubeServer.CloudProvider, kubeServer.CertDirectory, kubeServer.RootDirectory, kubeServer.RegisterNode, kubeServer.RegisterWithTaints, kubeServer.AllowedUnsafeSysctls, kubeServer.ExperimentalMounterPath, kubeServer.ExperimentalKernelMemcgNotification, kubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount, kubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold, kubeServer.MinimumGCAge, kubeServer.MaxPerPodContainerCount, kubeServer.MaxContainerCount, kubeServer.MasterServiceNamespace, kubeServer.RegisterSchedulable, kubeServer.KeepTerminatedPodVolumes, kubeServer.NodeLabels, kubeServer.SeccompProfileRoot, kubeServer.BootstrapCheckpointPath, kubeServer.NodeStatusMaxImages) if err != nil { return fmt.Errorf(\"failed to create kubelet: %v\", err) } // NewMainKubelet should have set up a pod source config if one didn't exist // when the builder was run. This is just a precaution. if kubeDeps.PodConfig == nil { return fmt.Errorf(\"failed to create kubelet, pod source config was nil\") } podCfg := kubeDeps.PodConfig e. 设置kubelet进程最大文件打开数 rlimit.RlimitNumFiles(uint64(kubeServer.MaxOpenFiles)) f. 启动kubelet服务 // process pods and exit. if runOnce { if _, err := k.RunOnce(podCfg.Updates()); err != nil { return fmt.Errorf(\"runonce failed: %v\", err) } klog.Info(\"Started kubelet as runonce\") } else { startKubelet(k, podCfg, &kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableCAdvisorJSONEndpoints, kubeServer.EnableServer) klog.Info(\"Started kubelet\") } 17.如果开启动态配置，则监听动态配置中的配置变化 // If the kubelet config controller is available, and dynamic config is enabled, start the config and status sync loops if utilfeature.DefaultFeatureGate.Enabled(features.DynamicKubeletConfig) && len(s.DynamicConfigDir.Value()) > 0 && kubeDeps.KubeletConfigController != nil && !standaloneMode && !s.RunOnce { if err := kubeDeps.KubeletConfigController.StartSync(kubeDeps.KubeClient, kubeDeps.EventClient, string(nodeName)); err != nil { return err } } 18.开启/healthz端点 if s.HealthzPort > 0 { mux := http.NewServeMux() healthz.InstallHandler(mux) go wait.Until(func() { err := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), mux) if err != nil { klog.Errorf(\"Starting healthz server failed: %v\", err) } }, 5*time.Second, wait.NeverStop) } 19.通知init进程kubelet服务启动完毕 if s.RunOnce { return nil } // If systemd is used, notify it that we have started go daemon.SdNotify(false, \"READY=1\") Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/06kubelet启动过程的关键流程解析.html":{"url":"2.容器/k8s/core/kubelet/启动/06kubelet启动过程的关键流程解析.html","title":"06kubelet启动过程的关键流程解析","keywords":"","body":"初始化运行时服务 初始化运行时服务调用: err = kubelet.PreInitRuntimeService(&s.KubeletConfiguration, kubeDeps, &s.ContainerRuntimeOptions, s.ContainerRuntime, s.RuntimeCgroups, s.RemoteRuntimeEndpoint, s.RemoteImageEndpoint, s.NonMasqueradeCIDR) if err != nil { return err } PreInitRuntimeService函数解析 func PreInitRuntimeService(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, crOptions *config.ContainerRuntimeOptions, containerRuntime string, runtimeCgroups string, remoteRuntimeEndpoint string, remoteImageEndpoint string, nonMasqueradeCIDR string) error { if remoteRuntimeEndpoint != \"\" { // remoteImageEndpoint is same as remoteRuntimeEndpoint if not explicitly specified if remoteImageEndpoint == \"\" { remoteImageEndpoint = remoteRuntimeEndpoint } } switch containerRuntime { case kubetypes.DockerContainerRuntime: // TODO: These need to become arguments to a standalone docker shim. pluginSettings := dockershim.NetworkPluginSettings{ HairpinMode: kubeletconfiginternal.HairpinMode(kubeCfg.HairpinMode), NonMasqueradeCIDR: nonMasqueradeCIDR, PluginName: crOptions.NetworkPluginName, PluginConfDir: crOptions.CNIConfDir, PluginBinDirString: crOptions.CNIBinDir, PluginCacheDir: crOptions.CNICacheDir, MTU: int(crOptions.NetworkPluginMTU), } // Create and start the CRI shim running as a grpc server. streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, &pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming) if err != nil { return err } if crOptions.RedirectContainerStreaming { kubeDeps.criHandler = ds } // The unix socket for kubelet dockershim communication, dockershim start before runtime service init. klog.V(5).Infof(\"RemoteRuntimeEndpoint: %q, RemoteImageEndpoint: %q\", remoteRuntimeEndpoint, remoteImageEndpoint) klog.V(2).Infof(\"Starting the GRPC server for the docker CRI shim.\") dockerServer := dockerremote.NewDockerServer(remoteRuntimeEndpoint, ds) if err := dockerServer.Start(); err != nil { return err } // Create dockerLegacyService when the logging driver is not supported. supported, err := ds.IsCRISupportedLogDriver() if err != nil { return err } if !supported { kubeDeps.dockerLegacyService = ds } case kubetypes.RemoteContainerRuntime: // No-op. break default: return fmt.Errorf(\"unsupported CRI runtime: %q\", containerRuntime) } var err error if kubeDeps.RemoteRuntimeService, err = remote.NewRemoteRuntimeService(remoteRuntimeEndpoint, kubeCfg.RuntimeRequestTimeout.Duration); err != nil { return err } if kubeDeps.RemoteImageService, err = remote.NewRemoteImageService(remoteImageEndpoint, kubeCfg.RuntimeRequestTimeout.Duration); err != nil { return err } kubeDeps.useLegacyCadvisorStats = cadvisor.UsingLegacyCadvisorStats(containerRuntime, remoteRuntimeEndpoint) return nil } 主要做了以下几件事： 初始化网络插件 创建并启动CRI shim作为grpc服务端运行 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/07启动流程内一些概念解析.html":{"url":"2.容器/k8s/core/kubelet/启动/07启动流程内一些概念解析.html","title":"07启动流程内一些概念解析","keywords":"","body":" Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/启动/cAdvisor.html":{"url":"2.容器/k8s/core/kubelet/启动/cAdvisor.html","title":"cAdvisor","keywords":"","body":"cAdvisor cAdvisor 是一个容器资源使用和性能分析的开源项目 它是专门为容器构建的，并且原生支持Docker容器。在Kubernetes中，cadvisor被集成到Kubelet二进制中。 cAdvisor自动发现机器中的所有容器，并收集CPU、内存、文件系统和网络使用统计信息。 Kubelet充当了Kubernetes控制节点和工作节点之间的桥梁。 它管理机器上运行的pod和容器。Kubelet将每个pod转换为容器组，并从cAdvisor获取单个容器的使用统计信息。 然后通过REST API发布聚合的pod资源使用统计信息。 可以浏览以下优秀文章，了解更多关于cAdvisor Resource Usage Monitoring in Kubernetes 容器监控实践—cAdvisor Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/垃圾回收/01镜像垃圾回收.html":{"url":"2.容器/k8s/core/kubelet/垃圾回收/01镜像垃圾回收.html","title":"01镜像垃圾回收","keywords":"","body":"镜像垃圾回收 关联启动配置/标识 镜像垃圾回收机制有两个关联参数： --image-gc-high-threshold: 磁盘使用百分比，当磁盘使用占比（100 * 已用/总量）大于等于该值，会启动镜像垃圾回收流程。 值必须在[0,100]范围内，要禁用镜像垃圾收集，设置为100。【默认值85】 --image-gc-low-threshold: 磁盘使用百分比，当垃圾回收流程启动后， 该值与--image-gc-high-threshold通过算术关系控制垃圾回收的空间大小。 该值取值范围为[0,100]，且不应大于--image-gc-high-threshold。【默认值80】 关于这两个参数的作用，举个栗子 假设: 镜像文件系统的磁盘总容量为100G，--image-gc-high-threshold、--image-gc-low-threshold默认值 镜像文件系统的磁盘已用容量为90G时，此时镜像文件系统的磁盘使用率为100% * 90G/100G = 90% > --image-gc-high-threshold（85%）。 此时将触发镜像垃圾回收，具体回收流程后续讨论。这里会计算出一个垃圾回收需要释放的空间大小amountToFree: amountToFree = 磁盘总量 * (100-`--image-gc-low-threshold`值)/100 - 磁盘可用大小 带入值进行计算需要释放出的空间大小为 amountToFree = 100G * (100-80)/100 - (100-90) amountToFree = 10G 通过上面的分析我们发现，其实kubelet自带的垃圾回收存在一定的利弊： 利: 周期性回收镜像，避免因镜像文件写满磁盘分区导致灾难性事故（例如：当存放镜像的分区为系统/分区） 弊: 只会按比例执行镜像清理，并不会完全清理掉某些无用的镜像（这些镜像会一直存在，直到后续垃圾回收流程触发，才可能被清理掉） 同一时间大批量删除镜像，将导致IO飙升 尽管kubelet自带镜像垃圾回收功能，但并不能完全清理掉所有无用镜像，从而导致过多冗余数据占用系统磁盘空间。所以存放镜像的分区最好单独指定。 如docker配置: \"data-root\": \"/data\" 流程解析 kubelet垃圾回收模块会周期性（每五分钟）对宿主机上的镜像执行垃圾回收。回收流程主要如下： 调用运行时接口，获取存放镜像的文件系统信息，主要获取两个值: 文件系统磁盘总容量 文件系统磁盘可用容量 计算磁盘使用率使用到达垃圾回收阈值（--image-gc-high-threshold），如果到达阈值启动镜像垃圾回收流程。 启动垃圾回收流程后，首先计算出一个要释放出空间大小的值 kubelet对本地镜像进行排序，找到未被容器使用的镜像，调用运行时接口对其释放。并且该镜像--minimum-image-ttl-duration的 垃圾回收流程源码实现 kubernetes\\pkg\\kubelet\\images\\image_gc_manager.go func (im *realImageGCManager) GarbageCollect() error { // Get disk usage on disk holding images. // 调用运行时获取存放镜像的文件系统状态： fsStats, err := im.statsProvider.ImageFsStats() if err != nil { return err } var capacity, available int64 if fsStats.CapacityBytes != nil { capacity = int64(*fsStats.CapacityBytes) } if fsStats.AvailableBytes != nil { available = int64(*fsStats.AvailableBytes) } if available > capacity { klog.Warningf(\"available %d is larger than capacity %d\", available, capacity) available = capacity } // Check valid capacity. if capacity == 0 { err := goerrors.New(\"invalid capacity 0 on image filesystem\") im.recorder.Eventf(im.nodeRef, v1.EventTypeWarning, events.InvalidDiskCapacity, err.Error()) return err } // If over the max threshold, free enough to place us at the lower threshold. usagePercent := 100 - int(available*100/capacity) // available=10G capacity=100G HighThresholdPercent=85% LowThresholdPercent=80% if usagePercent >= im.policy.HighThresholdPercent { // amountToFree=5G amountToFree := capacity*int64(100-im.policy.LowThresholdPercent)/100 - available klog.Infof(\"[imageGCManager]: Disk usage on image filesystem is at %d%% which is over the high threshold (%d%%). Trying to free %d bytes down to the low threshold (%d%%).\", usagePercent, im.policy.HighThresholdPercent, amountToFree, im.policy.LowThresholdPercent) freed, err := im.freeSpace(amountToFree, time.Now()) if err != nil { return err } // 判断释放的容量与期望释放的容量 if freed 关于docker运行时获取镜像文件系统信息 由于源码中涉及一些额外的概念（如文件系统唯一标识等），增加了理解负担。这里我们通过代入的方式进行讨论： 我们假设docker信息如下： docker根目录为: /data /data由/dev/sdb挂载，/dev/sdb磁盘容量大小为100G 那么获取镜像文件系统相关参数（主要为：总容量、已用容量）主要流程如下： 首先kubelet调用docker的/info接口（类似docker info） 解析上步返回值，获取docker根目录(/data) 获取存放镜像目录：docker根目录/imgae(如：/data/image) 递归计算镜像目录下文件总大小，即镜像文件系统已用空间大小（类似: du -sh /data/image） 调用系统接口，获取/data挂载点总容量，即镜像文件系统总容量（100G） 获取镜像文件系统使用信息 kubernetes\\pkg\\kubelet\\dockershim\\docker_image_linux.go func (ds *dockerService) ImageFsInfo(_ context.Context, _ *runtimeapi.ImageFsInfoRequest) (*runtimeapi.ImageFsInfoResponse, error) { info, err := ds.client.Info() if err != nil { klog.Errorf(\"Failed to get docker info: %v\", err) return nil, err } bytes, inodes, err := dirSize(filepath.Join(info.DockerRootDir, \"image\")) if err != nil { return nil, err } return &runtimeapi.ImageFsInfoResponse{ ImageFilesystems: []*runtimeapi.FilesystemUsage{ { Timestamp: time.Now().Unix(), FsId: &runtimeapi.FilesystemIdentifier{ Mountpoint: info.DockerRootDir, }, UsedBytes: &runtimeapi.UInt64Value{ Value: uint64(bytes), }, InodesUsed: &runtimeapi.UInt64Value{ Value: uint64(inodes), }, }, }, }, nil } 调用系统接口获取镜像文件系统的磁盘总容量 func (p *criStatsProvider) ImageFsStats() (*statsapi.FsStats, error) { resp, err := p.imageService.ImageFsInfo() if err != nil { return nil, err } // CRI may return the stats of multiple image filesystems but we only // return the first one. // // TODO(yguo0905): Support returning stats of multiple image filesystems. if len(resp) == 0 { return nil, fmt.Errorf(\"imageFs information is unavailable\") } fs := resp[0] s := &statsapi.FsStats{ Time: metav1.NewTime(time.Unix(0, fs.Timestamp)), UsedBytes: &fs.UsedBytes.Value, } if fs.InodesUsed != nil { s.InodesUsed = &fs.InodesUsed.Value } imageFsInfo := p.getFsInfo(fs.GetFsId()) if imageFsInfo != nil { // The image filesystem id is unknown to the local node or there's // an error on retrieving the stats. In these cases, we omit those // stats and return the best-effort partial result. See // https://github.com/kubernetes/heapster/issues/1793. s.AvailableBytes = &imageFsInfo.Available s.CapacityBytes = &imageFsInfo.Capacity s.InodesFree = imageFsInfo.InodesFree s.Inodes = imageFsInfo.Inodes } return s, nil } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/kubelet/感悟.html":{"url":"2.容器/k8s/core/kubelet/感悟.html","title":"感悟","keywords":"","body":" 了解Kubelet对容器日志的管理: 基于pod管理容器日志，同一Pod内的容器日志，以软链接的方式链接至同一目录下， 并发下的场景（如证书轮换）会加一定的抖动量，避免同一时间发送请求 var jitteryDuration = func(totalDuration float64) time.Duration { return wait.Jitter(time.Duration(totalDuration), 0.2) - time.Duration(totalDuration*0.3) } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/core/pause容器.html":{"url":"2.容器/k8s/core/pause容器.html","title":"pause容器","keywords":"","body":"(翻)pause容器解析 The Almighty Pause Container The Almighty Pause Container 前言 当您在Kubernetes集群的节点上执行，docker ps时，您会发现一些名为pause的容器正在运行: $ docker ps CONTAINER ID IMAGE COMMAND ... ... 3b45e983c859 gcr.io/google_containers/pause-amd64:3.0 \"/pause\" ... ... dbfc35b00062 gcr.io/google_containers/pause-amd64:3.0 \"/pause\" ... ... c4e998ec4d5d gcr.io/google_containers/pause-amd64:3.0 \"/pause\" ... ... 508102acf1e7 gcr.io/google_containers/pause-amd64:3.0 \"/pause\" ... 您可能很好奇：这些pause容器是什么? 为什么会有这么多这样的容器? 为了回答这些问题，您需要了解Kubernetes中的pod是如何实现的（尤其是基于常用的容器运行时：Docker/containerd）， 如果您对pod的实现原理不是很了解，请参考What are Kubernetes Pods Anyway? 我们都知道利用Docker启动运行单一进程的容器很简单， 然而当您想要同时运行多个软件组件时，这个模型可能会变得有点麻烦。 当开发人员创建Docker映像时，您经常会看到这种情况，这些映像使用entrypoint作为入口点来启动和管理多个进程。 对于生产系统，许多人发现将这些应用程序部署在部分隔离且部分共享环境的容器组中更为有用。 Kubernetes针对上述场景的需求，提供了一个称为pods的抽象。它屏蔽了Docker标志的复杂性（如启动个容器可能需要传递多个标识: docker run -itd --name ddd -v /etc/hosts:/etc/hosts nginx），以及管理容器、共享卷等操作。它还隐藏了容器运行时之间的差异，例如，rkt原生支持pod，因此Kubernetes要做的工作较少，但作为Kubernetes的用户，您不必担心这一点。 事实上，Docker原生就具备控制容器组之间的共享级别的能力——通过创建一个父容器，例如： 创建容器A，作为父容器，容器ID假设为A-ID 创建容器B，作为子容器，容器ID假设为B-ID，启动时指定PID命名空间标识为--pid=container:A-ID 创建容器C，作为子容器，容器ID假设为C-ID，启动时指定PID命名空间标识为--pid=container:A-ID 此时，容器B与容器C共享相同的PID命名空间(即容器A的PID命名空间) 通过上面的例子我们发现，使用原生的容器运行时实现起来比较繁琐，因为首先您得了解创建流程、所使用的标识，并管理这些容器的生命周期。 而在Kubernetes中，pause容器作为您pod中所有容器的父容器。 pause容器有两个核心职责： 首先，它作为在pod中共享Linux名称空间的基础容器。 其次，启用PID(进程ID)名称空间共享后，它将作为每个pod的PID 1进程（根进程），并回收僵尸进程。 接下来我们针对pause容器的职责逐一解析 关于共享命名空间 在Linux中，当运行一个新进程时，该进程从父进程继承其名称空间。 在新的命名空间中运行进程的方法是通过与父进程取消共享命名空间，从而创建一个新的命名空间。 下面是使用unshare工具在新的PID、UTS、IPC和mount名称空间中运行shell的示例。 $ sudo unshare --pid --uts --ipc --mount -f chroot rootfs /bin/sh 一旦进程运行，您可以将其他进程添加到进程的名称空间中，以形成一个pod。可以使用setns系统调用将新的进程添加到现有的命名空间中。 而pod的容器之间共享名称空间也是基于这个原理实现的。 Docker的实现则是将这个过程自动化一些，所以让我们看一个例子，看看如何通过使用pause容器和共享名称空间从头创建一个pod。 首先，我们需要使用Docker启动pause容器，并作端口映射，以便我们可以将容器添加到pod中。 $ docker run -d --name pause -p 8080:80 gcr.io/google_containers/pause-amd64:3.0 接下来为我们的pod容器，首先我们运行一个nginx容器，调整nginx的代理配置：监听80请求，并将请求转发至本地2368端口。 $ cat > nginx.conf > error_log stderr; > events { worker_connections 1024; } > http { > access_log /dev/stdout combined; > server { > listen 80 default_server; > server_name example.com www.example.com; > location / { > proxy_pass http://127.0.0.1:2368; > } > } > } > EOF $ docker run -d --name nginx -v `pwd`/nginx.conf:/etc/nginx/nginx.conf --net=container:pause --ipc=container:pause --pid=container:pause nginx 接下来创建一个ghost博客应用容器，作为服务端，端口监听为2368 $ docker run -d --name ghost --net=container:pause --ipc=container:pause --pid=container:pause ghost 通过上面的操作，我们将pause容器Network、PID、IPC命名空间共享给nginx与ghost容器，即三个容器共享相同Network、PID、IPC命名空间。 此时，当您访问http://localhost:8080/，实际被代理至ghost服务，流程如下: a. 容器宿主机访问http://localhost:8080/ b. 请求被转发至pause容器80端口，即nginx容器80端口 c. nginx将请求转发至本地2368端口，即ghost容器2368端口 显然，原生实现的流程还是比较复杂的（这还没有包括监控、管理这些容器生命周期） 关于回收僵尸进程 在Linux中，PID命名空间中的进程是一个树型结构，每个进程有一个父进程。在树的根上只有一个进程没有真正的父进程。这是init进程，其PID为1。 进程可以使用fork和exec系统调用来启动其他进程，此时新进程的父进程就是调用fork syscal的进程。 其中fork用于启动正在运行的进程的另一个副本，exec用于用一个新进程替换当前进程，保持相同的PID。 为了运行一个完全独立的应用程序，您需要运行fork和exec系统调用。一个进程使用fork用一个新的PID创建一个自己的新副本作为子进程，然后当子进程运行时，它检查它是否是子进程，并运行exec来用您真正想运行的进程替换它自己。 大多数语言都通过一个函数来实现这一点。 每个进程在系统进程表中都有一个条目，它记录进程状态和退出代码的信息。 当子进程完成运行后，它的进程表条目将一直保持到父进程使用wait系统调用检索其退出代码为止。这被称为回收僵尸进程。 什么是僵尸进程？ 僵尸进程是指已经停止运行但它们的进程表条目仍然存在的进程，因为父进程没有通过wait系统调用检索它。 从技术上讲，每个终止的进程在很短的一段时间内都是僵尸，但它们可以存活更长时间。 在UNIX系统中,一个子进程结束了,但是它的父进程没有等待(调用wait/waitpid)它, 那么它将变成一个僵尸进程. 孤儿进程 & 僵尸进程 孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所接管，并由init进程对它们完成状态收集工作。 僵尸进程：一个进程使用fork系统调用创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵尸进程。 僵尸进程是怎么产生的？ 出现僵尸进程的一种情况是： 父进程编写得很糟糕，省略了wait调用，或者父进程在子进程之前死亡，而新的父进程没有调用wait。 当一个进程的父进程在子进程之前死亡时，操作系统将该子进程分配给init进程或PID 1的进程。即init进程接纳子进程并成为其父进程。这意味着，现在当子进程退出时，新的父进程(init)必须调用wait来获取它的退出码，否则它的进程表条目将永远保留下来，成为僵死进程。 在容器中，应用运行的进程必须是init进程。在Docker中，每个容器通常都有自己的PID命名空间，ENTRYPOINT进程是init进程。当A容器在B容器的名称空间中运行时，B容器必须承担init进程的角色，而其A容器作为init进程的子进程添加到命名空间中。 $ docker run -d --name nginx -v `pwd`/nginx.conf:/etc/nginx/nginx.conf -p 8080:80 nginx $ docker run -d --name ghost --net=container:nginx --ipc=container:nginx --pid=container:nginx ghost 在这个例子中，nginx的角色是PID 1, ghost被添加为nginx的子进程。 当ghost自身分叉或使用exec运行子进程，并且ghost进程在ghost子进程完成之前崩溃，那么这些ghost孤儿子进程将被nginx进程接管。当这些孤儿进程完成退出时，它一直等待父进程（nginx进程）使用wait系统调用检索其退出代码。不幸的是nginx并没有被设计成能够作为一个init进程来运行并回收僵尸。 当我们存在很多这种容器组时，将可能导致很多容器内的僵尸进程无法回收。 僵尸进程的危害 僵尸进程会占用进程号，以及未回收的文件描述符占用空间，如果产生大量的僵尸进程，将会导致系统无法分配进程号 pod实现 在Kubernetes pod中，容器的运行方式与上述基本相同，但是为每个pod创建了一个特殊的pause容器。 这个pause容器运行了一个非常简单的进程，它不执行任何函数，本质上永远休眠(参见下面的pause()调用)。 其源码实现: /* Copyright 2016 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ #include #include #include #include #include #include static void sigdown(int signo) { psignal(signo, \"Shutting down, got signal\"); exit(0); } static void sigreap(int signo) { while (waitpid(-1, NULL, WNOHANG) > 0); } int main() { if (getpid() != 1) /* Not an error because pause sees use outside of infra containers. */ fprintf(stderr, \"Warning: pause should be the first process\\n\"); if (sigaction(SIGINT, &(struct sigaction){.sa_handler = sigdown}, NULL) 如你所见，它不仅仅处于休眠状态。它还有另外一个重要的功能。 从上述代码种我们发现，pause容器不仅仅调用pause()使进程休眠，还拥有另外一个重要的功能： 它假定自己为PID 1的角色，当僵尸进程被其父进程孤立时，通过调用wait来获取僵尸进程(见sigreap)。 这样一来就不会在Kubernetes pod的PID命名空间中堆积僵尸进程了。 关于进程命名空间共享说明 默认情况下，kubernetes同一pod内的容器不共享进程命名空间，需要指定配置。这意味着默认情况下，各个容器需要自己管理僵尸进程。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/faq/faq.html":{"url":"2.容器/k8s/faq/faq.html","title":"faq","keywords":"","body":"1.什么是K8s？ k8s是一套开源的容器编排系统，负责管理容器部署、扩缩容及负载均衡 2.Kubernetes与Docker有什么关系？ docker是一种容器运行时，基于命名空间与控制组技术实现进程级虚拟化。 k8s管理容器生命周期的编排系统。 kubelet PLEG模块 kubelet垃圾回收机制解析 每1分钟 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/faq/kubelet/垃圾回收.html":{"url":"2.容器/k8s/faq/kubelet/垃圾回收.html","title":"垃圾回收","keywords":"","body":"kubelet垃圾回收 kubelet每隔1分钟进行一次容器清理，每隔5分钟进行一次镜像清理 镜像回收流程 调用运行时接口，获取存放镜像的文件系统信息，主要获取两个值: 文件系统磁盘总容量 文件系统磁盘可用容量 计算磁盘使用率使用到达垃圾回收阈值（--image-gc-high-threshold），如果到达阈值启动镜像垃圾回收流程。 启动垃圾回收流程后，首先计算出一个要释放出空间大小的值 kubelet对本地镜像进行排序，找到未被容器使用的镜像，调用运行时接口对其释放。 直到磁盘使用率降到设定下限（LowThresholdPercent）或没有空闲镜像可以清理。 此外，在进行镜像清理时，会考虑镜像的生存年龄(默认两分钟)，对于年龄没有达到最短生存年龄（MinAge）要求的镜像，暂不予以清理。 K8s Kubelet 垃圾回收机制 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/helm/helm.html":{"url":"2.容器/k8s/helm/helm.html","title":"helm","keywords":"","body":"下载chart至本地 helm repo add bitnami https://charts.bitnami.com/bitnami helm pull bitnami/redis --version=17.9.4 --untar Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/ingress/ingress.html":{"url":"2.容器/k8s/ingress/ingress.html","title":"ingress","keywords":"","body":"nginx ingress 配置手册 可以通过下面三种方式配置nginx k8s ConfigMap配置项方式 k8s Annotations注解方式 注解方式-常用配置 注解的key与value取值为字符串类型. 布尔或数字等类型必须加引号如: \"true\", \"false\", \"100\" body体大小 设置每个location读取客户端请求体的缓冲区大小。如果请求体大于缓冲区，则整个请求体或仅其部分被写入一个临时文件。 默认情况下，缓冲区大小等于两个内存页。这在x86、其他32位平台和x86-64上是8K。在其他64位平台上通常是16K。 nginx.ingress.kubernetes.io/client-body-buffer-size: 1M 对应原生nginx配置 Syntax: client_body_buffer_size size; Default: client_body_buffer_size 8k|16k; Context: http, server, location Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/install/":{"url":"2.容器/k8s/install/","title":"install","keywords":"","body":" kubeadm kubeadm rke Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/install/binary.html":{"url":"2.容器/k8s/install/binary.html","title":"binary","keywords":"","body":"二进制安装 编译源码 安装golang 下载解压配置 tar zxvf go1.16.5.linux-amd64.tar.gz -C /usr/local/ cat >> ~/.bash_profile 编译全部组件 unzip kubernetes-1.18.6.zip cd kubernetes-1.18.6 yum install -y rsync rm -rf _output make -j4 安装k8s主节点 安装etcd 下载解压etcd etcd-v3.3.9-linux-amd64.tar.gz tar -zxvf etcd-v3.3.9-linux-amd64.tar.gz sudo cp etcd-v3.3.9-linux-amd64/{etcd,etcdctl} /usr/bin/ 配置服务 sudo tee /usr/lib/systemd/system/etcd.service 创建目录 mkdir /var/lib/etcd 启动 sudo systemctl daemon-reload sudo systemctl enable etcd.service --now 查看集群状态 [root@localhost ~]# etcdctl cluster-health member 8e9e05c52164694d is healthy: got healthy result from http://localhost:2379 cluster is healthy 上传启动 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/install/k8s-kubeadm.html":{"url":"2.容器/k8s/install/k8s-kubeadm.html","title":"k8s-kubeadm","keywords":"","body":" Table of Contents generated with DocToc 单机 HA 所有节点安装kubeadm和kubelet 配置负载均衡 创建集群 新worker节点加入集群 单机 配置yum源 升级内核 安装docker 关闭防火墙 systemctl stop firewalld && systemctl disable firewalld iptables -F && iptables -X && iptables -F -t nat && iptables -X -t nat && iptables -P FORWARD ACCEPT 关闭swap swapoff -a sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab sysctl -p 设置内核参数 modprobe br_netfilter cat 修改Linux 资源配置文件,调高ulimit最大打开数和systemctl管理的服务文件最大打开数 echo \"* soft nofile 655360\" >> /etc/security/limits.conf echo \"* hard nofile 655360\" >> /etc/security/limits.conf echo \"* soft nproc 655360\" >> /etc/security/limits.conf echo \"* hard nproc 655360\" >> /etc/security/limits.conf echo \"* soft memlock unlimited\" >> /etc/security/limits.conf echo \"* hard memlock unlimited\" >> /etc/security/limits.conf echo \"DefaultLimitNOFILE=1024000\" >> /etc/systemd/system.conf echo \"DefaultLimitNPROC=1024000\" >> /etc/systemd/system.conf 配置k8s源 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum clean all && yum makecache 安装依赖 yum install -y epel-release conntrack ipvsadm ipset jq sysstat curl iptables libseccomp yum-utils device-mapper-persistent-data lvm2 安装指定版本 查看版本列表 yum list kubeadm --showduplicates|sort -r 安装指定版本kubeadm 安装1.18.6版本 version=1.18.6-0 yum install -y kubelet-$version kubeadm-$version kubectl-$version --disableexcludes=kubernetes 安装命令补全 yum install -y bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 启动kubelet systemctl enable kubelet --now 下载k8s相关镜像 for i in `kubeadm config images list 2>/dev/null |sed 's/k8s.gcr.io\\///g'`; do docker pull registry.aliyuncs.com/google-containers/${i} docker tag registry.aliyuncs.com/google-containers/${i} k8s.gcr.io/${i} docker rmi registry.aliyuncs.com/google-containers/${i} done 修改kubelet配置 sed -i \"s;KUBELET_EXTRA_ARGS=;KUBELET_EXTRA_ARGS=\\\"--fail-swap-on=false\\\";g\" /etc/sysconfig/kubelet 启动kubelet systemctl enable --now kubelet 初始化集群 k8sversion=`kubeadm version -o yaml|grep gitVersion|sed 's#gitVersion:##g'|sed 's/ //g'` echo \"k8s version: $k8sversion\" kubeadm init --kubernetes-version=$k8sversion --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap 配置授权 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 安装网络 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml HA 参考地址1 参考地址2 架构图 节点说明 | hostname | IP地址 | 应用| | :----: | :----: | :----:| | node1 | 172.16.145.160 | docker kubelet kubeadm kubectl keepalived harproxy control-plane| | node2 | 172.16.145.161 | docker kubelet kubeadm kubectl keepalived harproxy control-plane| | node3 | 172.16.145.162 | docker kubelet kubeadm kubectl keepalived harproxy control-plane| | | 172.16.145.200 | | 172.16.145.200为虚拟IP 所有节点升级内核 所有节点安装docker 添加Host解析 cat >> /etc/hosts 所有节点关闭防火墙 systemctl stop firewalld && systemctl disable firewalld 所有节点关闭swap swapoff -a sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab sysctl -p 所有节点设置内核参数 modprobe br_netfilter cat 所有节点安装ipvs管理工具 yum install ipvsadm ipset -y 所有节点添加ipvs cat > /etc/sysconfig/modules/ipvs.modules 修改docker cgroup driver为systemd 根据文档CRI installation中的内容，对于使用systemd作为init system的Linux的发行版， 使用systemd作为docker的cgroup driver可以确保服务器节点在资源紧张的情况更加稳定， 因此这里修改各个节点上docker的cgroup driver为systemd cat >> /etc/docker/daemon.json 重启 systemctl restart docker 所有节点安装kubeadm和kubelet 配置k8s源 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum clean all && yum makecache 安装依赖 yum install -y epel-release conntrack jq sysstat curl iptables libseccomp yum-utils device-mapper-persistent-data lvm2 安装kubelet等 yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes 安装命令补全 yum install -y bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 修改kubelet配置 sed -i \"s;KUBELET_EXTRA_ARGS=;KUBELET_EXTRA_ARGS=\\\"--fail-swap-on=false\\\";g\" /etc/sysconfig/kubelet 启动kubelet systemctl enable --now kubelet 下载k8s相关镜像 for i in `kubeadm config images list 2>/dev/null |sed 's/k8s.gcr.io\\///g'`; do docker pull gcr.azk8s.cn/google-containers/${i} docker tag gcr.azk8s.cn/google-containers/${i} k8s.gcr.io/${i} docker rmi gcr.azk8s.cn/google-containers/${i} done 配置时钟同步 yum -y install ntpdate ntpdate ntp1.aliyun.com echo \"*/5 * * * * bash ntpdate ntp1.aliyun.com\" >> /etc/crontab 修改时区 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 配置负载均衡 keepalived+haproxy方式 所有节点安装harproxy yum -y install haproxy 修改配置文件 vim /etc/haproxy/haproxy.cfg 添加以下配置 frontend kubernetes-apiserver mode tcp bind *:8443 option tcplog default_backend kubernetes-apiserver backend kubernetes-apiserver mode tcp balance roundrobin server node1 172.16.145.160:6443 check server node2 172.16.145.161:6443 check server node3 172.16.145.162:6443 check 启动 systemctl enable haproxy --now 安装keepalived yum install -y keepalived node1节点配置 cat > /etc/keepalived/keepalived.conf node2节点配置 cat > /etc/keepalived/keepalived.conf node3节点配置 cat > /etc/keepalived/keepalived.conf 启动keepalived systemctl enable keepalived --now 创建集群 节点1初始化 kubeadm init --control-plane-endpoint \"172.16.145.200:8443\" --upload-certs --ignore-preflight-errors=Swap 所有节点执行配置授权 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 配置网络 kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml 根据init输出信息，获取到tocken等信息进行节点添加 其他master节点加入 kubeadm join 172.16.145.200:8443 --token awtvoq.ljszotcg6j99uy66 \\ --discovery-token-ca-cert-hash sha256:7e902395862e37d768dc4df48300013ad5571902a52302b2443856fa565fd657 \\ --control-plane --certificate-key dcd50768c16c5f124b86248820eca802f44ed1e9e4f546661e0f4d81750ee7fa 其他node节点加入集群 kubeadm join 172.16.145.200:8443 --token awtvoq.ljszotcg6j99uy66 \\ --discovery-token-ca-cert-hash sha256:7e902395862e37d768dc4df48300013ad5571902a52302b2443856fa565fd657 node2 node3加入集群成为control-plane kubeadm join 172.16.145.200:8443 --token awtvoq.ljszotcg6j99uy66 \\ --discovery-token-ca-cert-hash sha256:7e902395862e37d768dc4df48300013ad5571902a52302b2443856fa565fd657 \\ --control-plane --certificate-key dcd50768c16c5f124b86248820eca802f44ed1e9e4f546661e0f4d81750ee7fa mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 新worker节点加入集群 配置yum源 可选 升级内核 安装docker 关闭防火墙 systemctl stop firewalld && systemctl disable firewalld 关闭swap swapoff -a sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab sysctl -p 设置内核参数 modprobe br_netfilter cat 配置k8s源 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1t gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum clean all && yum makecache 安装依赖 yum install -y epel-release conntrack ipvsadm ipset jq sysstat curl iptables libseccomp yum-utils device-mapper-persistent-data lvm2 安装kubelet等 yum install -y kubelet kubeadm --disableexcludes=kubernetes 安装命令补全 yum install -y bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 下载k8s相关镜像 #k8s.gcr.io被墙，需要走微软代理地址 for i in `kubeadm config images list 2>/dev/null |sed 's/k8s.gcr.io\\///g'`; do docker pull gcr.azk8s.cn/google-containers/${i} docker tag gcr.azk8s.cn/google-containers/${i} k8s.gcr.io/${i} docker rmi gcr.azk8s.cn/google-containers/${i} done 配置时钟同步 yum -y install ntpdate ntpdate ntp1.aliyun.com echo \"*/5 * * * * bash ntpdate ntp1.aliyun.com\" >> /etc/crontab 修改时区 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 修改kubelet配置 sed -i \"s;KUBELET_EXTRA_ARGS=;KUBELET_EXTRA_ARGS=\\\"--fail-swap-on=false\\\";g\" /etc/sysconfig/kubelet 启动kubelet systemctl enable --now kubelet 获取加入集群的token等 kubeadm token create --print-join-command 设置Hostname hostnamectl --static set-hostname work1 查看节点信息 kubectl get node 重复添加解决 https://blog.csdn.net/wzygis/article/details/84098247 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/install/k8s-rke.html":{"url":"2.容器/k8s/install/k8s-rke.html","title":"k8s-rke","keywords":"","body":" Table of Contents generated with DocToc k8s rke安装k8s集群 k8s rke安装k8s集群 rke文档地址 1、环境准备 虚拟机 * 3 配置阿里云yum源 2、下载rke二进制文件 安装节点下载即可 curl -L https://github.com/rancher/rke/releases/download/v0.3.2/rke_linux-amd64 -o /usr/bin/rke chmod +x /usr/bin/rke 3、关闭防火墙、selinux等 关闭所有主机的selinux、firewalld setenforce 0 sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config systemctl stop firewalld && systemctl disable firewalld 4、安装docker 所有节点 yum install -y epel-release yum install -y yum-utils net-tools conntrack-tools wget yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce-18.06.1.ce 启动 systemctl start docker systemctl enable docker 配置加速 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json 5、初始化docker用户 所有节点 useradd -m docker && echo \"1qaz@WSX\" | passwd --stdin docker 6、安装节点配置互信 ssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa ssh-copy-id docker@节点地址 7、创建集群配置文件 cat > rancher-cluster.yml 8.下载镜像 可提前下载好所需镜像 docker pull rancher/rke-tools:v0.1.50 docker pull rancher/hyperkube:v1.15.5-rancher1 docker pull rancher/coreos-etcd:v3.3.10-rancher1 9、构建集群 rke up --config ./rancher-cluster.yml 10、查看结果 11、配置 主节点 mkdir ~/.kube cat kube_config_rancher-cluster.yml > ~/.kube/config 12、安装kubectl等 cat > /etc/yum.repos.d/kubernetes.repo 主机点安装Kubectl即可 yum -y install kubectl 安装自动补全 yum install -y bash-completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc 13、查看节点信息 kubectl get node 14、发布应用测试 cat >> nginx.yaml 创建 kubectl apply -f nginx.yaml 查看状态 kubectl get pod -o wide 查看svc，测试 kubectl get svc 删除 kubectl delete deploy --all kubectl delete svc/nginx-service Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/install/kubeadm-offline.html":{"url":"2.容器/k8s/install/kubeadm-offline.html","title":"kubeadm-offline","keywords":"","body":" Table of Contents generated with DocToc 离线安装 配置k8s源 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum clean all && yum makecache 安装yum插件 yum install yum-plugin-downloadonly -y 导出依赖 yum install --downloadonly --downloaddir=/root/k8s kubelet-1.15.5 kubeadm-1.15.5 kubectl-1.15.5 安装kububeadm yum install -y kubelet-1.15.5 kubeadm-1.15.5 kubectl-1.15.5 --disableexcludes=kubernetes 下载k8s相关镜像 #k8s.gcr.io被墙，需要走微软代理地址 for i in `kubeadm config images list 2>/dev/null |sed 's/k8s.gcr.io\\///g'`; do docker pull gcr.azk8s.cn/google-containers/${i} docker tag gcr.azk8s.cn/google-containers/${i} k8s.gcr.io/${i} docker rmi gcr.azk8s.cn/google-containers/${i} done 离线下载镜像 docker save $(docker images | grep -v REPOSITORY | awk 'BEGIN{OFS=\":\";ORS=\" \"}{print $1,$2}') -o /root/k8s/k8s-master.tar 打包资源包 tar zcvf k8s-1.15.5.tar.gz k8s/ 离线安装 离线安装docker 上传k8s资源包，解压 tar zxvf k8s-*.tar.gz 导入镜像 docker load -i k8s/k8s-master.tar 安装kubeadm rpm -ivh k8s/*.rpm --nodeps --force 修改时区 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 关闭防火墙 systemctl stop firewalld && systemctl disable firewalld 关闭swap swapoff -a sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab sysctl -p 设置内核参数 modprobe br_netfilter cat 修改Linux 资源配置文件,调高ulimit最大打开数和systemctl管理的服务文件最大打开数 echo \"* soft nofile 655360\" >> /etc/security/limits.conf echo \"* hard nofile 655360\" >> /etc/security/limits.conf echo \"* soft nproc 655360\" >> /etc/security/limits.conf echo \"* hard nproc 655360\" >> /etc/security/limits.conf echo \"* soft memlock unlimited\" >> /etc/security/limits.conf echo \"* hard memlock unlimited\" >> /etc/security/limits.conf echo \"DefaultLimitNOFILE=1024000\" >> /etc/systemd/system.conf echo \"DefaultLimitNPROC=1024000\" >> /etc/systemd/system.conf 修改kubelet配置 sed -i \"s;KUBELET_EXTRA_ARGS=;KUBELET_EXTRA_ARGS=\\\"--fail-swap-on=false\\\";g\" /etc/sysconfig/kubelet 配置hostname 172.16.145.140主要替换为实际IP hostnamectl --static set-hostname master echo \"172.16.145.140 master\" >> /etc/hosts 配置kubelet自启动 systemctl enable kubelet 初始化集群 k8sversion=`kubeadm version -o yaml|grep gitVersion|sed 's#gitVersion:##g'|sed 's/ //g'` echo \"k8s version: $k8sversion\" kubeadm init --kubernetes-version=$k8sversion --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap 配置授权 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 下载flannel.yml 下载以下镜像并导入 quay.io/coreos/flannel:v0.11.0-amd64 删除主节点污点 kubectl taint nodes --all node-role.kubernetes.io/master- 安装网络 kubectl apply -f kube-flannel.yml Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/issue/issue.html":{"url":"2.容器/k8s/issue/issue.html","title":"issue","keywords":"","body":" cgroups导致的内存泄露 网络 calico 1.异常: calico/node is not ready: BIRD is not ready: BGP not established with x.x.x.x 此时状态为: 运行中但未READY [root@k8s-master ~]# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-578894d4cd-gxfw2 1/1 Running 0 82m kube-system calico-node-jzjj5 0/1 Running 0 15s kube-system calico-node-vwfrr 0/1 Running 0 15s 解决方式 kubectl edit ds -n kube-system calico-node spec.template.spec.containers下新增如下环境变量（注意缩进不能用tab） - name: IP_AUTODETECTION_METHOD value: interface=ens.* Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/monitor/":{"url":"2.容器/k8s/monitor/","title":"monitor","keywords":"","body":" weavescpoe Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/monitor/weavescope.html":{"url":"2.容器/k8s/monitor/weavescope.html","title":"weavescope","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/network/":{"url":"2.容器/k8s/network/","title":"network","keywords":"","body":" cni calico dns Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/network/calico.html":{"url":"2.容器/k8s/network/calico.html","title":"calico","keywords":"","body":"Calico介绍 官方文档地址 Calico是什么? Calico是一个面向容器、虚拟机、基于原生主机的工作负载的开源网络和网络安全解决方案。 Calico支持广泛的平台，包括Kubernetes、OpenShift、Docker EE、OpenStack和裸金属服务器。 Calico的eBPF数据平面与Linux的标准网络管道，提供了惊人的性能和真正的云原生可伸缩性。 为什么使用Calico? 可选的数据面 可选性更多：Calico为您提供了一个数据面选择，包括一个纯Linux eBPF数据面、一个标准Linux网络数据面和一个Windows HNS数据面。 无论哪种选择适合您，您都将获得相同的、易于使用的基础网络、网络策略和IP地址管理功能，这些功能使Calico成为云原生应用程序最值得信赖的网络和网络策略解决方案。 网络安全的最佳实践 Calico丰富的网络策略模型可以很容易地锁定通信，加上内置的支持Wireguard加密，确保您的pod间的流量安全可靠 Calico的策略引擎可以在主机网络层和在服务网格层(如Istio & Envoy)执行相同的策略模型，保护基础设施不受损害的工作负载影响，保护工作负载不受损害的基础设施的影响。 优异的性能 根据您的偏好，Calico使用Linux eBPF或Linux内核高度优化的标准网络管道来提供高性能的网络。 Calico的网络选项足够灵活，可以在大多数环境中不使用覆盖运行，避免了封装encap/decap的开销。 Calico的控制平面和策略引擎在多年的生产使用中经过优化，使得CPU使用和占用率较低。 可伸缩性 Calico基于云原生设计模式，并结合了世界上最大的互联网运营商所信任的经过验证的基于标准的网络协议。 其结果是一个具有特殊可伸缩性的解决方案，多年来一直在大规模生产中运行。Calico的开发测试周期包括定期测试数千个节点集群，保证了优异的性能和可伸缩性。 兼容性 Calico使Kubernetes工作负载和非Kubernetes工作负载能够无缝、安全地通信。Kubernetes pods是您网络上的一等公民，能够与网络上的任何其他工作负载进行通信。 此外，Calico还可以保护现有的基于主机的工作负载（无论是在公共云中还是在VMs或裸机服务器上）。所有工作负载都受相同的网络策略模型的约束，因此唯一允许流动的流量是您期望流动的流量。 经过生产环境验证 Calico深受包括SaaS提供商、金融服务公司和制造商在内的大型企业的信任和生产。 最大的公共云提供商已经选择Calico为其托管的Kubernetes服务(Amazon EKS、Azure AKS、谷歌GKE和IBM IKS)提供网络安全性，这些服务运行在数万个集群中 完全支持Kubernetes网络策略 在开发API期间，Calico的网络策略引擎参考Kubernetes原生网络策略的实现。 Calico的区别在于它实现了API定义的全部特性，为用户提供了定义API时所设想的所有功能和灵活性。 对于那些需要更强大功能的用户，Calico支持一组扩展的网络策略功能，这些功能与Kubernetes API无缝协作，为用户在定义网络策略方面提供了更大的灵活性。 Calico实践 Calico网络选型 Kubernetes网络基础知识 Kubernetes网络模型定义了一个\"扁平\"网络，其中: 每个Pod拥有自己的IP地址 任何节点上的pod都可以在没有NAT的情况下与所有其他节点上的pod通信 这创建了一个干净、向后兼容的模型，从端口分配、命名、服务发现、负载平衡、应用程序配置和迁移的角度来看， pod可以像vm或物理主机一样对待。可以使用网络策略来定义网络分段，以限制这些基本网络功能内的通信。 在这个模型中，对于支持不同的网络方法和环境有很大的灵活性。网络实现的具体细节取决于使用的CNI、网络和云提供商插件的组合 CNI插件 CNI(Container Network Interface容器网络接口)是一个标准的API，允许不同的网络实现接入到Kubernetes中。 Kubernetes在创建或销毁pod时调用API。CNI插件有两种类型: CNI网络插件：负责向Kubernetes pod网络添加或删除pod。这包括创建/删除每个pod的网络接口，并将其连接/断开与其余网络实现的连接。 CNI IPAM插件：负责在pod被创建或删除时为pod分配和释放IP地址。根据插件的不同，这可能包括为每个节点分配一个或多个IP地址范围（CIDR），或者从底层公共云的网络获取IP地址以分配给POD。 Overlay网络 在底层网络不知道连接到overlay覆盖网络的设备的情况下，overlay覆盖网络允许网络设备通过底层网络进行通信。 从连接到overlay覆盖网络的设备的角度来看，它看起来就像一个普通的网络。 有许多不同类型的overlay覆盖网络，它们使用不同的协议来实现这一点，但通常它们都有一个共同的特征，取一个网络包，称为内部包，并将它封装在一个外部网络包中。通过这种方式，底层可以看到外部包，而不需要了解如何处理内部包。 overlay如何知道将包发送到哪里取决于overlay的类型和它们使用的协议。 同样，数据包的包装方式在不同的覆盖类型之间也有所不同。以VXLAN为例，内部报文被封装，外部报文以UDP协议发送。 overlay网络的优点是对底层网络基础设施的依赖性最小，但有以下缺点: 如果运行网络密集型工作负载，与非覆盖网络相比，性能影响较小， overlay上的工作负载不容易从网络的其余部分寻址。因此，NAT网关或负载均衡器需要桥接overlay和底层网络。 Calico网络选项是非常灵活的，所以通常你可以选择是否你喜欢Calico提供覆盖网络，或非覆盖网络。 你可以在Calico确定最佳网络选项指南中了解更多。 跨子网Overlay网络 除了标准的VXLAN或IP-In-IP覆盖，Calico还支持VXLAN和IP-In-IP的\"跨子网\"模式。在这种模式下，在每个子网中，底层网络充当L2网络。 在单个子网内发送的数据包不会被封装，因此可以获得非覆盖网络的性能。通过子网发送的数据包被封装，就像普通的覆盖网络一样，减少了对底层网络的依赖性（无需与底层网络集成或对底层网络进行任何更改） 与标准覆盖网络一样，底层网络不知道pod IP地址，pod IP地址不能在集群外部路由。 集群外部的Pod IP可路由性 不同Kubernetes网络实现的一个重要区别是pod IP地址是否可以通过更广泛的网络在集群之外路由 不可路由 如果Pod IP地址集群外不可路由,当一个Pod试图建立一个网络连接集群外的一个IP地址, Kubernetes使用了一种叫做SNAT(来源网络地址转换)来改变源IP地址的IP地址, IP地址的节点承载Pod。连接上的任何返回包都会自动映射回pod IP地址。 因此，pod对SNAT无感知，连接的目的地将节点视为连接的源，集群之外的网络永远不会看到pod IP地址。 对于相反方向的连接，集群外部需要连接到pod情况下，这只能通过Kubernetes services或Kubernetes ingress来实现。 集群之外不能直接连接到pod IP地址，因为集群之外的的网络不知道如何将数据包路由到pod IP地址 可路由 如果pod IP地址可在集群外部路由，那么pod可以连接到外部世界而无需SNAT， 并且外部世界可以直接连接到pod而无需通过Kubernetes服务或Kubernetes入口。 集群外部可路由pod IP地址的优点是： 避免出站连接SNAT，简化操作日志的调试和理解 pod需要直接访问，而无需通过Kubernetes服务或Kubernetes入口，那么可路由pod ip在操作上可能比使用主机网络pod更简单 在集群外部可路由的pod IP地址的主要缺点是，pod IP在更广泛的网络中必须是唯一的。 例如，如果运行多个集群，则需要为每个集群中的pods使用不同的IP地址范围(CIDR)。 在大规模运行时，或者在现有企业对IP地址空间有其他重大需求时，这会导致IP地址范围耗尽。 BGP BGP（Border Gateway Protocol）是一种基于标准的网络协议，用于在网络上共享路由。它是互联网的基本组成部分之一，具有非凡的扩展特性。 Calico内置了对BGP的支持。在on-prem部署中，这允许Calico与物理网络（通常是顶部或机架路由器）对等以交换路由，从而形成一个非覆盖网络，其中pod IP地址可在更广泛的网络上路由，就像连接到网络的任何其他工作负载一样。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/network/dns.html":{"url":"2.容器/k8s/network/dns.html","title":"dns","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/opterator/白皮书/01-摘要.html":{"url":"2.容器/k8s/opterator/白皮书/01-摘要.html","title":"01-摘要","keywords":"","body":"operator白皮书 摘要 Executive Summary 维护应用程序基础结构需要许多重复性的人为操作，而这些重复性操作通常没有太大的意义。 计算机是执行精确任务的首选方法，可以验证对象的状态，从而使基础设施需求能够被编码。operator提供了一种方法来封装应用程序所需的活动、检查和语句管理。 在Kubernetes中，operator通过扩展API的功能来提供智能的动态管理功能。 这些operator组件允许通用流程的自动化以及响应式应用程序不断适应其环境。这反过来促进应用更快速的开发，减少故障点，更低的平均恢复时间，并增加了工程自治权。 鉴于operator模式越来越受欢迎，白皮书的制定已成为当务之急，以此来帮助新手和专家学习社区认可的最佳实践，以实现他们的目标。 在本文档中，我们不仅概述了operator的分类，还介绍了operator应用程序管理系统的推荐配置、实现和用例。 简介 该白皮书在比Kubernetes更广泛的背景下定义了operator。白皮书描述了operator的特征和组件，概述了目前正在使用的常见模式，并解释了它们与Kubernetes控制器的区别。 白皮书还深入研究了operator的功能，如备份、恢复和自动配置调优，深入了解了当前正在使用的框架、生命周期管理、安全风险和用例。 本文包括最佳实践，包括可观察性和安全性、技术实现和CNCF维护的代码样本。 文档目标 本文的目标是在Kubernetes和其他容器协调器的上下文中为云原生应用程序提供operator的定义。 文档目标受众/最低经验水平 本文档面向应用程序开发人员、Kubernetes集群运营商和服务提供商(内部或外部)——他们希望了解运营商及其可以解决的问题。 它还可以帮助已经关注运营商的团队了解何时何地使用它们以达到最佳效果。白皮书假设使用者了解基本的Kubernetes知识，如熟悉Pods和deployment。 基本原理 Kubernetes和其他容器协调器的成功，一直归功于他们对容器的主要功能的关注。尽管部分企业开启了云原生路线，但与更具体的用例(微服务、无状态应用程序)合作更有意义。 随着Kubernetes和其他容器协调器的声誉和可扩展性的增长，需求变得更加雄心勃勃。使用协调器的完整生命周期功能的愿望也转移到了高度分布式的数据存储上 operator模式可以解决状态管理问题。通过利用Kubernetes内置的功能，如自愈、协调和扩展应用程序特定的复杂性; 可以将任何应用程序的生命周期、操作自动化，并将其转化为功能强大的产品。 operator与Kubernetes不应是绑定的，管理完全自动化的应用程序的思想可以导出到其他平台。本文的目的是把operator这个概念带到一个比Kubernetes本身更高的层次。 Operator设计模式 本节用高级概念描述operator模式。下一节Kubernetes operator定义将根据Kubernetes对象和概念描述operator模式的实现。 operator设计模式定义了如何使用领域特定的知识和声明性状态，来管理应用程序和基础设施资源。 operator模式的目标是通过在代码中捕获领域特定的知识并使用声明性API公开它， 从而减少保持应用程序处于健康和良好维护状态所需的手动强制性工作(如何备份、扩展、升级……) 通过使用operator模式，可以在代码中获取关于如何调整和维护资源的知识，通常也可以在单个服务(也称为控制器)中获取。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/opterator/00-k8s中的operator是什么.html":{"url":"2.容器/k8s/opterator/00-k8s中的operator是什么.html","title":"00-k8s中的operator是什么","keywords":"","body":"# Operator pattern Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/opterator/01-如何写一个operator.html":{"url":"2.容器/k8s/opterator/01-如何写一个operator.html","title":"01-如何写一个operator","keywords":"","body":"如何写一个operator 文章源地址请移步writing-a-controller-for-pod-labels 样例代码 k8s中的operator是什么？ operator旨在简化基于k8s部署有状态服务（例如：ceph集群、skywalking集群） 可以利用Operator SDK 构建一个operator， operator使扩展k8s及实现自定义调度变得更加简单。 尽管Operator SDK 适合构建功能齐全的operator， 但也可以使用它来编写单个控制器。 这篇文章将指导您在Go中编写一个Kubernetes控制器，该控制器将向具有特定注释的pod添加pod-name标签 为什么我们需要一个控制器呢？ 最近我们项目中有这么个需求：通过一个service将流量路由至同一ReplicaSet中指定pod内（service对应一个或多个pod） 而原生k8s并不能实现该功能，因为原生service只能通过label与Pod匹配，并且同一ReplicaSet内，Pod具有相同标签。 上述需求有两种解决方案: 创建service时不指定标签选择器，而是利用Endpoints或EndpointSlices关联pod 此时我们需要写一个自定义控制器，用于插入指定pod的端点地址至Endpoints或EndpointSlices对象 为每个Pod添加具有唯一value的标签，接下来我们就可以利用标签选择器进行service与Pod的关联。 由于k8s中的控制器实质是个控制循环程序，控制器可以对k8s的资源（Resource，比如namespace、service等）进行监听追踪。 此时如果我们创建一个控制器，仅监听Pod资源，针对指定Pod进行label处理，就可实现上述需求。 当然k8s原生资源StatefulSets也是可以实现这一功能的，但假设我们不想/不能使用StatefulSets类型去实现呢？ 一般情况下，我们很少直接创建Pod类型，而是通过Deployment, ReplicaSet间接创建Pod。 我们可以指定标签添加到PodSpec中的每个Pod，但不能使用动态值，因此无法复制StatefulSet的pod-name标签。 我们尝试使用mutating admission webhook 实现。 当任何人创建Pod时，webhook会自动注入一个包含Pod名称的标签对Pod进行修改。 遗憾的是这种方式并不能实现我们的需求： 并不是所有的Pod在创建前都有名字。 举个例子：当ReplicaSet控制器创建一个Pod时，他向kube-apiserver发送一个请求，获取一个namePrefix而非name kubeapi-server在将新的Pod持久化到etcd之前生成一个唯一的名称， 这个过程发生于在调用我们的许可webhook之后。所以在大多数情况下，我们无法知道一个带有mutating webhook的Pod的名字 一旦Pod持久化至K8s集群中时，它几乎不会发生变更，但我们仍然可以通过以下方式，添加label kubectl label my-pod my-label-key=my-label-value 我们需要观察Kubernetes API中任何Pod的变化，并添加我们想要的标签。 我们将编写一个控制器来为我们做这件事，而不是手动做这件事 利用Operator SDK构建一个控制器 控制器是一个控制循环，它从Kubernetes API中读取期望的资源状态，并采取行动使集群的实际状态达到期望状态 安装配置 1.安装Operator SDK 下载二进制 sudo curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v1.12.0/operator-sdk_linux_amd64 sudo mv operator-sdk_linux_amd64 /usr/local/bin/operator-sdk 2.构建工程 mkdir label-operator && cd label-operator 3.初始化工程 export GOPROXY=https://goproxy.cn operator-sdk init --domain=weiliang.io --repo=github.com/weiliang-ms/label-operator 4.创建控制器 接下来我们创建一个控制器，这个控制器将会处理Pod资源，而非自定义资源，所以不需要生成资源代码。 operator-sdk create api --group=core --version=v1 --kind=Pod --controller=true --resource=false 初始化编码 controllers/pod_controller.go解析 现在我们拥有了一个新文件: controllers/pod_controller.go。 该文件包含了PodReconciler类型，该类型包含两个方法： Reconcile函数： func (r *PodReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { _ = log.FromContext(ctx) // your logic here return ctrl.Result{}, nil } 当创建、更新、或删除Pod时会调用Reconcile方法，Pod名称与命名空间作为函数入参，存于ctrl.Request对象之中 SetupWithManager函数： func (r *PodReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(&corev1.Pod{}). Complete(r) } operator会在启动时执行SetupWithManager函数，SetupWithManager函数用于生命监听资源类型 因为我们只想要监听Pod资源变化，所以监听资源这部分代码不动 RBAC设置 接下来为我们的控制器配置RBAC权限，代码生成器生成的默认权限如下： //+kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;create;update;patch;delete //+kubebuilder:rbac:groups=core,resources=pods/status,verbs=get;update;patch //+kubebuilder:rbac:groups=core,resources=pods/finalizers,verbs=update 显然我们并不需要以上全部权限，我们控制器从不会CRUD Pod的status与finalizers字段。 控制器需要的仅仅是对Pod的读权限与更新权限，本着最小原则，我们调整权限如下 // +kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;update;patch 此时我们已经编写好了控制器的基本调用逻辑。 实现Reconcile函数 我们希望Reconcile实现以下功能： 通过入参ctrl.Request中的Pod名称与命名空间字段，请求k8s api获取Pod对象 如果Pod拥有add-pod-name-label注解，给这个Pod添加一个pod-name标签 将上一步Pod的变更回写k8s中 接下来我们为注解与标签定义一些常量 const ( addPodNameLabelAnnotation = \"padok.fr/add-pod-name-label\" podNameLabel = \"padok.fr/pod-name\" ) 根据入参获取Pod 首先我们根据入参信息，去k8s api获取Pod实例 func (r *PodReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { l := log.FromContext(ctx) var pod corev1.Pod if err := r.Get(ctx, req.NamespacedName, &pod); err != nil { l.Error(err, \"unable to fetch Pod\") return ctrl.Result{}, err } return ctrl.Result{}, nil } 异常处理 当创建、更新或删除一个Pod时，会触发我们控制器的Reconcile方法 但当事件为'删除事件'时，r.Get()会返回一个指定错误对象，接下来我们通过引用下面的包来处理这个异常。 package controllers import ( // other imports... apierrors \"k8s.io/apimachinery/pkg/api/errors\" // other imports... ) // other functions... func (r *PodReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { l := log.FromContext(ctx) var pod corev1.Pod if err := r.Get(ctx, req.NamespacedName, &pod); err != nil { if apierrors.IsNotFound(err) { // we'll ignore not-found errors, since we can get them on deleted requests. return ctrl.Result{}, nil } l.Error(err, \"unable to fetch Pod\") return ctrl.Result{}, err } return ctrl.Result{}, nil } // other functions... 编辑Pod，判断注解、标签是否存在 此时我们已经获取到了这个Pod对象（创建、更新事件），接下来我们获取Pod的注解元数据，判断是否需要添加标签 ... /* Step 1: 添加或移除标签. */ // 判断Pod是否存在注解 -> padok.fr/add-pod-name-label: true labelShouldBePresent := pod.Annotations[addPodNameLabelAnnotation] == \"true\" // 判断Pod是否存在标签 -> padok.fr/pod-name: Pod名称 labelIsPresent := pod.Labels[podNameLabel] == pod.Name // 如果期望状态与实际状态一致（含有上述标签、注解），返回 if labelShouldBePresent == labelIsPresent { log.Info(\"no update required\") return ctrl.Result{}, nil } // 存在注解 -> padok.fr/add-pod-name-label: true if labelShouldBePresent { // 判断标签map是否为空 if pod.Labels == nil { // 为空创建 pod.Labels = make(map[string]string) } // 添加标签 -> padok.fr/pod-name: Pod名称 pod.Labels[podNameLabel] = pod.Name log.Info(\"adding label\") } else { // 不存在注解 -> padok.fr/add-pod-name-label: true // 移除标签 delete(pod.Labels, podNameLabel) log.Info(\"removing label\") } ... 回写Pod至k8s /* Step 2: Update the Pod in the Kubernetes API. */ if err := r.Update(ctx, &pod); err != nil { l.Error(err, \"unable to update Pod\") return ctrl.Result{}, err } 当我们回写Pod变更至k8s时存在以下风险：集群内的Pod与我们获取到的Pod已经不一致（可能通过其他渠道变更了该Pod） 在编写一个k8s控制器时，我们应该明白一个问题：我们编写的控制器并不是唯一能操作k8s资源对象的实例(其他控制器、kubectl等亦能操作k8s资源对象) 当发生这种情况时，最好的做法是通过重新排队事件，从头开始处理。 if err := r.Update(ctx, &pod); err != nil { if apierrors.IsConflict(err) { // The Pod has been updated since we read it. // Requeue the Pod to try to reconciliate again. return ctrl.Result{Requeue: true}, nil } if apierrors.IsNotFound(err) { // The Pod has been deleted since we read it. // Requeue the Pod to try to reconciliate again. return ctrl.Result{Requeue: true}, nil } log.Error(err, \"unable to update Pod\") return ctrl.Result{}, err } 在k8s集群内运行该控制器 本人本地开发环境为windows10 + Ubuntu 20 本地ubuntu安装Kubectl并配置kube-config 集群信息 weiliang@DESKTOP-O8QG6I5:/mnt/d/github/label-operator$ kubectl get node NAME STATUS ROLES AGE VERSION node1 Ready master,worker 62d v1.18.6 node2 Ready master,worker 62d v1.18.6 node3 Ready master,worker 62d v1.18.6 node4 Ready worker 62d v1.18.6 label-operator下执行 shell目录 weiliang@DESKTOP-O8QG6I5:/mnt/d/github/label-operator$ pwd /mnt/d/github/label-operator 运行operator export GOPROXY=https://goproxy.cn make run 运行一个nginx服务Pod 新建一个ubuntu shell窗口执行 kubectl run --image=nginx:1.20.0 my-nginx 查看Pod信息 weiliang@DESKTOP-O8QG6I5:/mnt/d/github/label-operator$ kubectl get pod NAME READY STATUS RESTARTS AGE my-nginx 1/1 Running 0 78s 此时运行operator的窗口会输出如下信息，说明监听成功 2021-09-24T11:52:10.588+0800 INFO controller-runtime.manager.controller.pod no update required {\"reconciler group\": \"\", \"reconciler kind\": \"Pod\", \"name\": \"m y-nginx\", \"namespace\": \"default\"} 2021-09-24T11:52:10.597+0800 INFO controller-runtime.manager.controller.pod no update required {\"reconciler group\": \"\", \"reconciler kind\": \"Pod\", \"name\": \"m y-nginx\", \"namespace\": \"default\"} 2021-09-24T11:52:10.630+0800 INFO controller-runtime.manager.controller.pod no update required {\"reconciler group\": \"\", \"reconciler kind\": \"Pod\", \"name\": \"m y-nginx\", \"namespace\": \"default\"} 查看Pod标签 weiliang@DESKTOP-O8QG6I5:/mnt/d/github/label-operator$ kubectl get pod my-nginx --show-labels NAME READY STATUS RESTARTS AGE LABELS my-nginx 1/1 Running 0 4m38s run=my-nginx 此时我们给该Pod打上以下注解，并查看是否已自动添加新的标签 weiliang@DESKTOP-O8QG6I5:/mnt/d/github/label-operator$ kubectl annotate pod my-nginx padok.fr/add-pod-name-label=true pod/my-nginx annotated 查看标签 weiliang@DESKTOP-O8QG6I5:/mnt/d/github/label-operator$ kubectl get pod my-nginx --show-labels NAME READY STATUS RESTARTS AGE LABELS my-nginx 1/1 Running 0 6m39s padok.fr/pod-name=my-nginx,run=my-nginx 成功了！ 我们成功的实现上面的需求 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/opterator/02redis-operator.html":{"url":"2.容器/k8s/opterator/02redis-operator.html","title":"02redis-operator","keywords":"","body":"redis-operator 项目地址 redis-operator是什么 redis-operator负责在Kubernetes之上建立独立的redis和集群模式。它可以基于云环境或裸金属环境创建一个redis集群，并基于最佳实践设置。此外，通过集成redis-export提供了内置的监控功能。 特性列表 redis-operator具备以下特性： redis单机、集群模式创建 redis集群故障转移及恢复 内置prometheus exporter 动态存储模板支持 配额设置 redis密码/无密码 发布节点选择、亲和性设置 优先级管理（pod服务质量） 内核参数设置 这看起来跟使用helm安装redis cluster并无太大差别，那它是否有额外特性呢？ 架构 部署 演示环境信息： CentOS7 5.10.2-1.el7.elrepo.x86_64 Kubernetes v1.21.5 接下来我们部署redis-operator，并通过redis-operator维护redis: 下载redis-operator chart Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/quota/quota.html":{"url":"2.容器/k8s/quota/quota.html","title":"quota","keywords":"","body":" 配额 相关概念 配额限制维度 节点级计算资源限制 命名空间级计算资源限制 命名空间容器默认配额设置 容器级计算资源限制 参考文档 配额 相关概念 request与limits Kubernetes采用request和limit两种限制类型来对资源进行分配 request(资源需求)：即运行Pod的节点必须满足运行Pod的最基本需求才能运行Pod limits(资源限额)：描述Pod运行期间，内存最大可申请大小 一个容器申请0.5个CPU，就相当于申请1个CPU的一半，加个后缀m表示千分之一的概念。 比如说100m的CPU，表示0.1个cpu 配额与Pod优先级关系 Request=Limit: Pod类型为Guaranted（保证型），只有内存使用量超限（OOM）时才会被kill Request: Pod类型为Burstable(突发流量型)，节点计算资源不足时，可能会被kill回收 未设置Request Limit: Pod类型为Best Effort（尽最大努力型），节点计算资源不足时，会被首先kill 配额限制维度 节点级计算资源限制 Kubelet Node Allocatable用来为Kube组件和System进程预留资源， 从而保证当节点出现满负荷时也能保证k8s系统服务和System宿主机守护进程有足够的资源 Node Capacity: Node的所有硬件资源 kube-reserved: kube组件预留的资源 system-reserved: System进程预留的资源 eviction-threshold（阈值）: kubelet eviction(回收)的阈值设定 allocatable: 真正scheduler调度Pod时的参考值（保证Node上所有Pods的request resource不超过Allocatable） 查看当前节点的Capacity和Allocatable [root@node1 ~]# kubectl describe node node1 ... Capacity: cpu: 4 ephemeral-storage: 17394Mi hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 4004944Ki pods: 300 Allocatable: cpu: 3600m ephemeral-storage: 17394Mi hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 3371721521 pods: 300 ... 查看docker驱动 cgroup驱动如果为systemd，则开启不了Kubelet Node Allocatable [root@node1 ~]# docker info | grep \"Cgroup Driver\" Cgroup Driver: systemd 调整docker驱动为cgroupfs 调整/etc/docker/daemon.json内容，添加/修改以下值（需升级内核） \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"] 重启docker systemctl daemon-reload systemctl restart docker 调整kubelet参数配置 修改/var/lib/kubelet/kubeadm-flags.env，调整/增加以下参数： # 修改`kubelet cgroup`驱动`systemd`为`cgroupfs` --cgroup-driver=cgroupfs # 开启为kube组件和系统守护进程预留资源的功能 --enforce-node-allocatable=pods,kube-reserved,system-reserved # 设置k8s组件的cgroup --kube-reserved-cgroup=/system.slice/kubelet.service # 设置系统守护进程的cgroup --system-reserved-cgroup=/system.slice # 配置为k8s组件预留资源的大小，CPU、MEM --kube-reserved=cpu=1,memory=1Gi # 配置为系统进程（诸如 sshd、udev 等系统守护进程）预留资源的大小，CPU、MEM --system-reserved=cpu=0.5,memory=1Gi # 驱逐pod的配置：硬阈值（保证95%的内存利用率) --eviction-hard=memory.available 调整kubelet.service 调整文件/etc/systemd/system/kubelet.service 修改前 [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=http://kubernetes.io/docs/ [Service] ExecStart=/usr/local/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target 修改后 [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=http://kubernetes.io/docs/ [Service] ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.service ExecStartPre=/bin/mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.service ExecStart=/usr/local/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target 重启kubelet再次查看节点的Capacity和Allocatable [root@node1 ~]# systemctl daemon-reload [root@node1 ~]# systemctl restart kubelet [root@node1 ~]# kubectl describe node node1 ... Capacity: cpu: 4 ephemeral-storage: 17394Mi hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 4004944Ki pods: 300 Allocatable: cpu: 2500m ephemeral-storage: 14267554175 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 1748525873 pods: 300 ... 官方的样例说明 这是一个用于说明节点可分配（Node Allocatable）计算方式的示例： 节点拥有32Gi memeory，16 CPU和100Gi Storage资源: --kube-reserved被设置为cpu=1,memory=2Gi,ephemeral-storage=1Gi --system-reserved被设置为cpu=500m,memory=1Gi,ephemeral-storage=1Gi --eviction-hard被设置为memory.available 在这个场景下，Allocatable将会是14.5 CPUs、28.5Gi内存以及88Gi本地存储。 调度器保证这个节点上的所有Pod的内存requests总量不超过28.5Gi，存储不超过88Gi。 当Pod的内存使用总量超过28.5Gi或者磁盘使用总量超过88Gi时， kubelet将会驱逐它们。 如果节点上的所有进程都尽可能多地使用CPU，则Pod加起来不能使用超过14.5 CPUs的资源。 当没有执行kube-reserved和/或system-reserved策略且系统守护进程 使用量超过其预留时， 如果节点内存用量高于31.5Gi或存储大于90Gi，kubelet将会驱逐Pod 命名空间级计算资源限制 设置限定对象数据的资源配额 指定命名空间test01生效 cat 设置限定计算资源配额限制 指定命名空间test01生效 cat 命名空间容器默认配额设置 缺省值 创建测试命名空间 kubectl create ns test01 创建命名空间容器默认配额设置 cat 容器如果未声明request与limits -> 会根据命名空间下LimitRange策略对容器配额赋值 容器如果声明limits未声明request -> 则容器的内存request和limits值一致 容器如果声明request,未声明limits -> 容器request值被设置为声明的值，limits被设置成了LimitRange值 容器级计算资源限制 针对业务容器设置配额 apiVersion: v1 kind: Pod metadata: name: frontend spec: containers: - name: db image: mysql env: - name: MYSQL_ROOT_PASSWORD value: \"password\" resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" - name: wp image: wordpress resources: requests: memory: \"64M\" cpu: \"0.25\" limits: memory: \"128M\" cpu: \"0.5\" 参考文档 k8s 节点可分配资源限制 Node Allocatable k8s官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/security.html":{"url":"2.容器/k8s/security/security.html","title":"security","keywords":"","body":"云原生安全 k8s安全 本概述定义了一个模型，用于在Cloud Native安全性上下文中考虑Kubernetes安全性。 云原生安全的4个C 你可以分层去考虑安全性，云原生安全的4个C分别是云（Cloud）、集群（Cluster）、容器（Container）和代码（Code） 4C中的云 在许多方面，云（或者位于同一位置的服务器，或者是公司数据中心）是Kubernetes集群中的可信计算基础。 如果云层容易受到攻击（或者被配置成了易受攻击的方式），就不能保证在此基础之上构建的组件是安全的。 每个云提供商都会提出安全建议，以在其环境中安全地运行工作负载。 云提供商安全性 下面是一些比较流行的云提供商的安全性文档链接： Alibaba Cloud Amazon Web Services 基础设施安全 通过网络访问API服务（控制平面） 所有对Kubernetes控制平面的访问不允许在Internet上公开， 同时应由网络访问控制列表控制，该列表包含管理集群所需的IP地址集 通过网络访问Node（节点） 节点应配置为仅能从控制平面上通过指定端口来接受（通过网络访问控制列表）连接， 以及接受NodePort和LoadBalancer类型的Kubernetes服务连接。 如果可能的话，这些节点不应完全暴露在公共互联网上。 Kubernetes访问云提供商的API 每个云提供商都需要向Kubernetes控制平面和节点授予不同的权限集。 为集群提供云提供商访问权限时，最好遵循对需要管理的资源的最小特权原则。 访问etcd 对etcd（Kubernetes的数据存储）的访问应仅限于控制平面。根据配置情况，你应该尝试通过TLS来使用etcd etcd加密 在所有可能的情况下，最好对所有驱动器进行静态数据加密， 但是由于etcd拥有整个集群的状态（包括机密信息），因此其磁盘更应该进行静态数据加密。 集群 保护Kubernetes有两个方面需要注意： 保护可配置的集群组件 保护在集群中运行的应用程序 控制对Kubernetes API的访问 因为Kubernetes是完全通过API驱动的，所以，控制和限制谁可以通过API访问集群， 以及允许这些访问者执行什么样的API动作，就成为了安全控制的第一道防线。 为所有API交互使用传输层安全 （TLS） Kubernetes期望集群中所有的API通信在默认情况下都使用TLS加密，大多数安装方法也允许创建所需的证书并且分发到集群组件中。 请注意，某些组件和安装方法可能使用HTTP来访问本地端口， 管理员应该熟悉每个组件的设置，以识别潜在的不安全的流量 API认证 安装集群时，选择一个API服务器的身份验证机制，去使用与之匹配的公共访问模式。 例如，小型的单用户集群可能希望使用简单的证书或静态承载令牌方法。 更大的集群则可能希望整合现有的、OIDC、LDAP等允许用户分组的服务器。 所有API客户端都必须经过身份验证，即使它是基础设施的一部分，比如节点、代理、调度程序和卷插件。 这些客户端通常使用服务帐户或X509客户端证书，并在集群启动时自动创建或是作为集群安装的一部分进行设置 API授权 一旦通过身份认证，每个API的调用都将通过鉴权检查。 Kubernetes集成基于角色的访问控制（RBAC）组件，将传入的用户或组与一组绑定到角色的权限匹配。 这些权限将动作（get，create，delete）和资源（pod，service, node）在命名空间或者集群范围内结合起来， 根据客户可能希望执行的操作，提供了一组提供合理的违约责任分离的外包角色。 建议你将节点和RBAC一起作为授权者，再与NodeRestriction准入插件结合使用。 与身份验证一样，简单而广泛的角色可能适合于较小的集群，但是随着更多的用户与集群交互，可能需要将团队划分成有更多角色限制的单独的命名空间。 就鉴权而言，理解怎么样更新一个对象可能导致在其它地方的发生什么样的行为是非常重要的。 例如，用户可能不能直接创建Pod，但允许他们通过创建一个Deployment来创建这些Pod， 这将让他们间接创建这些Pod。同样地，从API删除一个节点将导致调度到这些节点上的Pod被中止， 并在其他节点上重新创建。原生的角色设计代表了灵活性和常见用例之间的平衡，但有限制的角色应该仔细审查， 以防止意外升级。如果外包角色不满足你的需求，则可以为用例指定特定的角色 控制对Kubelet的访问 Kubelet公开HTTPS端点，这些端点授予节点和容器强大的控制权。 默认情况下，Kubelet允许对此API进行未经身份验证的访问。 生产级别的集群应启用Kubelet身份验证和授权。 Kubelet 身份认证 要禁用匿名访问并向未经身份认证的请求发送401 Unauthorized响应，请执行以下操作： 带--anonymous-auth=false标志启动kubelet 要对kubelet的HTTPS端点启用X509客户端证书认证: 带--client-ca-file标志启动kubelet，提供一个CA证书包以供验证客户端证书 带--kubelet-client-certificate和--kubelet-client-key标志启动apiserver 要启用API持有者令牌（包括服务帐户令牌）以对kubelet的HTTPS端点进行身份验证，请执行以下操作： 确保在API服务器中启用了authentication.k8s.io/v1beta1 API组 带--authentication-token-webhook和--kubeconfig标志启动kubelet kubelet调用已配置的API服务器上的TokenReview API，以根据持有者令牌确定用户信息 集群中的组件（自定义应用）安全性 RBAC 授权(访问 Kubernetes API) 认证方式 应用程序 Secret 管理 etcd静态数据加密 Pod安全策略 服务质量（和集群资源管理） 网络策略 Kubernetes Ingress 的 TLS 支持 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/最佳实践/01不适用subPath挂载配置文件.html":{"url":"2.容器/k8s/security/最佳实践/01不适用subPath挂载配置文件.html","title":"01不适用subPath挂载配置文件","keywords":"","body":"卷挂载时避免使用subPath 漏洞样例 [control: CVE-2021-25741 - Using symlink for arbitrary host file system access.] failed 😥 Description: A user may be able to create a container with subPath volume mounts to access files & directories outside of the volume, including on the host filesystem. This was affected at the following versions: v1.22.0 - v1.22.1, v1.21.0 - v1.21.4, v1.20.0 - v1.20.10, version v1.19.14 and lower. Node - node1 Namespace champ Deployment - bcp-console Summary - Passed:35 Warning:0 Failed:1 Total:36 Remediation: To mitigate this vulnerability without upgrading kubelet, you can disable the VolumeSubpath feature gate on kubelet and kube-apiserver, and remove any existing Pods making use of the feature. 漏洞描述 用户可以创建一个带有subPath卷挂载的容器来访问卷之外的文件和目录，包括主机文件系统上的文件和目录。 这在以下版本受到影响:v1.22.0 - v1.22.1, v1.21.0 - v1.21.4, v1.20.0 - v1.20.10，版本v1.19.14及更低的版本。 漏洞修复 样例配置: Deployment（只截取了关键部分内容）: kind: Deployment apiVersion: apps/v1 metadata: name: bcp-console namespace: champ labels: app: bcp-console spec: replicas: 1 selector: matchLabels: app: bcp-console template: spec: volumes: - name: bcp-console-cm-volume configMap: name: bcp-console-cm defaultMode: 420 containers: - name: bcp-console image: 'xxx.xxx.xxx/xxx/xxx:xxxx' volumeMounts: - name: bcp-console-cm-volume readOnly: true mountPath: /opt/application.yml subPath: application.yml ConfigMap/bcp-console-cm（部分内容脱敏已删除）: kind: ConfigMap apiVersion: v1 metadata: name: bcp-console-cm namespace: champ labels: app: bcp-console-cm app.kubernetes.io/instance: bcp-console data: application.yml: |- spring: profiles: active: paas 变更Deployment挂载配置方式: 由 volumes: - name: bcp-console-cm-volume configMap: name: bcp-console-cm defaultMode: 420 containers: - name: bcp-console image: 'xxx.xxx.xxx/xxx/xxx:xxxx' volumeMounts: - name: bcp-console-cm-volume readOnly: true mountPath: /opt/application.yml subPath: application.yml 改为 volumes: - name: bcp-console-cm-volume configMap: name: bcp-console-cm defaultMode: 420 containers: - name: bcp-console image: 'xxx.xxx.xxx/xxx/xxx:xxxx' volumeMounts: - name: bcp-console-cm-volume readOnly: true mountPath: /opt 注意： 当ConfigMap只存在一对key value时，key可以设置为文件名称（如: application.yml），value设置为文件内容。 此时执行挂载时，只需指定挂载路径（如mountPath: /opt）， Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/最佳实践/02为容器指定资源配额.html":{"url":"2.容器/k8s/security/最佳实践/02为容器指定资源配额.html","title":"02为容器指定资源配额","keywords":"","body":"Resource policies 漏洞解析 漏洞扫描样例 [control: Resource policies] failed 😥 Description: CPU and memory resources should have a limit set for every container to prevent resource exhaustion. Namespace security Deployment - nginx Summary - Passed:0 Warning:0 Failed:1 Total:1 Remediation: Define LimitRange and ResourceQuota policies to limit resource usage for namespaces or nodes. 描述: 应该为每个容器设置CPU和内存资源的限制，以防止资源耗尽。 加固方案 1.为namespace配置LimitRange 原理描述： 基于命名空间创建全局缺省配额，保证容器存在默认配额，避免异常资源占用（如：死循环导致的高CPU占用）容器影响同一worker节点上其他容器正常运行，进而提升系统整理稳定性。 配置样例: 创建一个namespace: $ kubectl create ns security 为namespace下容器创建缺省配额: $ cat 创建样例应用: $ cat 查看pod: $ kubectl describe pod -n security -l app=nginx |grep -C 3 Requests Limits: cpu: 500m memory: 512Mi Requests: cpu: 200m memory: 256Mi Environment: 证明缺省配额生效。 此时kubescape重新扫描后发现，security依然存在漏洞 [control: Resource policies] failed 😥 Description: CPU and memory resources should have a limit set for every container to prevent resource exhaustion. Namespace security LimitRange - default-limit-range Summary - Passed:1 Warning:0 Failed:1 Total:2 Remediation: Define LimitRange and ResourceQuota policies to limit resource usage for namespaces or nodes. 2.为namespace配置ResourceQuota 原理描述： 基于命名空间创建配额总额，利用ResourceQuota限制命名空间中所有容器的内存请求总量，同样也可以限制内存限制总量、CPU请求总量、CPU限制总量。 配置样例: 为security命名空间创建配额: $ cat ResourceQuota在security命名空间中设置了如下要求： 每个容器必须有内存请求和限制，以及CPU请求和限制。 所有容器的内存请求总和不能超过1 GiB 所有容器的内存限制总和不能超过2 GiB 所有容器的CPU请求总和不能超过1 cpu 所有容器的CPU限制总和不能超过2 cpu 3.为容器添加配额 原理描述： 为不同容器显示配置合适的配额，而不使用缺省值，可以更合理的管理资源 配置样例: 删除前面步骤中创建的Deployment对象 $ kubectl delete -n security Deployment/nginx deployment.apps \"nginx\" deleted 创建样例应用: $ cat 此时kubescape重新扫描后发现，security依然存在漏洞 [control: Resource policies] failed 😥 Description: CPU and memory resources should have a limit set for every container to prevent resource exhaustion. Namespace security LimitRange - default-limit-range Summary - Passed:2 Warning:0 Failed:1 Total:3 Remediation: Define LimitRange and ResourceQuota policies to limit resource usage for namespaces or nodes. 怀疑为kubescape的bug 总结 通过三种方式对资源配额进行加固: LimitRange ResourceQuota 容器的resources字段 其中，LimitRange为必需方案，保证命名空间下的容器有一个缺省配额。ResourceQuota为推荐方案，非必须。 而为每个容器显示设置配额为强烈推荐方案。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/最佳实践/03使用secret存放敏感凭据.html":{"url":"2.容器/k8s/security/最佳实践/03使用secret存放敏感凭据.html","title":"03使用secret存放敏感凭据","keywords":"","body":"使用secret存放敏感凭据 Applications credentials in configuration files扫描异常样例 [control: Applications credentials in configuration files] failed 😥 Description: Attackers who have access to configuration files can steal the stored secrets and use them. Checks if ConfigMaps or pods have sensitive information in configuration. Namespace champ Job - xxl-job-mysql-job Summary - Passed:64 Warning:0 Failed:1 Total:65 Remediation: Use Kubernetes secrets to store credentials. Use ARMO secret protection solution to improve your security even more. 漏洞修复 修改前声明内容: kind: Job apiVersion: batch/v1 metadata: name: xxl-job-mysql-job namespace: champ labels: app.kubernetes.io/managed-by: Helm annotations: meta.helm.sh/release-name: xxl-job-admin meta.helm.sh/release-namespace: champ spec: parallelism: 1 completions: 1 activeDeadlineSeconds: 3000 backoffLimit: 1 selector: matchLabels: controller-uid: 6cca9ff1-731b-492c-b4ca-80c47779c6a7 template: metadata: labels: app: xxl-job-mysql-job controller-uid: 6cca9ff1-731b-492c-b4ca-80c47779c6a7 job-name: xxl-job-mysql-job spec: volumes: - name: volume-spnoig configMap: name: xxl-job-admin-mysql-sql-cm defaultMode: 420 containers: - name: container-xcefoj image: 'mysql:5.7.31' command: - /bin/bash - '-c' args: - >- mysql -hmysql-champ.champ -uroot -P3306 --default-character-set=utf8 对密码进行base64编码 $ echo -n '123456' | base64 MTIzNDU2 根据编码值创建secret对象 $ cat 控制器对象引用 修改前 ... env: - name: MYSQL_ROOT_PASSWORD value: \"123456\" - name: MYSQL_PWD value: \"123456\" 修改后 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: xxljob-init-mysql-secret key: mysql-root-password - name: MYSQL_PWD valueFrom: secretKeyRef: name: xxljob-init-mysql-secret key: mysql-password 重新apply生效 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/最佳实践/04取消服务账号自动挂载.html":{"url":"2.容器/k8s/security/最佳实践/04取消服务账号自动挂载.html","title":"04取消服务账号自动挂载","keywords":"","body":"取消服务账号自动挂载 漏洞解析 漏洞扫描样例 [control: Automatic mapping of service account] failed 😥 Description: Potential attacker may gain access to a POD and steal its service account token. Therefore, it is recommended to disable automatic mapping of the service account tokens in service account configuration and enable it only for PODs that need to use them. Namespace security ServiceAccount - ddd-sa ServiceAccount - default Summary - Passed:4 Warning:0 Failed:2 Total:6 Remediation: Only map token to PODs that are really using them. We suggest disabling the automatic mounting of service account tokens to PODs at the service account level, by specifying the securityContext.readOnlyRootFilesystem field to true, and explicitly enabling the map for the PODs which are using it at the POD spec level. > 描述: 潜在的攻击者可能通过获得`Pod`的访问权并窃取其服务帐户令牌。因此，建议在服务帐户配置中禁用服务帐户令牌的自动映射，只对需要使用它们的`pod`启用它。 ## 加固方案 ### 为ServiceAccount添加取消自动挂载属性 > 1.取消命名空间下默认服务账号自动挂载 ```shell $ kubectl patch -n security sa default -p '{\"automountServiceAccountToken\": false}' 2.取消其他服务账号自动挂载权限 方式同上一步骤 $ kubectl patch -n sa -p '{\"automountServiceAccountToken\": false}' 3.从源头取消服务账号自动挂载权限 创建时 $ cat chart声明时: prometheus/templates/rbac/server-serviceaccount.yaml {{- if .Values.server.enabled -}} {{- if .Values.serviceAccounts.server.create }} apiVersion: v1 kind: ServiceAccount automountServiceAccountToken: false metadata: labels: {{- include \"prometheus.server.labels\" . | nindent 4 }} name: {{ template \"prometheus.serviceAccountName.server\" . }} {{ include \"prometheus.namespace\" . | indent 2 }} annotations: {{ toYaml .Values.serviceAccounts.server.annotations | indent 4 }} {{- end }} {{- end }} Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/最佳实践/05配置合理的CAP.html":{"url":"2.容器/k8s/security/最佳实践/05配置合理的CAP.html","title":"05配置合理的CAP","keywords":"","body":"配置合理的CAP 漏洞解析 扫描样例： [control: Linux hardening] failed 😥 Description: Often, containers are given more privileges than actually needed. This behavior can increase the impact of a container compromise. Namespace security Deployment - nginx Summary - Passed:0 Warning:0 Failed:1 Total:1 Remediation: Make sure you define at least one linux security hardening property out of AppArmor, Seccomp, SELinux or Capabilities. 描述: 如果程序以特权身份运行，应尽量降低其权限。因为很多默认权限/能力程序本身并不需要，其存在可能被攻击者利用。 加固方案 建议DROP掉所有CAP: 基于容器的securityContext.capabilities字段配置 apiVersion: v1 kind: Pod metadata: name: api-server spec: containers: - name: api-server image: xzxwl/api-server-demo:latest securityContext: capabilities: drop: - ALL add: - CHOWN 按需添加 apiVersion: v1 kind: Pod metadata: name: api-server spec: containers: - name: api-server image: xzxwl/api-server-demo:latest securityContext: capabilities: drop: - ALL add: - CHOWN 关于CAP部分解析请参考： cap_chown解析 cap_dac_override解析 cap_fowner解析 cap_fsetid解析 cap_kill解析 cap_setgid解析 cap_setuid解析 cap_net_bind_service解析 cap_sys_chroot解析 cap_mknod解析 cap_audit_write解析 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/安全上下文/":{"url":"2.容器/k8s/security/安全上下文/","title":"安全上下文","keywords":"","body":"k8s安全上下文概述 安全上下文(security context)定义Pod或容器的特权和访问控制设置。安全上下文设置包括但不限于: 自由访问控制: 基于UID、GID文件/目录访问权限控制 安全增强的Linux (SELinux): 给对象分配安全标签 以特权或非特权的方式运行 Linux Capabilities: 赋予进程一些特权，而不是根用户的所有特权 AppArmor: 使用程序配置文件来限制单个程序的权限 Seccomp: 过滤程序系统调用 AllowPrivilegeEscalation(允许提权): 控制进程是否可以获得比其父进程更多的特权。该bool值直接控制是否在容器进程上设置no_new_privs标志。 AllowPrivilegeEscalation总是在容器以特权身份运行或具有CAP_SYS_ADMIN时为真 只读根文件系统: 将容器的根文件系统挂载为只读 完整的安全上下文配置参考SecurityContext 关于Linux中的安全机制的更多信息，参考overview-linux-kernel-security-features Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/安全上下文/01为Pod配置安全上下文.html":{"url":"2.容器/k8s/security/安全上下文/01为Pod配置安全上下文.html","title":"01为Pod配置安全上下文","keywords":"","body":"为Pod配置安全上下文 set-the-security-context-for-a-pod 通过在Pod声明中添加securityContext字段，为Pod指定安全设置。 securityContext字段是一个PodSecurityContext对象。 为Pod指定的安全设置适用于Pod中的所有容器。下面是一个Pod的配置文件，它包含一个securityContext和一个emptyDir卷: apiVersion: v1 kind: Pod metadata: name: security-context-demo spec: securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 volumes: - name: sec-ctx-vol emptyDir: {} containers: - name: sec-ctx-demo image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] volumeMounts: - name: sec-ctx-vol mountPath: /data/demo securityContext: allowPrivilegeEscalation: false 创建Pod kubectl apply -f https://k8s.io/examples/pods/security/security-context.yaml 针对上述配置，说明如下： runAsUser: 1000: 指定Pod中的所有容器内进程UID为1000 runAsGroup: 3000: 指定Pod中的所有容器内进程GID为3000，如果省略该字段GID将为root(0) 验证进程所属用户: $ kubectl exec -it security-context-demo -- sh / $ ps -ef|grep sleep 1 1000 0:00 sleep 1h 23 1000 0:00 grep sleep / $ 当指定runAsGroup时，新建文件权限为：1000:3000。 由于指定了fsGroup字段，因此容器中的volume /data/demo和在该卷中创建的任何文件的所有者将是GID 2000。 / $ ls -l /data/ total 0 drwxrwsrwx 2 root 2000 6 Oct 27 02:35 demo 新建文件，并查看文件权限 /data $ cd demo /data/demo $ echo hello > testfile /data/demo $ ls -l total 4 -rw-r--r-- 1 1000 2000 6 Oct 27 06:34 testfile 查看当前会话用户 / $ id uid=1000 gid=3000 groups=2000 返回值中gid是3000，与runAsGroup字段相同。 如果省略了runAsGroup，则gid将保持为0(根)，并且进程将能够与根(0)组拥有的文件交互，这些文件具有根(0)组所需的组权限。 注意： fsGroup针对emptyDir类型卷生效。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/安全上下文/02为Pods配置卷权限和所有权更改策略.html":{"url":"2.容器/k8s/security/安全上下文/02为Pods配置卷权限和所有权更改策略.html","title":"02为Pods配置卷权限和所有权更改策略","keywords":"","body":"为Pods配置卷权限和所有权更改策略 Configure volume permission and ownership change policy for Pods 特性状态: Kubernetes v1.20 [beta] 默认情况下，Kubernetes递归地更改每个卷内容的所有权和权限，以匹配当挂载该卷时Pod的securityContext中指定的fsGroup。 对于大量数据，检查和更改所有权和权限会花费大量时间，从而减慢Pod的启动。 您可以使用securityContext中的fsGroupChangePolicy字段来控制Kubernetes检查和管理卷的所有权和权限的方式。 fsGroupChangePolicy解析 fsGroupChangePolicy定义了在将卷暴漏给Pod之前更改卷的所有权和权限的行为。 此字段仅适用于支持fsGroup控制的所有权和权限的卷类型。该字段有两个可能的值: OnRootMismatch: 如果根目录的权限和所有权与卷的预期权限不匹配，将更改权限和所有权。这可以帮助缩短更改卷的所有权和许可所需的时间。 Always: 总是在挂载卷时更改卷的权限和所有权 样例 securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 fsGroupChangePolicy: \"OnRootMismatch\" 注意: 该字段对临时卷类型(如secret、configMap和emptydir)没有影响。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/安全上下文/03将卷权限和所有权更改委托给CSI驱动程序.html":{"url":"2.容器/k8s/security/安全上下文/03将卷权限和所有权更改委托给CSI驱动程序.html","title":"03将卷权限和所有权更改委托给CSI驱动程序","keywords":"","body":"将卷权限和所有权更改委托给CSI驱动程序 delegating-volume-permission-and-ownership-change-to-csi-driver 特性状态: Kubernetes v1.22 [alpha] 如果部署了支持VOLUME_MOUNT_GROUP NodeServiceCapability的CSI(Container Storage Interface)驱动， 则基于securityContext中指定的fsGroup来设置文件的归属和权限的过程将由CSI驱动来完成，而不是Kubernetes。 前提是启用了DelegateFSGroupToCSIDriver Kubernetes特性门控。 在本例中，由于Kubernetes没有执行任何所有权和权限更改，因此fsGroupChangePolicy不会生效，并且正如CSI所指定的那样， 驱动程序将使用提供的fsGroup挂载卷，从而产生一个fsGroup可读/可写的卷。 请参阅KEP 和VolumeCapability.MountVolume的描述。更多信息请参见CSI规范 中的volume_mount_group字段。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/安全上下文/04为容器设置安全上下文.html":{"url":"2.容器/k8s/security/安全上下文/04为容器设置安全上下文.html","title":"04为容器设置安全上下文","keywords":"","body":"为容器设置安全上下文 Set the security context for a Container 要为容器指定安全设置，需要在容器清单中添加securityContext字段。 securityContext字段是一个securityContext 对象 为容器指定的安全设置仅应用于单个容器，当存在重叠时，它们将覆盖在Pod级别进行的设置。容器设置不影响Pod的卷。 下面是具有一个容器的Pod的配置文件。Pod和容器都有一个securityContext字段: apiVersion: v1 kind: Pod metadata: name: security-context-demo-2 spec: securityContext: runAsUser: 1000 containers: - name: sec-ctx-demo-2 image: gcr.io/google-samples/node-hello:1.0 securityContext: runAsUser: 2000 allowPrivilegeEscalation: false 创建这个Pod: kubectl apply -f https://k8s.io/examples/pods/security/security-context-2.yaml 验证Pod的容器正在运行: $ kubectl get pod security-context-demo-2 在运行的容器中获取一个shell: $ kubectl exec -it security-context-demo-2 -- sh 在你的shell中，列出正在运行的进程: $ ps aux 输出显示进程以用户2000的身份运行。这是为容器指定的runAsUser的值。它覆盖为Pod指定的值1000。 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND 2000 1 0.0 0.0 4336 764 ? Ss 20:36 0:00 /bin/sh -c node server.js 2000 8 0.1 0.5 772124 22604 ? Sl 20:36 0:00 node server.js ... Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/安全上下文/05设置容器的capabilities.html":{"url":"2.容器/k8s/security/安全上下文/05设置容器的capabilities.html","title":"05设置容器的capabilities","keywords":"","body":"Linux CAP介绍与k8s下配置使用 关于capability 发音 美[keɪpəˈbɪləti] 英[keɪpə'bɪləti] 译为能力或功能，一般缩写CAP，以下我们简称Capabilities为CAP CAP历史回溯 从内核2.2开始，Linux将传统上与超级用户root关联的特权划分为不同的单元，称为CAP。 CAP作为线程(Linux并不真正区分进程和线程)的属性存在，每个单元可以独立启用和禁用。 如此一来，权限检查的过程就变成了： 在执行特权操作时，如果进程的有效身份不是root，就去检查是否具有该特权操作所对应的CAP，并以此决定是否可以进行该特权操作。 比如要向进程发送信号(kill())，就得具有CAP_KILL；如果设置系统时间，就得具有CAP_SYS_TIME。 在CAP出现之前，系统进程分为两种： 特权进程 非特权进程 特权进程可以做所有的事情: 进行管理级别的内核调用；而非特权进程被限制为标准用户的子集调用 某些可执行文件需要由标准用户运行，但也需要进行有特权的内核调用，它们需要设置suid位，从而有效地授予它们特权访问权限。(典型的例子是ping，它被授予进行ICMP调用的完全特权访问权。) 这些可执行文件是黑客关注的主要目标——如果他们可以利用其中的漏洞，他们就可以在系统上升级他们的特权级别。 由此内核开发人员提出了一个更微妙的解决方案:CAP。 意图很简单: 将所有可能的特权内核调用划分为相关功能组，赋予进程所需要的功能子集。 因此，内核调用被划分为几十个不同的类别，在很大程度上是成功的。 回到ping的例子，CAP的出现使得它仅被赋予一个CAP_NET_RAW功能，就能实现所需功能，这大大降低了安全风险。 注意： 比较老的操作系统上，会通过为ping添加SUID权限的方式，实现普通用户可使用。 这存在很大的安全隐患，笔者所用操作系统（CentOS7）上ping指令已通过CAP方式实现 $ ls -l /usr/bin/ping -rwxr-xr-x. 1 root root 66176 8月 4 2017 /usr/bin/ping $ getcap /usr/bin/ping /usr/bin/ping = cap_net_admin,cap_net_raw+p 设置容器的CAP Set capabilities for a Container 基于Linux capabilities ，您可以授予某个进程某些特权，而不授予root用户的所有特权。 要为容器添加或删除Linux功能，请在容器清单的securityContext部分中包含capability字段。 首先，看看未设置capability字段时会发生什么。下面是不添加或删除任何CAP的配置文件: apiVersion: v1 kind: Pod metadata: name: security-context-demo-3 spec: containers: - name: sec-ctx-3 image: centos:7 command: [\"tail\",\"-f\", \"/dev/null\"] 创建Pod $ kubectl apply -f https://k8s.io/examples/pods/security/security-context-3.yaml 查看Pod运行状态 $ kubectl get pod security-context-demo-3 在运行的容器中获取一个shell: kubectl exec -it security-context-demo-3 -- sh 在shell中，列出正在运行的进程: $ ps aux 输出显示了容器的进程id (pid): USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 4336 796 ? Ss 18:17 0:00 /bin/sh -c node server.js root 5 0.1 0.5 772124 22700 ? Sl 18:17 0:00 node server.js 在shell中，查看进程1的状态: $ cd /proc/1 $ cat status 输出显示了进程的能力位图: ... CapPrm: 00000000a80425fb CapEff: 00000000a80425fb ... 解码 $ capsh --decode=00000000a80425fb 0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap 记下能力位图，然后退出shell: $ exit 接下来，运行一个与前一个容器相同的容器，只是它有额外的功能集。 运行一个配置增加了CAP_NET_ADMIN和CAP_SYS_TIME功能的Pod: cat 在运行的容器中获取一个shell: kubectl exec -it security-context-demo-4 -- sh 在shell中，查看进程1的状态: $ cd /proc/1 $ cat status 进程的能力位图: ... CapPrm: 00000000aa0435fb CapEff: 00000000aa0435fb ... 进程的能力位图值解码 $ capsh --decode=00000000aa0435fb 0x00000000aa0435fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_admin,cap_net_raw,cap_sys_chroot,cap_sys_time,cap_mknod,cap_audit_write,cap_setfcap 对比两个进程的能力位图（解码后） # 未配置CAP 0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap # 配置CAP_NET_ADMIN和CAP_SYS_TIME 0x00000000aa0435fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_admin,cap_net_raw,cap_sys_chroot,cap_sys_time,cap_mknod,cap_audit_write,cap_setfcap 有关常capability数的定义，请参阅capability.h 。 注意: Linux capability常量的形式是CAP_XXX。 但是，当您在容器清单中列出功能时，必须忽略常量的CAP_部分。 例如，要添加CAP_SYS_TIME，请在功能列表中包含SYS_TIME。 关于进程状态值 这里我们介绍进程状态中与Capabilities相关的几个值: CapInh: 当前进程子进程可继承的能力 CapPrm: 当前进程可使用的能力（可以包含CapEff中没有的能力，CapEff是CapPrm的一个子集，进程放弃没有必要的能力有利于提高安全性） CapEff: 当前进程已使用/开启的能力 1.非容器特权进程CAP缺省值解析（共计35个） $ capsh --decode=000001ffffffffff 0x000001ffffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,35,36,37,38,39,40 cap_chown: 允许修改文件所有者权限 cap_dac_override: 忽略文件的DAC访问权限 cap_dac_read_search: 忽略文件读及目录检索的DAC访问权限 cap_fowner: 忽略文件属主ID必须与进程用户ID一致的权限 cap_fsetid: 允许设置文件setuid位的权限 cap_kill: 允许对不属于自己的进程发送信号的权限 cap_setgid: 允许修改进程的GID权限 cap_setuid: 允许修改进程的UID权限 cap_setpcap: 允许对子进程进行CAP授权 cap_linux_immutable: 允许修改文件的IMMUTABLE与APPEND属性权限 cap_net_bind_service: 允许绑定小于1024端口的权限 cap_net_broadcast: 允许网络广播及多播访问的权限 cap_net_admin: 允许执行网络管理任务的权限 cap_net_raw: 允许使用原始套接字的权限 cap_ipc_lock: 允许锁定共享内存片段的权限 cap_ipc_owner: 忽略IPC所有权检查的权限 cap_sys_module: 允许插入和删除内核模块的权限 cap_sys_rawio: 允许直接访问/devport,/dev/mem,/dev/kmem及原始块设备的权限 cap_sys_chroot: 允许使用chroot()系统调用的权限 cap_sys_ptrace: 允许追踪任何进程的权限 cap_sys_pacct: 允许执行进程的BSD式审计的权限 cap_sys_admin: 允许执行系统管理任务(如加载或卸载文件系统、设置磁盘配额等)的权限 cap_sys_boot: 允许重启系统的权限 cap_sys_nice: 允许提升优先级及设置其他进程优先级的权限 cap_sys_resource: 忽略资源限制的权限 cap_sys_time: 允许改变系统时钟的权限 cap_sys_tty_config: 允许配置TTY设备的权限 cap_mknod: 允许使用mknod()系统调用的权限 cap_lease: 允许修改文件锁的FL_LEASE标志的权限 cap_audit_write: 允许将记录写入内核审计日志的权限 cap_audit_control: 启用和禁用内核审计、改变审计过滤规则、检索审计状态和过滤规则的权限 cap_setfcap: 允许为可执行文件设置CAP的权限 cap_mac_override: 可覆盖Mandatory Access Control(MAC)的权限 cap_mac_admin: 允许MAC配置或状态改变的权限 cap_syslog: 允许使用syslog()系统调用的权限 2.容器特权进程默认CAP缺省值解析（共计14个） 借用上述例子中未配置CAP的进程能力位图 0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap cap_chown: 允许修改文件所有者权限 cap_dac_override: 忽略文件的DAC访问权限 cap_fowner: 忽略文件属主ID必须与进程用户ID一致的权限 cap_fsetid: 允许设置文件setuid位的权限 cap_kill: 允许对不属于自己的进程发送信号的权限 cap_setgid: 允许修改进程的GID权限 cap_setuid: 允许修改进程的UID权限 cap_setpcap: 允许对子进程进行CAP授权 cap_net_bind_service: 允许绑定小于1024端口的权限 cap_net_raw: 允许使用原始套接字的权限 cap_sys_chroot: 允许使用chroot()系统调用的权限 cap_mknod: 允许使用mknod()系统调用的权限 cap_audit_write: 允许将记录写入内核审计日志的权限 cap_setfcap: 允许为可执行文件设置CAP的权限 对比发现，容器运行时内的root用户并非拥有全部权限，仅仅是默认拥有14条权限，其他权限如果使用需要额外开启。 3.查看容器非特权进程默认CAP缺省值（0个） $ id uid=1000 gid=0(root) groups=0(root) $ cat /proc/1/status|grep CapEff CapEff: 0000000000000000 思考一个问题: 当运行时为非特权用户，CAP配置是否生效？ Deployment配置如下（镜像以非特权USER运行） kind: Deployment apiVersion: apps/v1 metadata: name: eureka-app namespace: champ labels: app: eureka-app app.kubernetes.io/instance: eureka-app annotations: configmap.reloader.stakater.com/reload: eureka-app-cm deployment.kubernetes.io/revision: '25' spec: replicas: 1 selector: matchLabels: app: eureka-app template: metadata: labels: app: eureka-app annotations: kubesphere.io/containerSecrets: '' spec: containers: - name: eureka-app image: 'xxx.xxx.xxx/xxx/xxx:xxx' ports: - name: http-8080 containerPort: 8080 protocol: TCP - name: http-5005 containerPort: 5005 protocol: TCP securityContext: capabilities: add: - SYS_TIME ... 查看进程状态 $ cat /proc/1/status CapPrm: 0000000000000000 CapEff: 0000000000000000 显然当镜像指定USER为非特权用户运行时，CAP配置并不生效 结论 当镜像指定USER为非特权用户运行时，CAP配置并不生效 容器内特权进程默认拥有14条CAP权限配置，相对非容器特权进程要少的多 Linux CAP旨在将特权细粒度划分 参考文献 Linux Capabilities 简介 Linux Capabilities: Why They Exist and How They Work Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/01cap_chown解析.html":{"url":"2.容器/k8s/security/容器CAP解析/01cap_chown解析.html","title":"01cap_chown解析","keywords":"","body":"cap_chown权限分析 cap_chown: 允许修改文件所有者 那么让我们基于k8s，透过下面几个例子来验证cap_chown的功能 1.容器以root用户运行且使用默认CAP时（14个CAP） cat 测试chown可用性 $ kubectl exec -it api-server -- sh /work # touch 111 /work # ls -l total 0 -rw-r--r-- 1 root root 0 Nov 3 08:55 111 /work # chown 1000:1000 111 /work # ls -l total 0 -rw-r--r-- 1 1000 1000 0 Nov 3 08:55 111 /work # exit 清理测试资源 $ kubectl delete pod api-server 2.容器以root用户运行且取消所有Linux CAP时 cat 测试chown可用性 $ kubectl exec -it api-server -- sh /work # touch 111 /work # ls -l total 0 -rw-r--r-- 1 root root 0 Nov 3 08:55 111 /work # chown 1000:1000 111 chown: 111: Operation not permitted /work # exit 清理测试资源 $ kubectl delete pod api-server 3.容器以root用户运行且取消所有Linux CAP，只添加CAP_CHOWN时 cat 测试chown可用性 $ kubectl exec -it api-server -- sh /work # touch 111 /work # ls -l total 0 -rw-r--r-- 1 root root 0 Nov 3 08:55 111 /work # chown 1000:1000 111 /work # ls -l total 0 -rw-r--r-- 1 1000 1000 0 Nov 4 01:48 111 /work # exit 清理测试资源 $ kubectl delete pod api-server Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/02cap_dac_override解析.html":{"url":"2.容器/k8s/security/容器CAP解析/02cap_dac_override解析.html","title":"02cap_dac_override解析","keywords":"","body":"cap_dac_override能力分析 cap_dac_override: 忽略对文件的DAC访问权限限制 首先我们需要了解：DAC是什么？ DAC全称Discretinoary Access Control，即自主访问控制。 它是传统的Linux访问控制方式。资源所有者负责管理访问控制权限，并通过ACL(Acess Control List)管理非所有者权限。 例： $ ls -l /etc/passwd -rw-r--r-- 1 root root 988 Mar 18 2021 /etc/passwd 即通过-rw-r--r--控制用户对/etc/passwd的访问 cap_dac_override能做什么？ 赋予进程/可执行文件cap_dac_override后，可无视DAC访问权限限制 那么让我们基于CentOS7，透过下面例子来验证cap_dac_override的功能： 创建用户test，用于测试 $ adduser test 切换至test用户尝试向/etc/passwd追加内容 $ su - test $ vim /etc/passwd $ exit 写入数据后并不能保存，显然test不具备对/etc/passwd的写权限 $ ls -l /etc/passwd -rw-r--r-- 1 root root 1223 Nov 4 02:16 /etc/passwd 对vim添加cap_dac_override的功能 $ setcap cap_dac_override=eip /usr/bin/vim 再次切换至test用户尝试向/etc/passwd追加内容 $ su - test $ vim /etc/passwd 写入以下内容保存(:wq!)，保存成功 ddd:x:1001:1001::/home/ddd:/bin/bash $ cat /etc/passwd bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin adm:x:3:4:adm:/var/adm:/sbin/nologin lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin sync:x:5:0:sync:/sbin:/bin/sync shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown halt:x:7:0:halt:/sbin:/sbin/halt mail:x:8:12:mail:/var/spool/mail:/sbin/nologin operator:x:11:0:operator:/root:/sbin/nologin games:x:12:100:games:/usr/games:/sbin/nologin ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin nobody:x:99:99:Nobody:/:/sbin/nologin systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin dbus:x:81:81:System message bus:/:/sbin/nologin polkitd:x:999:997:User for polkitd:/:/sbin/nologin postfix:x:89:89::/var/spool/postfix:/sbin/nologin chrony:x:998:996::/var/lib/chrony:/sbin/nologin sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin test:x:1000:1000::/home/test:/bin/bash ddd:x:1001:1001::/home/ddd:/bin/bash 查看vim的CAP $ getcap /usr/bin/vim /usr/bin/vim = cap_dac_override+eip 清理vim的CAP $ setcap -r /usr/bin/vim 清理测试用例 $ userdel -r test $ sed -i '/ddd/d' /etc/passwd Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/03cap_fowner解析.html":{"url":"2.容器/k8s/security/容器CAP解析/03cap_fowner解析.html","title":"03cap_fowner解析","keywords":"","body":"cap_fowner能力分析 cap_fowner: 忽略文件属主ID必须与进程用户ID一致 简单来说就是：当进程/可执行文件拥有cap_fowner能力时， 如果用户A(非特权用户)对文件F（属主为root）执行写入后，该文件的属主会变为A。 那么让我们基于CentOS7，透过下面例子来理解cap_fowner的功能： 创建用户test，用于测试 $ adduser test 使用root身份创建一个文件 $ touch /home/test/ddd $ ls -l /home/test/ddd -rw-r--r-- 1 root root 0 Nov 4 13:52 /home/test/ddd 对vim添加cap_fowner的功能 $ setcap cap_fowner=eip /usr/bin/vim 切换至test用户对/home/test/ddd编辑写入 $ su - test $ vim /home/test/ddd $ exit 写入数据后:wq!保存退出，查看此时文件属主（已变为test） $ ls -l /home/test/ddd -rw-r--r-- 1 test test 16 Nov 4 13:54 /home/test/ddd 查看vim的CAP $ getcap /usr/bin/vim /usr/bin/vim = cap_fowner+eip 清理vim的CAP $ setcap -r /usr/bin/vim 清理测试用例 $ userdel -r test Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/04cap_fsetid解析.html":{"url":"2.容器/k8s/security/容器CAP解析/04cap_fsetid解析.html","title":"04cap_fsetid解析","keywords":"","body":"cap_fsetid能力分析 cap_fsetid: 确保在文件被修改后不修改setuid/setgid位 简单来说就是：当进程/可执行文件拥有cap_fsetid能力时，当用户对文件F（含有setuid/setgid位）执行写操作后，该文件的setuid/setgid位不会发生变化。 首先我们先了解下什么是setuid/setgid位 SUID是什么？ 我们先看一下不带setuid位的文件权限 $ touch /tmp/ddd $ ls -l /tmp/ddd -rw-r--r-- 1 root root 0 Nov 4 14:29 /tmp/ddd -rw-r--r--中的第一位-表示/tmp/ddd类型为文件 -rw-r--r--中的第2-4位rw-表示/tmp/ddd文件属主拥有的权限为: 读写 -rw-r--r--中的第5-7位r--表示/tmp/ddd文件所属用户组（root）下其他用户对其拥有的权限为: 读 -rw-r--r--中的第5-7位r--表示其他用户组（非root）下用户对其拥有的权限为: 读 第一个root表示该文件属主为root 第二个root表示该文件所属用户组为root 当我们对其追加suid/时： $ chmod u+s /tmp/ddd $ ls -l /tmp/ddd -rwSr--r-- 1 root root 0 Nov 4 14:49 /tmp/ddd 当我们再追加+x可执行权限时，S变为了s $ chmod u+x /tmp/ddd $ ls -l /tmp/ddd -rwsr--r-- 1 root root 0 Nov 4 14:49 /tmp/ddd setuid的使用场景为：对归属root的程序/可执行文件（二进制）进行setuid，普通用户运行该程序时，是以程序所属的用户的身份(root)运行。 SGID是什么？ SGID和SUID的不同之处就在于，SUID赋予用户的是文件所有者的权限，而SGID赋予用户的是文件所属组的权限。 对比一下： 设置SUID的文件权限 $ ls -l /tmp/ddd -rwsr--r-- 1 root root 0 Nov 4 14:49 /tmp/ddd 设置SGID的文件权限 $ ls -l /tmp/ddd -rw-r-sr-- 1 root root 0 Nov 4 14:53 /tmp/ddd cap_fsetid应用样例 那么让我们基于CentOS7，透过下面两个例子来理解cap_fsetid的功能： 测试不设置cap_fsetid情况下对含有SUID/SGID位的文件进行修改 创建用户test，用于测试 $ adduser test 使用root身份创建一个文件 $ touch /tmp/123 $ chmod 6777 /tmp/123 $ ls -l /tmp/123 -rwsrwsrwx 1 root root 0 Nov 4 15:02 /tmp/123 切换至test用户对/tmp/123编辑写入 $ su - test $ vim /tmp/123 $ ls -l /tmp/123 -rwxrwxrwx 1 root root 5 Nov 4 15:03 /tmp/123 此时我们发现，在对/tmp/123写入后，文件权限已然发生变化。（无SUID/SGID） 清理测试用例 $ rm -f /tmp/123 $ userdel -r test 测试设置cap_fsetid情况下对含有SUID/SGID位的文件进行修改 创建用户test，用于测试 $ adduser test 使用root身份创建一个文件 $ touch /tmp/123 $ chmod 6777 /tmp/123 $ ls -l /tmp/123 -rwsrwsrwx 1 root root 0 Nov 4 15:02 /tmp/123 对vim添加cap_fsetid的功能 $ setcap cap_fsetid=eip /usr/bin/vim 切换至test用户对/tmp/123编辑写入 $ su - test $ vim /tmp/123 $ ls -l /tmp/123 -rwsrwsrwx 1 root root 24 Nov 4 15:07 /tmp/123 此时我们发现，在对/tmp/123写入后，文件权限并未发生变化。 清理测试用例 $ userdel -r test $ setcap -r /usr/bin/vim Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/05cap_kill解析.html":{"url":"2.容器/k8s/security/容器CAP解析/05cap_kill解析.html","title":"05cap_kill解析","keywords":"","body":"cap_kill能力分析 cap_kill: 允许对不属于自己的进程发送信号 举个简单的例子： 默认情况下普通用户是不能kill根用户进程的，但如果我们赋予/bin/kill以cap_kill能力后，普通用户就会拥有kill其他用户（包括root）进程的权限。 让我们通过下面的例子加深理解 创建用户test，用于测试 $ adduser test 以root身份运行一个进程 $ python -m SimpleHTTPServer 9099 新建会话，获取进程Pid，并以test身份kill掉 $ su - test $ /bin/kill -9 `ps -ef|grep \"python -m SimpleHTTPServer 9099\" | grep -v grep|awk '{print $2}'` kill: sending signal to 62220 failed: Operation not permitted 显然不具备权限 切换至root用户对kill指令添加CAP_KILL能力 $ setcap cap_kill=eip /bin/kill 切换至test用户再次执行kill $ su - test $ /bin/kill -9 `ps -ef|grep \"python -m SimpleHTTPServer 9099\" | grep -v grep|awk '{print $2}'` 此时test用户kill掉了属主为root的进程 注意： 必须为/bin/kill绝对路径引用，否则不生效 清理测试用例 $ setcap -r /bin/kill $ userdel -r test Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/06cap_setgid解析.html":{"url":"2.容器/k8s/security/容器CAP解析/06cap_setgid解析.html","title":"06cap_setgid解析","keywords":"","body":"cap_setgid能力分析 cap_setgid: 允许普通用户使用setgid函数 让我们通过下面的例子了解cap_setgid 创建用户test，用于测试 $ adduser test root身份创建文件/tmp/ddd $ whoami root $ echo \"123\" > /tmp/ddd $ chmod 640 /tmp/ddd 编写一个c程序来使用setgid()函数 函数说明：setgid(gid)用来将目前进程的真实组识别码(real gid)设成参数gid值. 如果是以超级用户身份执行此调用, 则real、effective与savedgid都会设成参数gid $ su - test $ tee ~/demo.c int main () { gid_t gid = 0; setgid(gid); system(\"/bin/cat /tmp/ddd\"); return 0; } EOF 以test用户执行demo程序 $ whoami test $ gcc demo.c -o demo $ ./demo /bin/cat: /tmp/ddd: Permission denied 显然默认情况下不具备调用setgid()函数的权限 切换root用户，授予/home/test/demo以CAP_SETGID能力 $ whoami root $ setcap cap_setgid=eip /home/test/demo 切换至test用户再次执行/home/test/demo $ whoami test $ /home/test/demo 123 此时test用户通过执行demo程序，拥有了读取/tmp/ddd文件内容的权限，而test用户依旧不具备该权限: $ whoami test $ cat /tmp/ddd cat: /tmp/ddd: Permission denied 这是因为通过对/home/test/demo程序授予了cap_setgid的能力，允许程序可以使用setgid()函数。而通过setgid()函数，/home/test/demo修改了进程所属组到root（修改前为test） 进而拥有了对/tmp/ddd文件的读权限。 清理测试用例 $ userdel -r test Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/07cap_setuid解析.html":{"url":"2.容器/k8s/security/容器CAP解析/07cap_setuid解析.html","title":"07cap_setuid解析","keywords":"","body":"cap_setuid能力分析 cap_setuid: 允许普通用户使用setuid函数 让我们通过下面的例子了解cap_setuid 创建用户test，用于测试 $ adduser test root身份创建文件/tmp/ddd $ whoami root $ echo \"123\" > /tmp/ddd $ chmod 640 /tmp/ddd 编写一个c程序来使用setuid()函数 函数说明：setuid()用来重新设置执行目前进程的用户识别码。 不过, 要让此函数有作用, 其有效的用户识别码必须为0(root). 在Linux下, 当root使用setuid()来变换成其他用户识别码时, root权限会被抛弃, 完全转换成该用户身份。 也就是说, 该进程往后将不再具有可setuid()的权利, 如果只是想暂时抛弃root权限, 稍后想重新取回权限, 则必须使用seteuid(). $ su - test $ tee ~/demo.c int main () { gid_t uid = 0; setuid(uid); system(\"/bin/cat /tmp/ddd\"); return 0; } EOF 以test用户执行demo程序 $ whoami test $ gcc demo.c -o demo $ ./demo /bin/cat: /tmp/ddd: Permission denied 显然默认情况下不具备调用setuid()函数的权限 切换root用户，授予/home/test/demo以CAP_SETUID能力 $ whoami root $ setcap cap_setuid=eip /home/test/demo 切换至test用户再次执行/home/test/demo $ whoami test $ /home/test/demo 123 此时test用户通过执行demo程序，拥有了读取/tmp/ddd文件内容的权限，而test用户依旧不具备该权限: $ whoami test $ cat /tmp/ddd cat: /tmp/ddd: Permission denied 这是因为通过对/home/test/demo程序授予了cap_setuid的能力，允许程序可以使用setuid()函数。而通过setuid()函数，/home/test/demo修改了进程属主为root（修改前为test） 进而拥有了对/tmp/ddd文件的读权限。 清理测试用例 $ userdel -r test Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/09cap_net_bind_service解析.html":{"url":"2.容器/k8s/security/容器CAP解析/09cap_net_bind_service解析.html","title":"09cap_net_bind_service解析","keywords":"","body":"cap_net_bind_service能力分析 cap_net_bind_service: 允许绑定小于1024端口 让我们通过下面的例子了解cap_net_bind_service 创建用户test，用于测试 $ adduser test 切换至test身份，编写测试程序 $ su - test $ cat > ~/ddd.c #include #include #include #include #include #include #include #include #include #include #include #define SERVER_PORT 80 /* 监听后，一直处于accept阻塞状态， 直到有客户端连接， 当客户端如数quit后，断开与客户端的连接 */ int main() { //调用socket函数返回的文件描述符 int serverSocket; //声明两个套接字sockaddr_in结构体变量，分别表示客户端和服务器 struct sockaddr_in server_addr; struct sockaddr_in clientAddr; int addr_len = sizeof(clientAddr); int client; char buffer[200]; int iDataNum; //socket函数，失败返回-1 //int socket(int domain, int type, int protocol); //第一个参数表示使用的地址类型，一般都是ipv4，AF_INET //第二个参数表示套接字类型：tcp：面向连接的稳定数据传输SOCK_STREAM //第三个参数设置为0 if((serverSocket = socket(AF_INET, SOCK_STREAM, 0)) 编译测试程序 $ whoami test $ gcc ddd.c -o ddd 以test身份调用ddd开启监听80端口 $ whoami test $ ./ddd connect: Permission denied 显然默认情况下，普通用户无法监听1~1023端口 切换至root用户授予/home/test/ddd以cap_net_bind_service $ whoami root $ setcap cap_net_bind_service=eip /home/test/ddd 切换至test用户，再次执行 $ whoami test $ ./ddd Listening on port: 80 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/11cap_sys_chroot解析.html":{"url":"2.容器/k8s/security/容器CAP解析/11cap_sys_chroot解析.html","title":"11cap_sys_chroot解析","keywords":"","body":"cap_chroot能力分析 cap_sys_chroot: 允许使用chroot()系统调用 chroot指令是其具体实现,我们首先了解下chroot指令 chroot能用来干什么？ 首先了解下chroot的含义：change root directory 默认情况下linux的目录均以'/'起始，如: /etc /usr/local/sbin /root 而chroot则可以变更文件系统的根，比如可以通过 chroot大致功能如下： 通过变更根目录，增加了系统的安全性，限制了用户的权力： 在经过chroot之后，在新的根下将访问不到旧系统的根目录结构和文件，这样就增强了系统的安全性。 一般会在用户登录前应用chroot，把用户的访问能力控制在一定的范围之内。 建立一个与原系统隔离的系统目录结构，方便用户的开发 使用chroot后，系统读取的是新根下的目录和文件，这是一个与原系统根下文件不相关的目录结构。在这个新的环境中，可以用来测试软件的静态编译以及一些与系统不相关的独立开发。 切换系统的根目录位置，引导Linux系统启动以及急救系统等： chroot的作用就是切换系统的根位置，而这个作用最为明显的是在系统初始引导磁盘的处理过程中使用，从初始RAM磁盘 (initrd) 切换系统的根位置并执行真正的init 接下来我们可以通过以下两个例子，进一步了解chroot chroot实践1: 重置root口令 比较常用的一个例子就是用来重置root口令(忘记root口令场景，也可通过单用户模式重置) 下来我们简单演示下具体流程 开机/重启操作系统，进入引导流程 选择内核界面，键入e对引导逻辑执行编辑 找到下面框住的那行 在行末尾（en_US.UTF-8后）添加rd.break，注意空格间隔 键入ctrl+x组合键保存修改内容，进入switch_root模式 查看挂载点 框住的内容是我们进行下一步所需要的信息: 第一个框说明此时的根目录在一个RAM disk中, 即rootfs 第二个框说明当前文件系统挂载于/sysroot目录，并且是只读的模式 修改/sysroot挂载点为读写模式 switch_root:/# mount -o remount,rw /sysroot 利用chroot切换根目录至/sysroot switch_root:/# chroot /sysroot 重置root口令 sh-4.2# echo \"new_root_pw\" | passwd --stdin root 创建/.autorelabel文件，确保开机时重新设定SELinux context（selinux标签验证,即允许你修改密码） sh-4.2# touch /.autorelabel 注意： 这一步骤很关键，如果修改密码后不执行这一步骤，重启操作系统后将不会登录成功 退出chroot sh-4.2# exit 再次执行退出，系统会自动重新开机 switch_root:/# exit chroot实践2: 打造一个ssh监狱 创建ssh登陆后的活动范围 $ mkdir -p /home/ssh 接下来，根据sshd_config手册找到所需的文件，ChrootDirectory选项指定在身份验证后要chroot到的目录的路径名。 该目录必须包含支持用户会话所必需的文件和目录 $ ls -l /dev/{null,zero,stdin,stdout,stderr,random,tty} crw-rw-rw-. 1 root root 1, 3 Nov 8 02:08 /dev/null crw-rw-rw-. 1 root root 1, 8 Nov 8 02:08 /dev/random lrwxrwxrwx. 1 root root 15 Nov 8 02:08 /dev/stderr -> /proc/self/fd/2 lrwxrwxrwx. 1 root root 15 Nov 8 02:08 /dev/stdin -> /proc/self/fd/0 lrwxrwxrwx. 1 root root 15 Nov 8 02:08 /dev/stdout -> /proc/self/fd/1 crw-rw-rw-. 1 root tty 5, 0 Nov 8 02:08 /dev/tty crw-rw-rw-. 1 root root 1, 5 Nov 8 02:08 /dev/zero 对于交互式会话，这需要至少一个shell，通常为sh和基本的/dev节点，例如null、zero、stdin、stdout、stderr和tty设备 通过mknod命令创建/dev下的文件 在下面的命令中，-m标志用来指定文件权限位，c意思是字符文件，两个数字分别是文件指向的主要号和次要号 $ mkdir -p /home/ssh/dev $ cd /home/ssh/dev $ mknod -m 666 null c 1 3 $ mknod -m 666 tty c 5 0 $ mknod -m 666 zero c 1 5 $ mknod -m 666 random c 1 8 在chroot监狱中设置合适的权限 注意: chroot监狱和它的子目录以及子文件必须被root用户所有，并且对普通用户或用户组不可 $ chown root:root /home/ssh $ chmod 0755 /home/ssh $ ls -ld /home/ssh drwxr-xr-x. 3 root root 17 Nov 8 02:28 /home/ssh 为SSH chroot监狱设置交互式shell 首先，创建bin目录并复制/bin/bash到bin中 $ mkdir -p /home/ssh/bin $ cp -v /bin/bash /home/ssh/bin 接下来获取bash所需的共享库，并复制它们到lib64中： $ ldd /bin/bash linux-vdso.so.1 => (0x00007fffc9f70000) libtinfo.so.5 => /lib64/libtinfo.so.5 (0x00007f541ac98000) libdl.so.2 => /lib64/libdl.so.2 (0x00007f541aa94000) libc.so.6 => /lib64/libc.so.6 (0x00007f541a6c6000) /lib64/ld-linux-x86-64.so.2 (0x00007f541aec2000) $ mkdir -p /home/ssh/lib64 $ cp -v /lib64/{libtinfo.so.5,libdl.so.2,libc.so.6,ld-linux-x86-64.so.2} /home/ssh/lib64 创建SSH用户，并初始化密码 $ useradd -m ddd && echo \"123456\" | passwd --stdin ddd 创建chroot监狱通用配置目录/home/ssh/etc并复制已更新的账号文件（/etc/passwd和/etc/group）到这个目录中 $ mkdir /home/ssh/etc $ cp -vf /etc/{passwd,group} /home/ssh/etc/ 配置SSH来使用chroot监狱 $ cat >> /etc/ssh/sshd_config 测试SSH的chroot监狱 新建ssh会话，会话信息如下： 用户: ddd 密码: 123456 尝试执行一些命令： -bash-4.2$ ls -bash: ls: command not found -bash-4.2$ pwd / -bash-4.2$ clear -bash: clear: command not found 从结果来看，我们可以看到ddd用户被锁定在了chroot监狱中，并且不能使用任何外部命令如（ls、date、uname等等）。 用户只可以执行bash以及它内置的命令（比如：pwd、history、echo等等） 创建用户的主目录并添加Linux命令 从前面的步骤中，我们可以看到用户被锁定在了root目录，我们可以为SSH用户创建一个主目录（以及为所有将来的用户这么做）： $ mkdir -p /home/ssh/home/ddd $ chown -R ddd:ddd /home/ssh/home/ddd $ chmod -R 0700 /home/ssh/home/ddd 添加几个指令 $ cp -v /bin/{ls,mkdir} /home/ssh/bin 将指令的共享库拷贝至chroot监狱中 ls $ ldd /usr/bin/ls linux-vdso.so.1 => (0x00007ffdafdf5000) libselinux.so.1 => /lib64/libselinux.so.1 (0x00007f817932d000) libcap.so.2 => /lib64/libcap.so.2 (0x00007f8179128000) libacl.so.1 => /lib64/libacl.so.1 (0x00007f8178f1f000) libc.so.6 => /lib64/libc.so.6 (0x00007f8178b51000) libpcre.so.1 => /lib64/libpcre.so.1 (0x00007f81788ef000) libdl.so.2 => /lib64/libdl.so.2 (0x00007f81786eb000) /lib64/ld-linux-x86-64.so.2 (0x00007f8179554000) libattr.so.1 => /lib64/libattr.so.1 (0x00007f81784e6000) libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f81782ca000) $ \\cp -v /lib64/{libselinux.so.1,libcap.so.2,libacl.so.1,libc.so.6,libpcre.so.1,libdl.so.2,ld-linux-x86-64.so.2,libattr.so.1,libpthread.so.0} /home/ssh/lib64 mkdir $ ldd /usr/bin/mkdir $ \\cp /lib64/{libselinux.so.1,libc.so.6,libpcre.so.1,libdl.so.2,ld-linux-x86-64.so.2,libpthread.so.0} /home/ssh/lib64 测试指令是否可用 -bash-4.2$ /bin/ls / bin dev etc home lib64 -bash-4.2$ /bin/mkdir -p ddd2 -bash-4.2$ /bin/ls ddd2 此时ddd被限制到了指定的目录中，而从其视角来看与正常操作系统并无大的区别，只是仅能执行少数的操作，少了很多系统目录/文件。 通过上述两个例子，我们了解了cap_chroot的一些常用使用场景及功能。 与其他CAP类似，默认非特权用户无法调用chroot()函数: $ su - ddd Last login: Mon Nov 8 03:16:29 EST 2021 from 192.168.109.1 on pts/2 $ mkdir ttt $ chroot ttt chroot: cannot change root directory to ttt: Operation not permitted 此时需要在root对可执行文件chroot添加cap_sys_chroot能力 $ whoami root $ setcap cap_sys_chroot+ep /usr/sbin/chroot 再次切换至普通用户测试其调用 $ su - ddd Last login: Mon Nov 8 03:37:18 EST 2021 on pts/0 $ mkdir ccc $ chroot ccc chroot: failed to run command ‘/bin/bash’: No such file or directory 显然已具备权限，异常原因是因为没有设置bash $ mkdir ccc/bin $ cp /usr/bin/bash ccc/bin/ $ ldd /bin/bash linux-vdso.so.1 => (0x00007fffc9f70000) libtinfo.so.5 => /lib64/libtinfo.so.5 (0x00007f541ac98000) libdl.so.2 => /lib64/libdl.so.2 (0x00007f541aa94000) libc.so.6 => /lib64/libc.so.6 (0x00007f541a6c6000) /lib64/ld-linux-x86-64.so.2 (0x00007f541aec2000) $ mkdir -p ccc/lib64 $ cp -v /lib64/{libtinfo.so.5,libdl.so.2,libc.so.6,ld-linux-x86-64.so.2} ccc/lib64 $ chroot ccc bash-4.2$ pwd / 注意： 这里为了更好的解释说明，直接使用了chroot指令（chroot()调用的一个实现）进行分析cap_sys_chroot。 并不是指cap_sys_chroot开启后，普通用户就能使用chroot指令了，而实际是指： 当可执行文件（chroot或其他有chroot()调用的二进制文件）及进程具备了调用chroot()权限。 参考文章 linux chroot 命令 使用 chroot 监狱限制 SSH 用户访问指定目录 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/12cap_mknod解析.html":{"url":"2.容器/k8s/security/容器CAP解析/12cap_mknod解析.html","title":"12cap_mknod解析","keywords":"","body":"cap_mknod解析 cap_mknod: 允许使用mknod()系统调用 首先我们需要了解：mknod()是用来干什么？ mknod()主要用于创建块设备，mknod指令是其具体实现。 接下来我们了解下mknod指令。 mknod指令 指令格式 $ mknod [选项] [名称] [类型] [主设备号] [次设备号] 创建用户test，用于测试 $ adduser test root用户创建一个块设备 $ mknod /dev/sdb b 8 0 $ ls /dev/sdb brw-r--r--. 1 root root 8, 0 Nov 6 07:02 /dev/sdb test用户创建一个块设备 $ whoami test $ mknod /tmp/tnod1 c 1 5 mknod: ‘/tmp/tnod1’: Permission denied root用户为/usr/bin/mknod授予cap_mknod能力 $ whoami root $ setcap cap_mknod=eip /usr/bin/mknod 切换test用户再次创建一个块设备 $ whoami test $ mknod /tmp/tnod1 c 1 5 清理测试用例 $ userdel -r test $ setcap -r /usr/bin/mknod Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/13cap_audit_write解析.html":{"url":"2.容器/k8s/security/容器CAP解析/13cap_audit_write解析.html","title":"13cap_audit_write解析","keywords":"","body":"cap_audit_write解析 cap_audit_write: 允许将记录写入内核审计日志的权限 首先我们需要了解：内核审计日志用来记录什么？ 主要用来记录什么时间，哪个用户，执行了哪个程序，操作了哪个文件，成功与否。 例如执行过su命令，`/tmp``目录是否被写入过 文件路径为：/var/log/audit/audit.log 让我们看下文件内容格式： $ cat /var/log/audit/audit.log ... type=CRED_REFR msg=audit(1636423606.888:8019): pid=258705 uid=0 auid=1001 ses=570 subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 msg='op=PAM:setcred grantors=pam_env,pam_unix acct=\"root\" exe=\"/usr/bin/sudo\" hostname=? addr=? terminal=/dev/pts/0 res=success' type=USER_START msg=audit(1636423606.895:8020): pid=258705 uid=0 auid=1001 ses=570 subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 msg='op=PAM:session_open grantors=pam_keyinit,pam_keyinit,pam_limits,pam_systemd,pam_unix acct=\"root\" exe=\"/usr/bin/sudo\" hostname=? addr=? terminal=/dev/pts/0 res=success' type=USER_AUTH msg=audit(1636423606.902:8021): pid=258707 uid=0 auid=1001 ses=570 subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 msg='op=PAM:authentication grantors=pam_rootok acct=\"root\" exe=\"/usr/bin/su\" hostname=localhost.localdomain addr=? terminal=pts/0 res=success' type=USER_ACCT msg=audit(1636423606.902:8022): pid=258707 uid=0 auid=1001 ses=570 subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 msg='op=PAM:accounting grantors=pam_succeed_if acct=\"root\" exe=\"/usr/bin/su\" hostname=localhost.localdomain addr=? terminal=pts/0 res=success' type=CRED_ACQ msg=audit(1636423606.903:8023): pid=258707 uid=0 auid=1001 ses=570 subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 msg='op=PAM:setcred grantors=pam_rootok acct=\"root\" exe=\"/usr/bin/su\" hostname=localhost.localdomain addr=? terminal=pts/0 res=success' type=USER_START msg=audit(1636423606.904:8024): pid=258707 uid=0 auid=1001 ses=570 subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 msg='op=PAM:session_open grantors=pam_keyinit,pam_keyinit,pam_limits,pam_systemd,pam_unix,pam_xauth acct=\"root\" exe=\"/usr/bin/su\" hostname=localhost.localdomain addr=? terminal=pts/0 res=success' ... 默认记录用户登录相关的信息，我们接下来添加一条规则： 添加记录删除文件规则 $ echo \"-w /bin/rm -p x -k removefile\" >> /etc/audit/rules.d/audit.rules 重启服务 $ service auditd restart 查看规则 $ auditctl -l -w /bin/rm -p x -k removefile 再次查看审计日志: $ cat /var/log/audit/audit.log | tail -5 type=EXECVE msg=audit(1636424696.310:8047): argc=4 a0=\"rm\" a1=\"-i\" a2=\"-f\" a3=\"/tmp/ddd\"type=CWD msg=audit(1636424696.310:8047): cwd=\"/root\" type=PATH msg=audit(1636424696.310:8047): item=0 name=\"/bin/rm\" inode=1610620608 dev=fd:00 mode=0100755 ouid=0 ogid=0 rdev=00:00 obj=system_u:object_r:bin_t:s0 objtype=NORMAL type=PATH msg=audit(1636424696.310:8047): item=1 name=\"/lib64/ld-linux-x86-64.so.2\" inode=32646 dev=fd:00 mode=0100755 ouid=0 ogid=0 rdev=00:00 obj=system_u:object_r:ld_so_t:s0 objtype=NORMAL type=PROCTITLE msg=audit(1636424696.310:8047): proctitle=726D002D69002D66002F746D702F646464 格式不太友好，我们换成方式查看: $ ausearch -i|tail -5 type=PATH msg=audit(11/08/2021 21:24:56.310:8047) : item=1 name=/lib64/ld-linux-x86-64.so.2 inode=32646 dev=fd:00 mode=file,755 ouid=root ogid=root rdev=00:00 obj=system_u:object_r:ld_so_t:s0 objtype=NORMAL type=PATH msg=audit(11/08/2021 21:24:56.310:8047) : item=0 name=/bin/rm inode=1610620608 dev=fd:00 mode=file,755 ouid=root ogid=root rdev=00:00 obj=system_u:object_r:bin_t:s0 objtype=NORMAL type=CWD msg=audit(11/08/2021 21:24:56.310:8047) : cwd=/root type=EXECVE msg=audit(11/08/2021 21:24:56.310:8047) : argc=4 a0=rm a1=-i a2=-f a3=/tmp/ddd type=SYSCALL msg=audit(11/08/2021 21:24:56.310:8047) : arch=x86_64 syscall=execve success=yes exit=0 a0=0x1ffaba0 a1=0x1ffda20 a2=0x2000100 a3=0x7fffa0fcbfa0 items=2 ppid=258708 pid=258816 auid=neusoft uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=pts0 ses=570 comm=rm exe=/usr/bin/rm subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 key=removefile 我们发现删除的操作已被记录，更多的审计规则请移步sec-defining_audit_rules_and_controls 这里不做过多讨论 针对cap_audit_write应用场景，暂时没有找到合适的例子，后续待补充。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/容器CAP解析/特权容器缺省CAP权限.html":{"url":"2.容器/k8s/security/容器CAP解析/特权容器缺省CAP权限.html","title":"特权容器缺省CAP权限","keywords":"","body":"特权容器CAP权能解析 在配置k8s容器的securityContext.capabilities字段时，不知道该排除/添加哪些CAP属性。 我们先了解特权容器的14个CAP字段： 容器特权进程默认CAP缺省值解析（共计14个） 0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap cap_chown: 允许修改文件所有者权限 cap_dac_override: 忽略对文件的DAC访问权限控制 cap_fowner: 忽略文件属主ID必须与进程用户ID一致的权限 cap_fsetid: 确保在文件被修改后不修改setuid/setgid位 cap_kill: 允许对不属于自己的进程发送信号的权限 cap_setgid: 允许普通用户使用setgid函数 cap_setuid: 允许普通用户使用setuid函数 cap_setpcap: 允许对子进程进行CAP授权 cap_net_bind_service: 允许绑定小于1024端口的权限 cap_net_raw: 允许使用原始套接字的权限 cap_sys_chroot: 允许使用chroot()系统调用的权限 cap_mknod: 允许使用mknod()系统调用的权限 cap_audit_write: 允许将记录写入内核审计日志的权限 cap_setfcap: 允许为可执行文件设置CAP的权限 k8s下建议关闭所有CAP，按需添加 基于容器的securityContext字段进行配置 apiVersion: v1 kind: Pod metadata: name: api-server spec: containers: - name: api-server image: xzxwl/api-server-demo:latest securityContext: capabilities: drop: - ALL add: - CHOWN 对CAP的操作 首先我先了解下如果对进程/可执行文件，设置/撤销CAP 可执行文件添加CAP属性 $ setcap cap_fowner=eip /usr/bin/vim cap_fowner=eip是将fowner的能力以cap_effective(e),cap_inheritable(i),cap_permitted(p)三种位图的方式授权给vim. 查看可执行文件的CAP属性 $ getcap /usr/bin/vim /usr/bin/vim = cap_fowner+eip 清空可执行文件CAP属性 $ setcap -r /usr/bin/vim 关于容器特权用户下默认的14条CAP解析如下： cap_chown解析 cap_dac_override解析 cap_fowner解析 cap_fsetid解析 cap_kill解析 cap_setgid解析 cap_setuid解析 cap_net_bind_service解析 cap_sys_chroot解析 cap_mknod解析 cap_audit_write解析 参考文章 Linux的capability深入分析 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/关于NSA&CISA发布的Kubernetes加固指南.html":{"url":"2.容器/k8s/security/关于NSA&CISA发布的Kubernetes加固指南.html","title":"关于NSA&CISA发布的Kubernetes加固指南","keywords":"","body":"加固指南分析 词汇表 NSA: 美国国家安全局（USA's National Security Agency） CISA: 网络安全和基础设施安全局(the Cybersecurity and Infrastructure Security Agency) 背景介绍 NSA与CISA在2021-08-03发布了kubernetes加固指南 该指南详细描述了Kubernetes存在的潜在漏洞与隐患，并提供了安全配置指南以最小化风险。 本文摘抄了指南中的部分内容，详细内容请移步kubernetes加固指南 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/security/非root的容器与设备.html":{"url":"2.容器/k8s/security/非root的容器与设备.html","title":"非root的容器与设备","keywords":"","body":"非根用户下的容器与设备 Non-root Containers And Devices Author: Mikko Ylinen (Intel) 当用户希望在Linux上部署使用加速器设备的容器(通过Kubernetes Device Plugins )时，Pod的securityContext中与用户/组ID相关的安全设置会导致一个问题 在这篇博文中，我将讨论这个问题，并描述到目前为止为解决这个问题所做的工作。解决该issue 并不需要长篇大论。 相反，这篇文章的目的是提高人们对这个问题的认识，并强调重要的设备用例。这是Kubernetes需要的，因为Kubernetes要处理新的相关特性，比如对用户名称空间的支持。 为什么非根容器无权使用设备？ 在Kubernetes中运行容器的关键安全原则之一是最小权限原则: Pod/container securityContext指定要设置的配置选项，例如Linux功能(CAP)、MAC策略和用户/组ID值。 此外，集群管理员还可以使用PodSecurityPolicy(已弃用)或PodSecurity Admission(alpha)等工具来对部署在集群中的Pod实施所需的安全设置 例如，这些设置可能要求容器必须是runAsNonRoot，或者禁止它们在runAsGroup或supplementalGroups中使用root的组ID运行 在Kubernetes中，kubelet构建容器可用的设备资源列表(基于来自设备插件的输入)， 该列表包含在发送给CRI容器运行时的CreateContainer CRI消息中。 每个设备包含很少的信息: 主机/容器设备路径和所需的设备组权限。 { \"type\": \"\", \"path\": \"\", \"major\": , \"minor\": , \"fileMode\": , \"uid\": , \"gid\": }, CRI容器运行时(containerd, CRI-O)负责从主机获取每个设备的信息。默认情况下，运行时复制主机设备的用户和组id: uid(uint32，可选)-容器命名空间中设备所有者的id gid(uint32，可选)-容器命名空间中设备组的id 类似地，运行时还提供了一些其他配置选项。基于CRI字段的config.json部分进行定义， 包括securityContext: runAsUser/runAsGroup中定义的部分内容， 它通过以下方式成为POSIX平台用户结构的一部分: uid(int, 必需): 指定容器名称空间中的用户ID gid(int, 必需): 指定容器名称空间中的组ID additionalGids(int数组, 可选): 在容器名称空间中指定要添加到进程的附加组id。 然而，以config.json中的配置运行容器时将导致以下问题： 当运行容器既添加了设备，又通过runAsUser/runAsGroup设置了非根用户uid/gid的容器时，将导致以下问题： 容器用户进程没有使用设备的权限(即使设备的组id是允许非根用户组使用的)。 这是因为容器用户不属于那个主机组(例如，通过additionalGids)。 如何解决这个问题呢？ 您可能已经从问题定义中注意到，至少可以通过手动将设备gid添加到supplementalGroups来解决问题。 或者在只有一个设备的情况下，将runAsGroup设置为设备的组id。 然而，这是有问题的，因为设备的gid可能有不同的值，这取决于集群中的节点的发行版/版本。 例如，对于不同的发行版和版本，下面的命令返回不同的gid: Fedora 33: $ ls -l /dev/dri/ total 0 drwxr-xr-x. 2 root root 80 19.10. 10:21 by-path crw-rw----+ 1 root video 226, 0 19.10. 10:42 card0 crw-rw-rw-. 1 root render 226, 128 19.10. 10:21 renderD128 $ grep -e video -e render /etc/group video:x:39: render:x:997: Ubuntu 20.04: $ ls -l /dev/dri/ total 0 drwxr-xr-x 2 root root 80 19.10. 17:36 by-path crw-rw---- 1 root video 226, 0 19.10. 17:36 card0 crw-rw---- 1 root render 226, 128 19.10. 17:36 renderD128 $ grep -e video -e render /etc/group video:x:44: render:x:133: 所以说在securityContext中应该设置哪个数字? 此外，如果runAsGroup/runAsUser值是通过外部安全策略自动分配的，不能硬编码，该怎么办? 与带有fsGroup属性的卷不同，这些设备没有CRI运行时(或kubelet)能够使用的deviceGroup/deviceUser的正式概念。 如果使用由设备插件设置的容器注释(例如，io.kubernetes.cri.hostDeviceSupplementalGroup/)来获得自定义的OCI的conf.json中uid/gid值。 这将需要改变所有现有的设备插件，这不是理想的。 相反，这里有一个对终端用户更好的解决方案 -> 设备复用securityContext的runAsUser和runAsGroup值: 设备uid对应runAsUser 设备gid对应runAsGroup { \"type\": \"c\", \"path\": \"/dev/foo\", \"major\": 123, \"minor\": 4, \"fileMode\": 438, \"uid\": , \"gid\": }, 使用runc OCI运行时(在non-rootless模式下)下，设备将在容器名称空间中创建(通过mknod(2))，并且使用chmod(2)将所有权更改为runAsUser/runAsGroup。 在容器名称空间中更新所有权是合理的，因为用户进程是唯一访问设备的进程。 只考虑runAsUser/runAsGroup，例如，容器中的USER设置当前被忽略。 containerd与CRI-O中通过配置下面参数生效（设备权限从安全上下文中获取: 默认false） device_ownership_from_security_context (bool) 使用样例 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/sidecar/sidecar.html":{"url":"2.容器/k8s/sidecar/sidecar.html","title":"sidecar","keywords":"","body":"SideCar模式 SideCar中文译为边车，是附着在摩托车旁的小型车辆，用于载客。 在编程世界中，其主要功能是将主应用与外围辅助服务进行解耦，提供更灵活的应用部署方式。 其理念符合设计模式中的单一职责原则，让主应用和辅助服务分离，更专注自身功能。 使用场景-共享存储 基于K8S Pod特性，同一个Pod可以共享根容器中挂载的Volume。基于该特性，我们可以想到以下SideCar应用方式： 日志收集上传 我们可以应用日志挂载到共享的Volume上，业务容器写日志，SideCar容器读日志，并上传日志分析平台，以生产者消费者方式进行解耦。 应用Jar包挂载 因为Java应用需要依赖拥有Java运行环境，因此大多使用open-jdk等镜像作为基础镜像。 而这类镜像大多上百M。通过共享存储，我们可以利用busybox这类体积只有几M的镜像作为基础镜像，然后将jar包拷贝到共享Volume下。 并将这个承载jar的镜像作为InitContainer，主业务容器使用该共享Volume下的jar包启动业务。 后续应用版本更新，只需要更新jar包镜像。这个jar包镜像便是一个SideCar。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/storage/":{"url":"2.容器/k8s/storage/","title":"storage","keywords":"","body":" OpenEBS rook Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/storage/pvc/ReadWriteOncePod访问模式.html":{"url":"2.容器/k8s/storage/pvc/ReadWriteOncePod访问模式.html","title":"ReadWriteOncePod访问模式","keywords":"","body":" 卷的ReadWriteOncePod访问模式 ReadWriteOncePod是什么？有何应用场景？ ReadWriteOncePod原理 对比ReadWriteOnce访问模式 我们如何使用ReadWriteOnce？ 使用样例 变更现有卷访问模式为ReadWriteOnce 哪些卷插件支持ReadWriteOncePod？ 作为CSI提供者，如何支持ReadWriteOncePod？ 卷的ReadWriteOncePod访问模式 Introducing Single Pod Access Mode for PersistentVolumes Author: Chris Henzie (Google) 随着Kubernetes v1.22版本的更新，k8s为我们带来了一个新的alpha特性：存储卷新的访问方式 -> ReadWriteOncePod（单Pod访问类型的pv与pvc），换句话来讲， 指定pvc访问类型为ReadWriteOncePod时，仅有一个Pod可以访问使用该pvc（持化卷声明） ReadWriteOncePod是什么？有何应用场景？ 当我们使用存储的时候，有很多不同的消费存储模式： 多节点读写：如通过网络共享的文件系统（NFS、Cephfs） 单节点读写：高度敏感的存储数据 多点只读 在k8s世界，可通过对存储卷（pv、pvc）指定Access Modes（访问模式），实现对存储的消费方式。 如多节点读写： kind: PersistentVolumeClaim apiVersion: v1 metadata: name: shared-cache spec: accessModes: - ReadWriteMany # Allow many pods to access shared-cache simultaneously. resources: requests: storage: 1Gi Kubernetes v1.22版本之前，对存储卷有三种方式： ReadWriteOnce：单节点读写 ReadOnlyMany ：多节点只读 ReadWriteMany：多节点读写 以上三种对存储卷访问方式的控制，是通过kube-controller-manager和kubelet组件实现。 ReadWriteOncePod原理 Kubernetes v1.22提供了第四种访问PV、PVC的访问模式：ReadWriteOncePod（单一Pod访问方式） 当你创建一个带有pvc访问模式为ReadWriteOncePod的Pod A时，Kubernetes确保整个集群内只有一个Pod可以读写该PVC。 此时如果你创建Pod B并引用了与Pod A相同的PVC(ReadWriteOncePod)时，那么Pod B会由于该pvc被Pod A引用而启动失败。 Pod B事件可能如下： Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 1s default-scheduler 0/1 nodes are available: 1 node has pod using PersistentVolumeClaim with the same name and ReadWriteOncePod access mode. 乍一看，是不是觉得与ReadWriteOnce访问模式很像？但其实并不一样。 对比ReadWriteOnce访问模式 ReadWriteOnce：该访问模式约束仅有一个node节点可以访问pvc。换句话来说，同一node节点的不同pod是可以对同一pvc进行读写的 这种访问模式对于一些应用是存在隐患的，特别是对数据有写入安全（同一时间仅有一个写操作）要求的应用。 ReadWriteOncePod的出现，解决了上述隐患。 我们如何使用ReadWriteOnce？ ReadWriteOncePod方式模式是Kubernetes v1.22版本的alpha特性，并且只支持CSI类型的卷 k8s版本需为v1.22+ 首先需要k8s集群需添加该特性门控（k8s中alpha功能特性默认关闭，beta功能特性默认开启） 涉及服务组件: kube-apiserver kube-scheduler kubelet --feature-gates=\"...,ReadWriteOncePod=true\" 升级csi边车，版本要求如下： csi-provisioner:v3.0.0+ csi-attacher:v3.3.0+ csi-resizer:v1.3.0+ 使用样例 pvc声明样例 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # Allow only a single pod to access single-writer-only. resources: requests: storage: 1Gi 如果您的存储插件支持动态配置（StorageClass），那么将使用ReadWriteOncePod访问模式创建新的PersistentVolumes 变更现有卷访问模式为ReadWriteOnce 您可以变更现有PVC访问模式为ReadWriteOncePod访问模式。接下来我们通过一个迁移样例，了解迁移的流程。 样例信息： pv: cat-pictures-pv pvc: cat-pictures-pvc Deployment: cat-pictures-writer 命名空间: default 三者关系为：名为cat-pictures-writer的Deployment，声明挂载了一个名为cat-pictures-pvc的pvc，该pvc对应的pv 为cat-pictures-pv step1: 变更pv回收策略 变更cat-pictures-pv的回收策略为Retain，确保删除pvc时，对应的pv不会被删除 kubectl patch pv cat-pictures-pv -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}' step2: 停止Deployment下所有工作负载（缩容至0） kubectl scale --replicas=0 deployment cat-pictures-writer step3: 删除cat-pictures-pvc kubectl delete pvc cat-pictures-pvc step4: 清理cat-pictures-pv的spec.claimRef.uid属性，确保重新创建pvc时可以绑定新的pvc kubectl patch pv cat-pictures-pv -p '{\"spec\":{\"claimRef\":{\"uid\":\"\"}}}' step5: 变更cat-pictures-pv的访问模式为ReadWriteOncePod kubectl patch pv cat-pictures-pv -p '{\"spec\":{\"accessModes\":[\"ReadWriteOncePod\"]}}' 注意：ReadWriteOncePod不能与其他访问模式结合使用，确保ReadWriteOncePod是PV的唯一访问模式，否则无法成功绑定。 step6: 变更cat-pictures-pvc的访问模式为ReadWriteOncePod(且唯一) 未配置StorageClass情况需手动创建pvc kubectl apply -f cat-pictures-pvc.yaml kubectl apply -f cat-pictures-writer-deployment.yaml 若配置StorageClass仅需变更Deployment中的pvc访问模式 step7: 变更PV回收方式由Retain为Delete kubectl patch pv cat-pictures-pv -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Delete\"}}' step8: 恢复cat-pictures-writer工作负载实例数 kubectl scale --replicas=1 deployment cat-pictures-writer 哪些卷插件支持ReadWriteOncePod？ 只有CSI类型存储驱动支持，原生卷插件（如Hostpath）并不支持ReadWriteOncePod模式， 因为原生卷插件作为CSI迁移的一部分正在被弃用， 当ReadWriteOncePod达到beta版本时，原生卷插件可能会被k8s原生支持。 绝大多数生产环境，都会使用第三方CSI插件（Ceph CSI），很少会使用原生卷插件类型。 作为CSI提供者，如何支持ReadWriteOncePod？ 请移步原文section Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/storage/OpenEBS.html":{"url":"2.容器/k8s/storage/OpenEBS.html","title":"OpenEBS","keywords":"","body":" Table of Contents generated with DocToc OpenEBS 简介 OpenEBS是什么？ OpenEBS能做什么？ 对比传统分布式存储 OpenEBS存储引擎建议 OpenEBS特性 CAS介绍 OpenESB架构介绍 控制面 数据面 CAS引擎 存储引擎概述 存储引擎类型 存储引擎声明 CAS引擎使用场景 节点磁盘管理器（NDM） 落地实践 Local PV Hostpath实践 Local PV Device实践 总结 OpenEBS 简介 OpenEBS是什么？ OpenEBS是一种开源云原生存储解决方案，托管于CNCF基金会，目前该项目处于沙箱阶段， OpenEBS是一组存储引擎，允许您为有状态工作负载(StatefulSet)和Kubernetes平台类型选择正确的存储解决方案。 在高层次上，OpenEBS支持两大类卷——本地卷和复制卷 OpenEBS是Kubernetes本地超融合存储解决方案，它管理节点可用的本地存储，并为有状态工作负载提供本地或高可用的分布式持久卷。 作为一个完全的Kubernetes原生解决方案的另一个优势是，管理员和开发人员可以使用kubectl、Helm、 Prometheus、Grafana、Weave Scope等Kubernetes可用的所有优秀工具来交互和管理OpenEBS OpenEBS能做什么？ OpenEBS管理k8s节点上存储，并为k8s有状态负载（StatefulSet）提供本地存储卷或分布式存储卷。 本地卷（Local Storage） OpenEBS可以使用宿主机裸块设备或分区，或者使用Hostpaths上的子目录，或者使用LVM、ZFS来创建持久化卷 本地卷直接挂载到Stateful Pod中，而不需要OpenEBS在数据路径中增加任何开销 OpenEBS为本地卷提供了额外的工具，用于监控、备份/恢复、灾难恢复、由ZFS或LVM支持的快照等 对于分布式卷(即复制卷) OpenEBS使用其中一个引擎(Mayastor、cStor或Jiva)为每个分布式持久卷创建微服务 有状态Pod将数据写入OpenEBS引擎，OpenEBS引擎将数据同步复制到集群中的多个节点。 OpenEBS引擎本身作为pod部署，并由Kubernetes进行协调。 当运行Stateful Pod的节点失败时，Pod将被重新调度到集群中的另一个节点，OpenEBS将使用其他节点上的可用数据副本提供对数据的访问 有状态的Pods使用iSCSI (cStor和Jiva)或NVMeoF (Mayastor)连接OpenEBS分布式持久卷 OpenEBS cStor和Jiva专注于存储的易用性和持久性。它们分别使用自定义版本的ZFS和Longhorn技术将数据写入存储。 OpenEBS Mayastor是最新开发的以耐久性和性能为设计目标的引擎，高效地管理计算(大页面、核心)和存储(NVMe Drives)，以提供快速分布式块存储 注意: OpenEBS分布式块卷被称为复制卷，以避免与传统的分布式块存储混淆，传统的分布式块存储倾向于将数据分布到集群中的许多节点上。 复制卷是为云原生有状态工作负载设计的，这些工作负载需要大量的卷，这些卷的容量通常可以从单个节点提供，而不是使用跨集群中的多个节点分片的单个大卷 对比传统分布式存储 OpenEBS与其他传统存储解决方案不同的几个关键方面: 使用微服务体系结构构建，就像它所服务的应用程序一样。 OpenEBS本身作为一组容器部署在Kubernetes工作节点上。使用Kubernetes本身来编排和管理OpenEBS组件 完全建立在用户空间，使其高度可移植性，以运行在任何操作系统/平台。 完全意图驱动，继承了Kubernetes易用性的相同原则 OpenEBS支持一系列存储引擎，因此开发人员可以部署适合于其应用程序设计目标的存储技术。 像Cassandra这样的分布式应用程序可以使用LocalPV引擎进行最低延迟的写操作。 像MySQL和PostgreSQL这样的单片应用程序可以使用使用NVMe和SPDK构建的Mayastor或基于ZFS的cStor来实现弹性。 像Kafka这样的流媒体应用程序可以在边缘环境中使用NVMe引擎Mayastor以获得最佳性能。 驱使用户使用OpenEBS的主要原因是: 在所有的Kubernetes发行版上都是可移植的 提高了开发人员和平台SRE的生产力 与其他解决方案相比，易于使用 优秀的社区支持 免费开源 本地卷类型 本地卷只能从集群中的单个节点访问。必须在提供卷的节点上调度使用Local Volume的Pods。 本地卷通常是分布式工作负载的首选，比如Cassandra、MongoDB、Elastic等，这些工作负载本质上是分布式的，并且内置了高可用性（分片） 根据附加到Kubernetes工作节点上的存储类型，您可以从不同的动态本地PV进行选择——Hostpath、Device、LVM、ZFS或Rawfile 可复制卷类型 复制卷顾名思义，是指将数据同步复制到多个节点的卷。卷可以支持节点故障。还可以跨可用性区域设置复制，以帮助应用程序跨可用性区域移动。 复制卷还能够提供像快照、克隆、卷扩展等企业存储特性。复制卷是有状态工作负载(如Percona/MySQL、Jira、GitLab等)的首选。 根据附加到Kubernetes工作节点的存储类型和应用程序性能需求，您可以从Jiva、cStor或Mayastor中进行选择 OpenEBS存储引擎建议 应用需求 存储类型 OpenEBS卷类型 低时延、高可用性、同步复制、快照、克隆、精简配置 SSD/云存储卷 OpenEBS Mayastor 高可用性、同步复制、快照、克隆、精简配置 机械/SSD/云存储卷 OpenEBS cStor 高可用性、同步复制、精简配置 主机路径或外部挂载存储 OpenEBS Jiva 低时延、本地PV 主机路径或外部挂载存储 Dynamic Local PV - Hostpath, Dynamic Local PV - Rawfile 低时延、本地PV 本地机械/SSD/云存储卷等块设备 Dynamic Local PV - Device 低延迟，本地PV，快照，克隆 本地机械/SSD/云存储卷等块设备 OpenEBS Dynamic Local PV - ZFS , OpenEBS Dynamic Local PV - LVM 总结： 多机环境，如果有额外的块设备（非系统盘块设备）作为数据盘，选用OpenEBS Mayastor、OpenEBS cStor 多机环境，如果没有额外的块设备（非系统盘块设备）作为数据盘，仅单块系统盘块设备，选用OpenEBS Jiva 单机环境，建议本地路径Dynamic Local PV - Hostpath, Dynamic Local PV - Rawfile，由于单机多用于测试环境，数据可靠性要求较低。 由此看来，OpenEBS常用场景为以上三个场景 OpenEBS特性 容器附加存储 OpenEBS是一个容器附加存储(Container Attached Storage, CAS)的例子。 通过OpenEBS提供的卷总是被容器化。每个卷都有一个专用的存储控制器，用于提高有状态应用程序的持久性存储操作的敏捷性和粒度。 同步复制 同步复制是OpenEBS的一个可选的流行特性。 当与Jiva、cStor和Mayastor存储引擎一起使用时，OpenEBS可以同步复制数据卷以实现高可用性。 跨Kubernetes区域进行复制，从而为跨AZ设置提供高可用性。 这个特性对于使用GKE、EKS和AKS等云提供商服务上的本地磁盘构建高可用状态应用程序特别有用 快照和克隆 写时拷贝快照是OpenEBS另一个可选的流行特性。 使用cStor引擎时，快照是瞬时创建的，并且不受快照个数的限制。 增量快照功能增强了跨Kubernetes集群和跨不同云提供商或数据中心的数据迁移和可移植性。 对快照和克隆的操作完全以Kubernetes原生方法执行，使用标准kubectl命令。 常见的用例包括用于备份的高效复制和用于故障排除或针对数据的只读副本进行开发的克隆 备份和恢复 OpenEBS卷的备份和恢复可以通过开源的OpenEBS Velero插件与Kubernetes备份和恢复解决方案(如Velero(前身为Heptio Ark))协同工作。 经常使用OpenEBS增量快照功能，将数据备份到AWS S3、GCP object storage、MinIO等对象存储目标。 这种存储级别的快照和备份只使用增量数据进行备份，节省了大量的带宽和存储空间。 真正的Kubernetes云原生存储 OpenEBS是Kubernetes上有状态应用程序的云原生存储，云原生意味着遵循松散耦合的体系结构。 因此，云原生、松散耦合体系结构的一般好处是适用的。 例如，开发人员和DevOps架构师可以使用标准的Kubernetes技能和实用程序来配置、使用和管理持久存储需求 减少存储TCO高达50% 在大多数云上，块存储的收费是基于购买的多少，而不是使用的多少; 为了实现更高的性能，并在充分利用容量时消除中断的风险，容量经常被过度配置。 OpenEBS的精简配置能力可以共享本地存储或云存储，然后根据需要增加有状态应用程序的数据量。 可以动态添加存储，而不会中断暴露给工作负载或应用程序的卷。 某些用户报告说，由于使用了OpenEBS的精简配置，节省了超过60%的资源。 高可用性 由于OpenEBS遵循CAS架构，在节点故障时，Kubernetes将重新调度OpenEBS控制器，而底层数据则通过使用一个或多个副本来保护。 更重要的是——因为每个工作负载都可以利用自己的OpenEBS——不存在因存储丢失而导致系统大范围宕机的风险。 例如，卷的元数据不是集中的，它可能会像许多共享存储系统那样受到灾难性的通用中断的影响。 相反，元数据保持在卷的本地。丢失任何节点都会导致只存在于该节点上的卷副本的丢失。 由于卷数据至少在其他两个节点上进行了同步复制，因此当一个节点出现故障时，这些数据将在相同的性能级别上继续可用 CAS介绍 在CAS或容器附加存储(Container Attached Storage)体系结构中，存储在容器中运行，并且与存储绑定到的应用程序密切相关。 存储作为微服务运行，没有内核模块依赖关系。 像Kubernetes这样的编排系统编排存储卷，就像任何其他微服务或容器一样。CAS具有DAS和NAS的优点 非CAS系统上的pv 在非CAS模型中，Kubernetes持久卷仍然与内核模块紧密耦合，使得Kubernetes节点上的存储软件本质上是单片的 基于CAS系统上的pv 相反，CAS使您能够利用云原生应用程序的灵活性和可伸缩性。 定义Kubernetes PV (Persistent Volume)的存储软件是基于微服务架构的。 存储软件的控制平面(存储控制器)和数据平面(存储副本)作为Kubernetes Pods运行，因此，使您能够将云原生的所有优势应用到CAS。 CAS优势 敏捷 CAS中的每个存储卷都有一个容器化的存储控制器和相应的容器化副本。 因此，围绕这些组件的资源的维护和调优是真正敏捷的。 Kubernetes滚动升级功能可以实现存储控制器和存储副本的无缝升级。可以使用容器cGroups调优CPU和内存等资源配额。 存储策略粒度化 将存储软件容器化并将存储控制器专用于每个卷可以带来最大的存储策略粒度。 在CAS体系结构中，可以按卷配置所有存储策略。 此外，您可以监视每个卷的存储参数，并动态更新存储策略，以实现每个工作负载的预期结果。 随着卷存储策略中这种额外粒度级别的增加，存储吞吐量、IOPS和延迟的控制也会增加。 云原生 CAS将存储软件装入容器，并使用Kubernetes自定义资源定义(CRDs)来声明低级存储资源，如磁盘和存储池。 这个模型使存储能够无缝地集成到其他云原生工具中。 可以使用Prometheus、Grafana、Fluentd、Weavescope、Jaeger等云原生工具来供应、监控和管理存储资源 PV是CAS中的一个微服务 如上图所示，在CAS架构中，存储控制器和副本的软件完全是基于微服务的，因此不涉及内核组件。 通常，存储控制器POD被调度在与持久卷相同的节点上，以提高效率，副本POD可以被调度在集群节点上的任何位置。 每个副本使用本地磁盘、SAN磁盘和云磁盘的任意组合完全独立于其他副本进行配置。 这为大规模管理工作负载的存储分配提供了巨大的灵活性。 超融合非分布式 CAS架构没有遵循典型分布式存储架构。通过从存储控制器到存储副本的同步复制，存储变得高度可用。 卷副本的元数据不是在节点之间共享的，而是在每个本地节点上独立管理。 如果一个节点故障，存储控制器(在本例中是一个无状态容器)将在一个节点上轮转，该节点上运行着第二个或第三个副本，数据仍然可用。 与超融合系统类似，CAS中的卷的存储和性能是可扩展的。由于每个卷都有自己的存储控制器，因此存储可以在一个节点的存储容量允许的范围内进行扩展。 在给定的Kubernetes集群中，随着容器应用程序数量的增加，会增加更多的节点，从而提高存储容量和性能的整体可用性，从而使存储对新的应用程序容器可用。 这一过程与Nutanix等成功的超融合系统非常相似。 OpenESB架构介绍 OpenESB遵循容器附加存储（CAS）模型，每个卷都有一个专用的控制器POD和一组副本POD。 CAS体系结构的优点在CNCF博客 上进行了讨论。 OpenEBS操作和使用都很简单，因为它看起来和感觉上就像其他云原生和Kubernetes友好的项目。 OpenEBS有许多组件，可以分为以下类别: 控制面组件 - Provisioner, API Server, volume exports,volume sidecars 数据面组件 - Jiva、cStor 节点磁盘管理器 - Discover, monitor, 管理连接k8s的媒介 与云原生工具的集成 - 已经与Prometheus,Grafana, Fluentd、Jaeger集成 控制面 OpenEBS集群的控制平面通常被称为Maya OpenEBS控制平面负责提供卷、相关的卷操作，如快照、克隆、创建存储策略、执行存储策略、导出Prometheus/grafana使用的卷指标，等等。 OpenEBS提供了一个动态提供程序，这是Kubernetes的标准外部存储插件。 OpenEBS PV提供者的主要任务是向应用程序PODS启动卷供应，并实现PV的Kubernetes规范。 m-apiserver开放存储的REST API，并承担大量卷策略处理和管理工作。 控制平面和数据平面之间的连通性使用Kubernetes sidecar模式。控制平面需要与数据平面通信的场景如下所示。 对于卷的统计，如IOPS，吞吐量，延迟等--通过卷暴漏的sidecar实现 使用卷控制器pod执行卷策略，使用卷副本pod进行磁盘/池管理-通过卷管理sidecar实现 OpenEBS PV Provisioner 此组件作为POD运行，并做出配置决策,它的使用方式是: 开发人员用所需的卷参数构造一个声明，选择适当的存储类，并在YAML规范上调用kubelet。 OpenEBS PV动态提供程序与maya-apiserver交互，在适当的节点上为卷控制器pod和卷副本pod创建部署规范。 可以使用PVC规范中的注释来控制卷Pod(控制器/副本)的调度。 目前，OpenEBS Provisioner只支持一种绑定类型，即iSCSI。 Maya-ApiServer m-apiserver作为POD运行。顾名思义，m-apiserver公开OpenEBS REST api m-apiserver还负责创建创建卷pod所需的部署规范文件。 在生成这些规范文件之后，它将调用kube-apiserver来相应地调度这些pods。 OpenEBS PV提供者在卷发放结束时，将创建一个PV对象并将其挂载到应用程序pod上。 PV由控制器pod承载，控制器pod由不同节点中的一组副本pod支持。 控制器pod和复制pod是数据平面的一部分，在存储引擎部分有更详细的描述。 m-apiserver的另一个重要任务是卷策略管理。OpenEBS为表示策略提供了非常细粒度的规范。 m-apiserver解释这些YAML规范，将它们转换为可执行的组件，并通过容量管理sidecar来执行它们 Maya Volume Exporter Maya卷导出器是每个存储控制器pod的sidecar 这些sidecar将控制平面连接到数据平面以获取统计信息。统计信息的粒度在卷级别。一些统计数据示例如下： 卷读取延迟 卷写入延迟 卷每秒读取速度 卷每秒写入速度 读取块大小 写入块大小 容量统计 这些统计信息通常由Prometheus客户端来拉取，该客户端在OpenBS安装期间安装和配置 卷管理sidecar Sidecars还用于将控制器配置参数和卷策略传递给卷控制器pod(卷控制器pod是一个数据平面)， 并将副本配置参数和副本数据保护参数传递给卷副本pod。 数据面 OpenEBS数据平面负责实际的卷IO路径。存储引擎在数据平面实现实际的IO路径。 目前，OpenEBS提供了两个可以轻松插入的存储引擎。它们被称为Jiva和cStor。 这两个存储引擎都完全运行在Linux用户空间中，并基于微服务架构。 Jiva Jiva存储引擎基于Rancher's LongHorn与gotgt开发实现， 使用go语言开发，并运行于用户命名空间下。 LongHorn控制器将输入的IO同步复制到LongHorn副本。该副本将Linux稀疏文件视为构建存储特性(如精简配置、快照、重建等)的基础。 cStor cStor数据引擎使用C语言编写，具有高性能的iSCSI target和Copy-On-Write块系统，提供数据完整性、数据弹性和时间点的快照和克隆。 cStor有一个池特性，它以条带、镜像或RAIDZ模式聚合一个节点上的磁盘，以提供更大的容量和性能单位。 cStor还可以跨区域将数据同步复制到多个节点，从而避免节点丢失或节点重启导致数据不可用。 LocalPV 对于那些不需要存储级复制的应用程序，LocalPV可能是很好的选择，因为它能提供更高的性能。 OpenEBS LocalPV与Kubernetes LocalPV类似，不同之处在于它是由OpenEBS控制平面动态提供的， 就像任何其他常规PV一样。OpenEBS LocalPV有两种类型:hostpath LocalPV和device LocalPV。 hostpath LocalPV指的是主机上的子目录，LocalPV指的是在节点上发现的磁盘(可以是直接连接的，也可以是网络连接的)。 OpenEBS引入了一个LocalPV提供者，用于根据PVC和存储类规范中的一些标准选择匹配的磁盘或主机路径。 节点磁盘管理器 节点磁盘管理器(NDM)填补了使用Kubernetes管理有状态应用程序的持久存储所需的工具链的空白。 容器时代的DevOps架构师必须以一种自动化的方式满足应用程序和应用程序开发人员的基础设施需求， 这种方式可以跨环境提供弹性和一致性。这些要求意味着存储堆栈本身必须非常灵活， 以便Kubernetes和云原生生态系统中的其他软件可以轻松地使用这个堆栈。 NDM在Kubernetes的存储堆栈中起着基础性的作用，它统一了不同的磁盘， 并通过将它们标识为Kubernetes对象，提供了将它们汇聚的能力。 此外，NDM发现、提供、监视和管理底层磁盘的方式，可以让Kubernetes PV提供者(如OpenEBS和其他存储系统)和Prometheus管理磁盘子系统 CAS引擎 存储引擎概述 存储引擎是持久化卷IO路径的数据平面组件。 在CAS架构中，用户可以根据不同的配置策略，为不同的应用工作负载选择不同的数据平面。 存储引擎可以通过特性集或性能优化给定的工作负载。 操作员或管理员通常选择具有特定软件版本的存储引擎，并构建优化的卷模板， 这些卷模板根据底层磁盘的类型、弹性、副本数量和参与Kubernetes集群的节点集进行微调。 用户可以在发放卷时选择最优的卷模板，从而在给定的Kubernetes集群上为所有存储卷运行最优的软件和存储组合提供最大的灵活性。 存储引擎类型 OpenEBS提供了三种存储引擎 Jiva Jiva是OpenEBS 0.1版中发布的第一个存储引擎，使用起来最简单。它基于GoLang开发，内部使用LongHorn和gotgt堆栈。 Jiva完全在用户空间中运行，并提供同步复制等标准块存储功能。 Jiva通常适用于容量较小的工作负载，不适用于大量快照和克隆特性是主要需求的情况 cStor cStor是OpenEBS 0.7版本中最新发布的存储引擎。cStor非常健壮，提供数据一致性，很好地支持快照和克隆等企业存储特性。 它还提供了一个健壮的存储池特性，用于在容量和性能方面进行全面的存储管理。 cStor与NDM (Node Disk Manager)一起，为Kubernetes上的有状态应用程序提供完整的持久化存储特性 OpenEBS Local PV OpenEBS Local PV是一个新的存储引擎，它可以从本地磁盘或工作节点上的主机路径创建持久卷或PV。 CAS引擎可以从OpenEBS的1.0.0版本中获得。使用OpenEBS Local PV，性能将等同于创建卷的本地磁盘或文件系统(主机路径)。 许多云原生应用程序可能不需要复制、快照或克隆等高级存储特性，因为它们本身就提供了这些特性。这类应用程序需要以持久卷的形式访问管理的磁盘 SP 存储池，表示Jiva自定义存储资源 CV cStor卷，表示cStor卷自定义资源 CVR cStor卷副本 SPC 存储池声明，表示cStor池聚合的自定义资源 CSP cStor存储池，表示cStor Pool每个节点上的自定义资源 一个SPC对应多个CSP，相应的一个CV对应多个CVR 存储引擎声明 通过指定注释openebs来选择存储引擎。 StorageClass规范中的io/cas-type。StorageClass定义了提供程序的细节。为每个CAS引擎指定单独的供应程序。 cStor存储类规范文件内容 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: cStor-storageclass annotations: openebs.io/cas-type: cstor cas.openebs.io/config: | - name: StoragePoolClaim value: \"cStorPool-SSD\" provisioner: openebs.io/provisioner-iscsi --- Jiva存储类规范文件内容 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: jiva-storageclass annotations: openebs.io/cas-type: jiva cas.openebs.io/config: | - name: StoragePool value: default provisioner: openebs.io/provisioner-iscsi --- 当cas类型为Jiva时，StoragePool的default值具有特殊含义。 当pool为默认值时，Jiva引擎将从容器(副本pod)本身的存储空间中为副本pod开辟数据存储空间。 当所需的卷大小很小(比如5G到10G)时，StoragePool default工作得很好，因为它可以容纳在容器本身内。 Local PV存储类规范文件内容-主机路径 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: localpv-hostpath-sc annotations: openebs.io/cas-type: local cas.openebs.io/config: | - name: BasePath value: \"/var/openebs/local\" - name: StorageType value: \"hostpath\" provisioner: openebs.io/local --- Local PV存储类规范文件内容-主机设备 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: localpv-device-sc annotations: openebs.io/cas-type: local cas.openebs.io/config: | - name: StorageType value: \"device\" - name: FSType value: ext4 provisioner: openebs.io/local --- cStor、Jiva、LocalPV特性比较： 特性 Jiva cStor Local PV 轻量级运行于用户空间 Yes Yes Yes 同步复制 Yes Yes No 适合低容量工作负载 Yes Yes Yes 支持快照，克隆 Basic Advanced No 数据一致性 Yes Yes NA 使用Velero恢复备份 Yes Yes Yes 适合高容量工作负载 No Yes Yes 自动精简配置 Yes No 磁盘池或聚合支持 Yes No 动态扩容 Yes Yes 数据弹性(RAID支持) Yes No 接近原生磁盘性能 No No Yes 大多数场景推荐cStor，因其提供了强大的功能，包括快照/克隆、存储池功能（如精简资源调配、按需扩容等）。 Jiva适用于低容量需求的工作负载场景，例如5到50G。 尽管使用Jiva没有空间限制，但建议将其用于低容量工作负载。 Jiva非常易于使用，并提供企业级容器本地存储，而不需要专用硬盘。 有快照和克隆功能的需求的场景，优先考虑使用cStor而不是Jiva。 CAS引擎使用场景 如上表所示，每个存储引擎都有自己的优势。 选择引擎完全取决于应用程序的工作负载以及它当前和未来的容量和/或性能增长。 下面的指导原则在定义存储类时为选择特定的引擎提供一些帮助。 选择cStor的理想条件 当需要同步复制数据并在节点上有多个磁盘时 当您从每个节点上的本地磁盘或网络磁盘池管理多个应用程序的存储时。 通过精简配置、存储池和卷的按需扩容、存储池的性能按需扩容等特性，实现对存储层的管理。 cStor用于在本地运行的Kubernetes集群上构建Kubernetes本地存储服务，类似于AWS EBS或谷歌PD。 当需要存储级快照和克隆能力时 当您需要企业级存储保护特性，如数据一致性、弹性(RAID保护)。 如果您的应用程序不需要存储级复制，那么使用OpenEBS主机路径LocalPV或OpenEBS设备LocalPV可能是更好的选择。 选择Jiva的理想条件: 当您想要数据的同步复制，并且拥有单个本地磁盘或单个管理磁盘(如云磁盘(EBS、GPD))，并且不需要快照或克隆特性时 Jiva是最容易管理的，因为磁盘管理或池管理不在这个引擎的范围内。Jiva池是本地磁盘、网络磁盘、虚拟磁盘或云磁盘的挂载路径。 以下场景Jiva更优于cStor: 当程序不需要存储级的快照、克隆特性 当节点上没有空闲磁盘时。Jiva可以在主机目录上使用，并且仍然可以实现复制。 当不需要动态扩展本地磁盘上的存储时。将更多磁盘添加到Jiva池是不可能的，因此Jiva池的大小是固定的，如果它在物理磁盘上。 但是，如果底层磁盘是虚拟磁盘、网络磁盘或云磁盘，则可以动态地更改Jiva池的大小 容量需求较小。大容量应用通常需要动态增加容量，cStor更适合这种需求 选择OpenEBS主机路径LocalPV的理想条件: 当应用程序本身具备管理复制能力（例如：es）时，不需要在存储层进行复制。在大多数这样的情况下，应用程序是作为statefulset部署的 高于Jiva与cStor的读写性能需求 当特定应用程序没有专用的本地磁盘或特定应用程序不需要专用的存储时，建议使用Hostpath。 如果您想跨多个应用程序共享一个本地磁盘，主机路径LocalPV是正确的方法 选择OpenEBS主机设备LocalPV的理想条件: 当应用程序管理复制本身，不需要在存储层进行复制时。在大多数这种情况下，应用程序被部署为有状态集 高于Jiva与cStor的读写性能需求 高于OpenEBS主机路径LocalPV的读写性能需求 当需要接近磁盘性能时。该卷专用于写入单个SSD或NVMe接口以获得最高性能 总结 如果应用程序处于生产中，并且不需要存储级复制，那么首选LocalPV 如果您的应用程序处于生产状态，并且需要存储级复制，那么首选cStor 如果应用程序较小，需要存储级复制，但不需要快照或克隆，则首选Jiva 节点磁盘管理器（NDM） 节点磁盘管理器(NDM)是OpenEBS体系结构中的一个重要组件。 NDM将块设备视为需要监视和管理的资源，就像CPU、内存和网络等其他资源一样。 它是一个在每个节点上运行的守护进程，基于过滤器检测附加的块设备，并将它们作为块设备自定义资源加载到Kubernetes中。这些定制资源旨在通过提供类似于: 轻松访问Kubernetes集群中可用的块设备清单 预测磁盘的故障，以帮助采取预防措施 允许动态地将磁盘挂载/卸载到存储Pod中，而无需重新启动在磁盘挂载/卸载的节点上运行的相应NDM Pod 尽管做了上述所有工作，NDM还是有助于提供持久卷的总体简化。 NDM是在OpenEBS安装期间作为守护进程部署的。NDM daemonset发现每个节点上的磁盘，并创建一个名为Block Device或BD的自定义资源。 访问权限说明 NDM守护进程运行在容器中，必须访问底层存储设备并以特权模式运行。 NDM需要特权模式，因为它需要访问/dev、/proc和/sys目录来监视附加设备，还需要使用各种探测器获取附加设备的详细信息。 NDM负责发现块设备并过滤掉不应该被OpenEBS使用的设备;例如，检测有OS文件系统的磁盘。 NDM pod默认情况下在容器中挂载主机的/proc目录，然后加载/proc/1/mounts，以找到操作系统使用的磁盘 NDM守护程序功能 发现Kubernetes节点上的块设备 在启动时发现块设备-创建和/或更新状态。 维护集群范围内磁盘的唯一id: 对WWN / PartitionUUID / FileSystemUUID / DeviceMapperUUID进行Hash计算 检测节点中添加/移除块设备，并更新块设备状态 添加块设备作为Kubernetes自定义资源，具有以下属性： spec: 如果可用，将更新以下内容 设备路径 设备链接 供应商和型号信息 WWN和序列号 容量 扇区和区块大小 labels: 主机名称(kubernetes.io/hostname) 块设备类型(ndm.io/blockdevice-type) Managed (ndm.io/managed) status: 状态可以有以下值 Active : 节点上存在块设备 Inactive : 给定节点上不存在块设备 Unknown : NDM在块设备最后被检测到的节点上停止/无法确定状态 过滤器 为要创建块设备CR的块设备类型配置过滤器。过滤器可以通过供应商类型、设备路径模式或挂载点进行配置 过滤器可以是包含过滤器，也可以是排除过滤器。它们被配置为configmap。 管理员用户可以在OpenEBS安装时通过更改OpenEBS操作员yaml文件或helm值中的NDM configmap来配置这些过滤器。 yaml文件。如果这些过滤器需要在安装后更新，那么可以遵循以下方法之一: 使用operator方式安装OpenEBS。在Yaml文件中，更新configmap中的过滤器并应用operator.yaml 如果OpenEBS是使用helm安装的，更新values.yaml中的configmap并使用helm进行升级 或者，使用kubectl编辑NDM configmap，更新过滤器 落地实践 OpenEBS的cStor与Jiva引擎由于组件过多，配置相较其他存储方案繁琐，生产环境不建议使用，这里我们仅论述Local PV引擎。 Local PV引擎不具备存储级复制能力，适用于k8s有状态服务的后端存储（如: es、redis等） Local PV Hostpath实践 对比Kubernetes Hostpath卷相比，OpenEBS本地PV Hostpath卷具有以下优势: OpenEBS本地PV Hostpath允许您的应用程序通过StorageClass、PVC和PV访问Hostpath。 这为您提供了更改PV提供者的灵活性，而无需重新设计应用程序YAML 使用Velero备份和恢复进行数据保护 通过对应用程序YAML和pod完全屏蔽主机路径来防范主机路径安全漏洞 环境依赖: k8s 1.12以上 OpenEBS 1.0以上 实践环境: docker 19.03.8 k8s 1.18.6 CentOS7 [root@localhost ~]# kubectl get node NAME STATUS ROLES AGE VERSION node1 Ready master,worker 8m8s v1.18.6 node2 Ready master,worker 7m15s v1.18.6 node3 Ready master,worker 7m15s v1.18.6 创建数据目录 在将要创建Local PV Hostpaths的节点上设置目录。这个目录将被称为BasePath。默认位置是/var/openebs/local 节点node1、node2、node3创建/data/openebs/local目录 （/data可以预先挂载数据盘，如未挂载额外数据盘，则使用操作系统'/'挂载点存储空间） mkdir -p /data/openebs/local 下载应用描述文件 yaml文件 发布openebs应用 根据上述配置文件，保证k8s集群可访问到如下镜像（建议导入本地私有镜像库，如: harbor） openebs/node-disk-manager:1.5.0 openebs/node-disk-operator:1.5.0 openebs/provisioner-localpv:2.10.0 更新openebs-operator.yaml中镜像tag为实际tag image: openebs/node-disk-manager:1.5.0 image: openebs/node-disk-operator:1.5.0 image: openebs/provisioner-localpv:2.10.0 发布 kubectl apply -f openebs-operator.yaml 查看发布状态 [root@localhost openebs]# kubectl get pod -n openebs -w NAME READY STATUS RESTARTS AGE openebs-localpv-provisioner-6d6d9cfc99-4sltp 1/1 Running 0 10s openebs-ndm-85rng 1/1 Running 0 10s openebs-ndm-operator-7df6668998-ptnlq 0/1 Running 0 10s openebs-ndm-qgqm9 1/1 Running 0 10s openebs-ndm-zz7ps 1/1 Running 0 10s 创建存储类 更改配置文件中的内容 value: \"/var/openebs/local/\" 发布创建存储类 cat > openebs-hostpath-sc.yaml 创建pvc验证可用性 cat > local-hostpath-pvc.yaml 查看pvc状态 [root@localhost openebs]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-hostpath-pvc Pending openebs-hostpath 2m15s 输出显示STATUS为Pending。这意味着PVC还没有被应用程序使用。 创建pod cat > local-hostpath-pod.yaml > /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done' volumeMounts: - mountPath: /mnt/store name: local-storage EOF 发布创建 kubectl apply -f local-hostpath-pod.yaml 验证数据是否写入卷 [root@localhost openebs]# kubectl exec hello-local-hostpath-pod -- cat /mnt/store/greet.txt Thu Jun 24 15:10:45 CST 2021 [node1] Hello from OpenEBS Local PV. 验证容器是否使用Local PV Hostpath卷 [root@localhost openebs]# kubectl describe pod hello-local-hostpath-pod Name: hello-local-hostpath-pod Namespace: default Priority: 0 ... Volumes: local-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: local-hostpath-pvc ReadOnly: false default-token-98scc: Type: Secret (a volume populated by a Secret) SecretName: default-token-98scc Optional: false ... 查看pvc状态 [root@localhost openebs]# kubectl get pvc local-hostpath-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE local-hostpath-pvc Bound pvc-6eac3773-49ef-47af-a475-acb57ed15cf6 5G RWO openebs-hostpath 10m 查看该pv卷数据存储目录为 [root@localhost openebs]# kubectl get -o yaml pv pvc-6eac3773-49ef-47af-a475-acb57ed15cf6|grep 'path:' f:path: {} path: /data/openebs/local/pvc-6eac3773-49ef-47af-a475-acb57ed15cf6 并且pv配置了亲和性，制定了调度节点为node2 spec: accessModes: - ReadWriteOnce capacity: storage: 5G claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: local-hostpath-pvc namespace: default resourceVersion: \"9034\" uid: 6eac3773-49ef-47af-a475-acb57ed15cf6 local: fsType: \"\" path: /data/openebs/local/pvc-6eac3773-49ef-47af-a475-acb57ed15cf6 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node2 persistentVolumeReclaimPolicy: Delete storageClassName: openebs-hostpath volumeMode: Filesystem 验证三个节点存储目录下 结果证明数据仅存在于node2下 清理pod kubectl delete pod hello-local-hostpath-pod 基准测试 下载基准测试Job声明文件 调整以下内容 image: openebs/perf-test:latest # 调整为内网镜像库tag claimName: dbench # 调整为local-hostpath-pvc 发布运行 kubectl create -f fio-deploy.yaml 查看运行状态 [root@node1 openebs]# kubectl get pod NAME READY STATUS RESTARTS AGE dbench-729cw-nqfpt 1/1 Running 0 24s 查看基准测试结果 [root@node1 openebs]# kubectl logs -f dbench-729cw-nqfpt ... All tests complete. ================== = Dbench Summary = ================== Random Read/Write IOPS: 2144/6654. BW: 131MiB/s / 403MiB/s Average Latency (usec) Read/Write: 4254.08/3661.59 Sequential Read/Write: 1294MiB/s / 157MiB/s Mixed Random Read/Write IOPS: 1350/443 清理 kubectl delete pvc local-hostpath-pvc kubectl delete sc openebs-hostpath Local PV Device实践 对比Kubernetes本地持久卷，OpenEBS本地PV设备卷有以下优点: OpenEBS本地PV设备卷provider是动态的，Kubernetes设备卷provider是静态的 OpenEBS NDM更好地管理用于创建本地pv的块设备。 NDM提供了发现块设备属性、设置设备筛选器、度量集合以及检测块设备是否已经跨节点移动等功能 环境依赖: k8s 1.12以上 OpenEBS 1.0以上 实践环境: docker 19.03.8 k8s 1.18.6 CentOS7 [root@localhost ~]# kubectl get node NAME STATUS ROLES AGE VERSION node1 Ready master,worker 8m8s v1.18.6 node2 Ready master,worker 7m15s v1.18.6 node3 Ready master,worker 7m15s v1.18.6 三个节点上的/dev/sdb作为块设备存储 [root@node1 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom [root@node2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom [root@node3 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom 创建数据目录 在将要创建Local PV Hostpaths的节点上设置目录。这个目录将被称为BasePath。默认位置是/var/openebs/local 节点node1、node2、node3创建/data/openebs/local目录 （/data可以预先挂载数据盘，如未挂载额外数据盘，则使用操作系统'/'挂载点存储空间） mkdir -p /data/openebs/local 下载应用描述文件 yaml文件 发布openebs应用 根据上述配置文件，保证k8s集群可访问到如下镜像（建议导入本地私有镜像库，如: harbor） openebs/node-disk-manager:1.5.0 openebs/node-disk-operator:1.5.0 openebs/provisioner-localpv:2.10.0 更新openebs-operator.yaml中镜像tag为实际tag image: openebs/node-disk-manager:1.5.0 image: openebs/node-disk-operator:1.5.0 image: openebs/provisioner-localpv:2.10.0 发布 kubectl apply -f openebs-operator.yaml 查看发布状态 [root@localhost openebs]# kubectl get pod -n openebs -w NAME READY STATUS RESTARTS AGE openebs-localpv-provisioner-6d6d9cfc99-4sltp 1/1 Running 0 10s openebs-ndm-85rng 1/1 Running 0 10s openebs-ndm-operator-7df6668998-ptnlq 0/1 Running 0 10s openebs-ndm-qgqm9 1/1 Running 0 10s openebs-ndm-zz7ps 1/1 Running 0 10s 创建存储类 cat > local-device-sc.yaml 创建pod及pvc cat > local-device-pod.yaml > /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done' volumeMounts: - mountPath: /mnt/store name: local-storage EOF 发布 kubectl apply -f local-device-pod.yaml 查看pod状态 [root@node1 openebs]# kubectl get pod hello-local-device-pod -w NAME READY STATUS RESTARTS AGE hello-local-device-pod 1/1 Running 0 9s 确认pod关联pvc是否为local-device-pvc [root@node1 openebs]# kubectl describe pod hello-local-device-pod Name: hello-local-device-pod Namespace: default Node: node2/192.168.1.112 ... Volumes: local-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: local-device-pvc ReadOnly: false ... 观察到调度的节点为node2，确认node2节点/dev/sdb是否被使用 [root@node2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk sr0 11:0 1 4.4G 0 rom [root@node2 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 400G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 399G 0 part └─centos-root 253:0 0 399G 0 lvm / sdb 8:16 0 20G 0 disk /var/lib/kubelet/pods/266b7b14-5eb7-40ec-bccb-3ac189acf939/volumes/kubernetes.io~local-volume/pvc-9bd89019-13dc-4 sr0 11:0 1 4.4G 0 rom 确实被使用，OpenEBS强大之处则在于此，极致的简洁。 如上文我们讨论的那样，NDM负责发现块设备并过滤掉不应该被OpenEBS使用的设备，例如，检测有OS文件系统的磁盘。 基准测试 创建基准测试pvc cat > dbench-pvc.yaml 下载基准测试Job声明文件 调整以下内容 image: openebs/perf-test:latest # 调整为内网镜像库tag 发布运行 kubectl create -f dbench-pvc.yaml kubectl create -f fio-deploy.yaml 查看运行状态 [root@node1 openebs]# kubectl get pod NAME READY STATUS RESTARTS AGE dbench-vqk68-f9877 1/1 Running 0 24s 查看基准测试结果 [root@node1 openebs]# kubectl logs -f dbench-vqk68-f9877 ... All tests complete. ================== = Dbench Summary = ================== Random Read/Write IOPS: 3482/6450. BW: 336MiB/s / 1017MiB/s Average Latency (usec) Read/Write: 2305.77/1508.63 Sequential Read/Write: 6683MiB/s / 2312MiB/s Mixed Random Read/Write IOPS: 3496/1171 从结果来看，相较Local PV HostPath模式性能翻倍 总结 在整个测试验证过程，OpenEBS给我的感觉是：极简的操作，尤其Local PV引擎的部署使用。 但OpenEBS现阶段也存在一些不足： cStor与Jiva数据面组件较多，配置较为繁琐（第一感觉概念性的组件过多，） cStor与Jiva部分组件创建依赖内部定义的镜像tag，在离线环境下无法通过调整为私有库tag导致组件无法成功运行 存储类型单一，多个引擎仅支持块存储类型，不支持原生多节点读写（需结合NFS实现），对比ceph等稍显逊色 建议以下场景使用OpenEBS作为后端存储： 单机测试环境 多机实验/演示环境 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/storage/rook.html":{"url":"2.容器/k8s/storage/rook.html","title":"rook","keywords":"","body":" Table of Contents generated with DocToc Rook Rook架构设计 Rook实践 Rook管理ceph-基础版 环境说明 依赖说明 开启准入控制器 创建rook operator 创建ceph集群 创建ceph工具箱 创建块存储 创建共享存储 开启控制面板 Rook Rook架构设计 Rook使Ceph存储系统能够使用Kubernetes原生资源对象在Kubernetes上运行。 下图说明了Ceph Rook如何与Kubernetes集成： 通过在Kubernetes集群中运行Ceph，Kubernetes应用可以挂载Rook管理的块设备和文件系统， 或者可以使用S3/swiftapi进行对象存储。 Rook operator自动配置存储组件并监视集群，以确保存储保持可用和正常。 Rook operator是一个简单的容器，它拥有引导和监视存储集群所需的所有东西。 operator将启动和监控Ceph monitor pods, Ceph OSD守护进程提供RADOS存储，以及启动和管理其他Ceph守护进程。 operator通过初始化运行服务所需的pods和其他组件来管理池、对象存储(S3/Swift)和文件系统的crd。 operator将监视存储守护程序，以确保群集正常运行。Ceph-mons将在必要时启动或故障转移， 并随着集群的增长或收缩进行其他调整。operator还将监视api服务请求的所需状态更改，并应用更改。 Rook operator还初始化消耗存储所需的代理。Rook会自动配置Ceph CSI驱动程序，将存储装载到pod中。 rook/ceph镜像包括管理集群所需的所有工具。许多Ceph概念，如放置组和crush map是隐藏的。 Rook在物理资源、池、卷、文件系统和存储桶方面为管理员创建了一个非常简化的用户体验。同时，当需要Ceph工具时，可以应用高级配置 Rook基于golang实现。Ceph是基于C++中实现的，其中数据路径是高度优化的。二者是最好的组合。 Rook实践 Rook管理ceph-基础版 环境说明 k8s集群信息 [root@node1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready master,worker 2d4h v1.18.6 node2 Ready master,worker 2d4h v1.18.6 node3 Ready master,worker 2d4h v1.18.6 内核版本 5.4.108-1.el7.elrepo.x86_64 磁盘设备 node1-node3各挂载一块100G磁盘，设备路径为/dev/sdb 依赖说明 要配置Ceph存储群集，至少需要以下一个本地存储选项： 原始设备（没有分区或格式化文件系统） 原始分区（无格式化文件系统） k8s存储类提供的块模式PV 安装LVM包 在以下场景中，Ceph OSD依赖于LVM： OSD是在原始设备或分区上创建的 如果启用了加密（encryptedDevice: true） 指定了元数据设备 在以下情况下，OSD不需要LVM： 使用storageClassDeviceSets在PVC上创建OSD 如果您的场景需要LVM，那么LVM需要在运行OSD的主机上可用。有些Linux发行版不附带lvm2包。 要运行Ceph OSDs，k8s群集中的所有存储节点上都需要此包。如果没有这个包， 即使Rook能够成功地创建Ceph OSD，当一个节点重新启动时，在重新启动的节点上运行的OSD pods将无法启动。 请使用Linux发行版的包管理器安装LVM。例如： yum install -y lvm2 操作系统内核 RBD类型 Ceph需要一个用RBD模块构建的Linux内核。 许多Linux发行版都有这个模块，但不是所有发行版都有。例如，GKE容器优化OS（COS）没有RBD。 您可以通过运行modprobe rbd来测试Kubernetes节点 cephfs类型 如果要从Ceph共享文件系统（CephFS）创建卷，建议的最低内核版本为4.17。 如果内核版本低于4.17，则不会强制执行所请求的PVC大小。存储配额将仅在较新的内核上强制执行。 开启准入控制器 准入控制器在对象持久化之前拦截到Kubernetes API服务器的请求，但在对请求进行身份验证和授权之后。 建议启用Rook准入控制器，以提供额外级别的验证，以确保Rook是使用自定义资源(CR)设置正确配置的。 下载ceph配置声明文件上传至k8s节点/root下,解压 rook-1.6.6.tar.gz tar zxvf v1.6.6.tar.gz mv rook-1.6.6 rook 可利用助手脚本自动配置部署Rook准入控制器,这个脚本将帮助我们完成以下任务: 创建自签名证书 为证书创建证书签名请求（CSR），并从Kubernetes集群获得批准 将这些证书存储为Kubernetes Secret 创建Service Account、ClusterRole和ClusterRoleBindings，以便以最低权限运行webhook服务 创建ValidatingWebhookConfig并使用来自集群的适当值填充CA bundle cd rook/ kubectl create -f cluster/examples/kubernetes/ceph/crds.yaml -f cluster/examples/kubernetes/ceph/common.yaml 离线环境下，手动下载cert-manager.yaml 上传至/root/rook/下，替换文件路径 sed -i \"s#https://github.com/jetstack/cert-manager/releases/download/\\$CERT_VERSION/#rook/#g\" \\ tests/scripts/deploy_admission_controller.sh 调整/root/rook/cert-manager.yaml文件内引用镜像tag(替换为可访问的镜像tag) [root@node1 rook]# cat cert-manager.yaml |grep 'image:' image: harbor.chs.neusoft.com/ceph-csi/jetstack/cert-manager-cainjector:v1.2.0 image: harbor.chs.neusoft.com/ceph-csi/jetstack/cert-manager-controller:v1.2.0 image: harbor.chs.neusoft.com/ceph-csi/jetstack/cert-manager-webhook:v1.2.0 发布 tests/scripts/deploy_admission_controller.sh 创建rook operator 调整/root/rook/cluster/examples/kubernetes/ceph/operator.yaml文件内引用镜像tag(替换为可访问的镜像tag) [root@node1 rook]# grep \"image:\" cluster/examples/kubernetes/ceph/operator.yaml image: rook/ceph:v1.6.6 调整以下内容，并放开注释 # ROOK_CSI_CEPH_IMAGE: \"quay.io/cephcsi/cephcsi:v3.3.1\" # ROOK_CSI_REGISTRAR_IMAGE: \"k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.2.0\" # ROOK_CSI_RESIZER_IMAGE: \"k8s.gcr.io/sig-storage/csi-resizer:v1.2.0\" # ROOK_CSI_PROVISIONER_IMAGE: \"k8s.gcr.io/sig-storage/csi-provisioner:v2.2.2\" # ROOK_CSI_SNAPSHOTTER_IMAGE: \"k8s.gcr.io/sig-storage/csi-snapshotter:v4.1.1\" # ROOK_CSI_ATTACHER_IMAGE: \"k8s.gcr.io/sig-storage/csi-attacher:v3.2.1\" 发布 kubectl apply -f cluster/examples/kubernetes/ceph/operator.yaml 查看状态 [root@node1 rook]# kubectl get pod -n rook-ceph NAME READY STATUS RESTARTS AGE rook-ceph-admission-controller-549f58dd9-rs8qs 1/1 Running 0 86s rook-ceph-admission-controller-549f58dd9-tmfnr 1/1 Running 0 86s rook-ceph-operator-869777bc74-dtv2h 1/1 Running 0 2m9s 此时，operator将自动启动准入控制器部署，而Webhook将开始拦截对Rook资源的请求。 创建ceph集群 /root/rook/cluster/examples/kubernetes/ceph/cluster.yaml 调整镜像tag ceph/ceph:v15.2.13 配置ceph数据盘 修改以下配置，其他默认 storage: # cluster level storage configuration and selection useAllNodes: true nodes: - name: \"node1\" devices: # specific devices to use for storage can be specified for each node - name: \"sdb\" - name: \"node2\" devices: # specific devices to use for storage can be specified for each node - name: \"sdb\" - name: \"node3\" devices: # specific devices to use for storage can be specified for each node - name: \"sdb\" 发布 kubectl apply -f /root/rook/cluster/examples/kubernetes/ceph/cluster.yaml 查看状态 查看pod [root@node1 ceph]# kubectl get pod -n rook-ceph NAME READY STATUS RESTARTS AGE csi-cephfsplugin-jsw67 3/3 Running 0 55s csi-cephfsplugin-provisioner-6667846fc9-g4jj2 6/6 Running 0 52s csi-cephfsplugin-provisioner-6667846fc9-wmzc7 6/6 Running 0 53s csi-cephfsplugin-tb4mh 3/3 Running 0 55s csi-cephfsplugin-xvk4s 3/3 Running 0 55s csi-rbdplugin-gl4tx 3/3 Running 0 57s csi-rbdplugin-provisioner-7bf5687dcd-4rsst 6/6 Running 0 57s csi-rbdplugin-provisioner-7bf5687dcd-7hf94 6/6 Running 0 57s csi-rbdplugin-x57s2 3/3 Running 0 57s csi-rbdplugin-zs64m 3/3 Running 0 57s rook-ceph-admission-controller-549f58dd9-rs8qs 1/1 Running 0 18h rook-ceph-admission-controller-549f58dd9-tmfnr 1/1 Running 0 18h rook-ceph-mon-a-canary-6d7bf84b9c-pjmd4 1/1 Running 0 50s rook-ceph-mon-b-canary-5b647bf496-pdvhj 1/1 Running 0 50s rook-ceph-mon-c-canary-54d67bb847-8pb7x 1/1 Running 0 49s rook-ceph-operator-869777bc74-dtv2h 1/1 Running 0 18h 查看集群状态 kubectl -n rook-ceph get CephCluster -o yaml 创建ceph工具箱 相当于ceph客户端，用于与ceph集群交互 创建发布 离线环境注意替换镜像tag(rook/ceph:v1.6.6) cat 测试可用性 [root@node1 ceph]# kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash 创建块存储 创建块存储池 cat 创建块存储存储类 cat 查看存储类 [root@node1 ceph]# kubectl get sc rook-ceph-block NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE rook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate false 3s 验证可用性 cat > /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done' volumeMounts: - mountPath: /mnt/store name: ceph-rbd-storage EOF 查看pvc状态 [root@node1 kubernetes]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-c1d19cfe-5028-43de-8cbd-405af96eb66c 20Gi RWO rook-ceph-block 93s 查看pod状态 [root@node1 ceph]# kubectl get pod NAME READY STATUS RESTARTS AGE ceph-block-pod 1/1 Running 0 13s 查看写入内容是否正确 [root@node1 ceph]# kubectl exec ceph-block-pod -- cat /mnt/store/greet.txt Tue Jun 29 16:20:07 CST 2021 [node1] Hello from Ceph RBD PV. 创建共享存储 一个共享的文件系统支持多pod读/写，这对于可以使用共享文件系统进行集群的应用程序可能很有用。 创建文件系统CRD cat 查看ceph mds服务状态 [root@node1 ceph]# kubectl -n rook-ceph get pod -l app=rook-ceph-mds NAME READY STATUS RESTARTS AGE rook-ceph-mds-myfs-a-5c9d84c7f8-z4csx 1/1 Running 0 48s rook-ceph-mds-myfs-b-5485989ff8-zl7g2 1/1 Running 0 47s 查看池 可以看到池myfs-data0（数据池）与myfs-metadata（元数据池）已自动创建 [root@node1 ceph]# kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph df --- RAW STORAGE --- CLASS SIZE AVAIL USED RAW USED %RAW USED hdd 300 GiB 297 GiB 137 MiB 3.1 GiB 1.04 TOTAL 300 GiB 297 GiB 137 MiB 3.1 GiB 1.04 --- POOLS --- POOL ID PGS STORED OBJECTS USED %USED MAX AVAIL device_health_metrics 1 1 0 B 0 0 B 0 94 GiB rbd-pool 2 32 31 MiB 154 119 MiB 0.04 94 GiB myfs-metadata 3 32 2.2 KiB 22 1.5 MiB 0 94 GiB myfs-data0 4 32 0 B 0 0 B 0 94 GiB 创建存储类（sc） cat 查看已有存储类 [root@node1 ceph]# kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE openebs-hostpath (default) openebs.io/local Delete WaitForFirstConsumer false 3d4h rook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate false 3h39m rook-cephfs rook-ceph.cephfs.csi.ceph.com Delete Immediate false 4s 测试可用性 离线环境注意替换镜像tag cat > kube-registry.yaml 发布 kubectl apply -f kube-registry.yaml 查看运行状态 [root@node1 ceph]# kubectl get pod -A|grep regi kube-system kube-registry-64f97cd5d6-f6hfz 1/1 Running 0 7m20s kube-system kube-registry-64f97cd5d6-l5h5m 1/1 Running 0 7m20s kube-system kube-registry-64f97cd5d6-nrvkc 1/1 Running 0 7m20s 开启控制面板 安装上述步骤搭建ceph集群，自带dashboard 查看服务信息 [root@node1 ceph]# kubectl -n rook-ceph get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.233.41.137 8080/TCP,8081/TCP 80m csi-rbdplugin-metrics ClusterIP 10.233.53.251 8080/TCP,8081/TCP 80m rook-ceph-admission-controller ClusterIP 10.233.18.242 443/TCP 19h rook-ceph-mgr ClusterIP 10.233.32.164 9283/TCP 77m rook-ceph-mgr-dashboard ClusterIP 10.233.38.102 8443/TCP 77m rook-ceph-mon-a ClusterIP 10.233.35.191 6789/TCP,3300/TCP 79m rook-ceph-mon-b ClusterIP 10.233.39.246 6789/TCP,3300/TCP 78m rook-ceph-mon-c ClusterIP 10.233.30.204 6789/TCP,3300/TCP 78m 创建NodePort类型服务 cat > /root/rook/cluster/examples/kubernetes/ceph/dashboard-external-https.yaml 发布 kubectl apply -f /root/rook/cluster/examples/kubernetes/ceph/dashboard-external-https.yaml 获取登录信息 登录地址（http://节点IP:32445） [root@node1 ceph]# kubectl get svc rook-ceph-mgr-dashboard-external-https -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE rook-ceph-mgr-dashboard-external-https NodePort 10.233.39.95 8443:32445/TCP 21s 登录口令 kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\"{['data']['password']}\" | base64 --decode && echo 查看控制面板 主页包含集群状态、存储容量、存储池等 存储池信息 存储osd信息 块存储信息 文件系统信息 经过上述实践，我们发现在现有k8s集群搭建一套ceph存储只要满足以下两点即可： 内核高于4.17 未被格式化分区的存储设备 整个部署流程对比非容器化部署更容易上手，利用k8s天生优势，保证了存储服务的高可用性。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/storage/Velero.html":{"url":"2.容器/k8s/storage/Velero.html","title":"Velero","keywords":"","body":"Velero 基于v1.6 介绍 Velero（以前叫Heptio Ark）提供了备份和恢复Kubernetes集群资源和持久卷的工具。 Velero主要功能如下： 备份k8s集群，并在集群丢失时恢复 迁移k8s集群资源至另一个集群 将生产集群复制到开发和测试集群 Velero组件: 服务端：运行于k8s集群内部 客户端：cli工具 工作原理 每个Velero操作（按需备份、定时备份、还原）都是一个自定义资源，由Kubernetes自定义资源定义（CRD）定义并存储在etcd中。 Velero还包括处理自定义资源以执行备份、恢复和所有相关操作的控制器。 可以备份或还原群集中的所有对象，也可以按类型、命名空间和/或标签筛选对象。 Velero非常适合于灾难恢复用例，以及在集群上执行系统操作（如升级）之前对应用程序状态进行快照 按需备份 备份操作包括： 将复制的Kubernetes对象上传到云对象存储中 调用云提供程序API以生成持久卷的磁盘快照（如果指定） 您可以选择指定备份期间要执行的备份hook。 例如，在拍摄快照之前，您可能需要通知数据库将其内存缓冲区刷新到磁盘。 请注意，群集备份并不是严格的原子备份。 如果备份时正在创建或编辑Kubernetes对象，则备份中可能不包括这些对象。 捕捉不一致信息的几率很低，但这是可能的 定时备份 定时备份操作允许您定期备份数据。 第一次备份是在第一次创建计划时执行的，随后的备份将按计划的指定间隔进行。这些间隔由Cron表达式指定。 定时备份以名称-保存，其中的格式为YYYYMMDDhhmmss 数据恢复/还原 还原操作允许您从以前创建的备份还原所有对象和持久卷。也可以仅还原对象和持久卷的筛选子集。 Velero支持多个命名空间重新映射—例如: 命名空间abc中的对象可以在命名空间def下重新创建 命名空间123中的对象可以在456下重新创建 还原的默认名称是-，其中的格式为YYYYMMDDhhmmss。也可以指定自定义名称。 还原的对象还包括一个带有键velero.io/restore-name和值的标签。 默认情况下，以读写模式创建备份存储位置。 但是，在还原过程中，可以将备份存储位置配置为只读模式，这将禁用存储位置的备份创建和删除。 这有助于确保在还原场景中不会无意中创建或删除备份。 您可以选择指定要在还原期间或资源还原之后执行的还原hook。 例如，您可能需要在启动数据库应用程序容器之前执行自定义数据库还原操作。 备份操作流程 当你执行velero backup create test-backup时： Velero客户端调用kubernetes api服务来创建备份对象 BackupController控制器发现新的备份对象并对其执行验证 BackupController控制器开始备份过程。它通过查询API服务的资源来收集要备份的数据。 BackupController调用对象存储服务（例如，AWS S3）来上载备份文件 默认情况下，velero backup create为任何持久卷创建磁盘快照。 您可以通过指定其他标志来调整快照。运行velero backup create--帮助查看可用标志。 可以使用选项--snapshot volumes=false禁用快照。 备份的API版本 Velero为每个组/资源使用Kubernetes API服务器的首选版本备份资源。 还原资源时，目标群集中必须存在相同的API组/版本才能成功还原。 例如，如果要备份的集群在things API组中有一个gizmos资源， 其中包含group/versions things/v1alpha1、things/v1beta1和things/v1， 并且服务器的首选组/版本是things/v1，那么将从things/v1api端点备份所有gizmos。 还原此群集的备份时，目标群集必须具有things/v1端点才能还原gizmo。 注意，things/v1不需要是目标集群中的首选版本；它只需要存在。 配置备份过期时间 创建备份时，可以通过添加标志--TTL来指定TTL（生存时间）。 如果Velero发现现有备份资源已过期，它将删除： 备份资源对象 来自云对象存储的备份文件 所有PersistentVolume快照 所有相关恢复数据 TTL标志允许用户使用以小时、分钟和秒为单位的值指定备份保留期， 格式为TTL 24h0m0s。如果未指定，将应用默认的TTL值30天。 同步对象存储与集群的备份信息 Velero会不断检查是否始终存在正确的备份资源。 如果存储桶中有格式正确的备份文件，但Kubernetes API中没有相应的备份资源， 则Velero会将对象存储中的信息同步到Kubernetes。 这使得恢复功能可以在群集迁移场景中工作，其中新群集中不存在原始备份对象。 同样，如果备份对象存在于Kubernetes中，但不在对象存储中， 那么它将从Kubernetes中删除，因为备份tarball不再存在。 安装Velero 基础安装 安装环境要求 k8s主节点 kubectl可用 Velero使用对象存储来存储备份和相关的工件。它还可以选择与受支持的块存储系统集成以快照持久卷。 在开始安装过程之前，应该从兼容提供程序列表中标识要使用的对象存储提供程序和可选块存储提供程序。 安装cli工具 下载velero-v1.6.1-linux-amd64.tar.gz 解压安装 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/troubleshoot/rook.html":{"url":"2.容器/k8s/troubleshoot/rook.html","title":"rook","keywords":"","body":"挂载Pvc失败提示已被占用 错误描述： image replicapool/csi-vol-1d14612c-6e37-11ee-a86b-5a729ca4e4e3 is still being used 解决流程： 查询rbd对象管理端Pod for pod in `kubectl -n rook-ceph get pods|grep rbdplugin|grep -v provisioner|awk '{print $1}'`; do echo $pod; kubectl exec -it -n rook-ceph $pod -c csi-rbdplugin -- rbd device list; done|grep csi-vol-1d14612c-6e37-11ee-a86b-5a729ca4e4e3 -C 3 连接rbd csi-rbdplugin kubectl exec -it csi-rbdplugin-8w8wx -n rook-ceph -c csi-rbdplugin -- bash 确认 rbd device list|grep csi-vol-1d14612c-6e37-11ee-a86b-5a729ca4e4e3 0 replicapool csi-vol-1d14612c-6e37-11ee-a86b-5a729ca4e4e3 - /dev/rbd0 强制卸载映射 rbd unmap -o force /dev/rbd0 恢复使用 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/uninstall/uninstall.html":{"url":"2.容器/k8s/uninstall/uninstall.html","title":"uninstall","keywords":"","body":"sudo rm -rvf $HOME/.kube sudo rm -rvf ~/.kube/ sudo rm -rvf /etc/kubernetes/ sudo rm -rvf /etc/systemd/system/kubelet.service.d sudo rm -rvf /etc/systemd/system/kubelet.service sudo rm -rvf /usr/bin/kube* sudo rm -rvf /etc/cni sudo rm -rvf /opt/cni sudo rm -rvf /var/lib/etcd sudo rm -rvf /var/etcd docker kill $(docker ps -a -q) docker rm $(docker ps -a -q) docker rmi -f $(docker images -q) yum remove docker-ce -y rm -rf /var/lib/docker* rm -f /usr/bin/docker rm -rf /var/lib/kubelet mount | grep '/var/lib/kubelet'| awk '{print $3}'|xargs sudo umount sudo iptables -F && sudo iptables -X && sudo iptables -F -t nat && sudo iptables -X -t nat Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/workload/pod/":{"url":"2.容器/k8s/workload/pod/","title":"pod","keywords":"","body":"kubernetes Pod到底是什么？ 官方文档 对其有详细描述及定义 提到Pod我们不得不了解下容器，pod实质是一个容器集合。 什么是容器？ 我们所说的容器本身其实并不存在，是一个抽象的概念。在Linux中没有所谓的容器，容器的实质是一个视图被隔离、资源受限的进程。 众所周知，容器是使用Linux内核的两个特性——命名空间和cgroup来执行的普通进程。 命名空间/名称空间 命名空间允许您为进程提供一个视图，该视图隐藏那些名称空间之外的所有内容，从而为进程提供自己的运行环境。 常用命名空间包括: Cgroup命名空间: 隔离控制组根目录 (Linux 内核4.6新增) IPC命名空间: 隔离系统进程通信等 (Linux 内核2.6.19新增) MNT命名空间: 隔离挂载点(Linux 内核2.4.19新增) NET命名空间: 隔离网络设备、栈、端口等（Linux 内核2.6.24新增） PID命名空间: 隔离进程ID（Linux 内核2.6.24新增） USER命名空间: 隔离用户、用户组ID（Linux 内核2.6.23新增，Linux 内核3.8完善） UTS命名空间: 隔离主机名和NIS域名（Linux 内核2.6.19新增） 控制组 默认情况下，一个进程可以使用它所运行的物理机器上的所有资源，从而消耗其他进程的资源。 为了限制这种情况，Linux有一个叫做cgroups的特性。进程可以在cgroup中运行，就像一个命名空间，但是cgroup限制了进程可以使用的资源。 这些资源包括CPU、RAM、块I/O、网络I/O等。CPU通常受微核(单个CPU核心的1/1000)的限制，内存受RAM字节的限制。 进程本身可以正常运行，但它只能使用cgroup所允许的CPU，如果超过cgroup设置的内存限制，就会出现内存不足错误。 linux通过命名空间与控制组使进程视图被隔离、资源受限，此时该进程即可称为一个容器。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/workload/pod/01静态pod.html":{"url":"2.容器/k8s/workload/pod/01静态pod.html","title":"01静态pod","keywords":"","body":"背景介绍 最近翻看kubelet源码，看到配置项的时候，发现了静态pod这一概念。 而本人也是使用kubesphere对k8s进行管理的，其中kubesphere启动k8s控制节点服务（控制器、调度器、apiserver）也是通过静态pod进行管理的。 由此，想学习学习静态pod的概念。 静态pod 什么是静态pod？ 由kubelet直接管理的pod被称为静态pod 我们通常通过以下方式创建pod: 调用api-server创建一个pod类型资源 cat 调用api-server创建一个管理pod的控制器类型资源（如: Deployment、StatefulSet） cat 实际调用流程大致为：kubectl -> apiserver -> pod配置清单写入etcd后，调用scheduler获取调度节点，将调度信息写入etcd -> 通知Kubelet进行创建 -> 调用容器运行时创建容器 而静态pod的创建流程非常简单: kubelet -> 调用容器运行时创建容器 那么请思考一个问题: kubelet直接管理的Pod为什么还能通过apiserver获取到？ 这里我们举个例子： kube-apiserver、kube-controller-manager、kube-scheduler通过以静态pod的方式创建。 $ kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-7b4d558c97-qnbw4 1/1 Running 4 91d calico-node-dwh57 1/1 Running 4 91d coredns-7bc876b67f-l99ls 1/1 Running 4 91d coredns-7bc876b67f-pdfpn 1/1 Running 4 91d kube-apiserver-node1 1/1 Running 12 91d kube-controller-manager-node1 1/1 Running 13 91d kube-proxy-rwcw9 1/1 Running 8 91d kube-scheduler-node1 1/1 Running 14 91d metrics-server-784d57f6c7-l2svw 1/1 Running 10 91d nodelocaldns-648pv 1/1 Running 6 91d snapshot-controller-0 1/1 Running 4 91d 我们看下apiserver的pod描述，Controlled By说明了该pod由Node/node1管理 $ kubectl describe pod kube-apiserver-node1 -n kube-system Name: kube-apiserver-node1 Namespace: kube-system Priority: 2000000000 Priority Class Name: system-cluster-critical Node: node1/192.168.1.1 Start Time: Sat, 23 Oct 2021 21:52:41 +0800 Labels: component=kube-apiserver tier=control-plane Controlled By: Node/node1 ... 我们看下coredns pod的描述，作下对比。（很明显coredns pod由ReplicaSet控制器管理） $ kubectl describe pod coredns-7bc876b67f-l99ls -n kube-system Name: coredns-7bc876b67f-l99ls Namespace: kube-system Priority: 2000000000 Priority Class Name: system-cluster-critical Node: node1/x.x.x.x Start Time: Tue, 17 Aug 2021 13:32:32 +0800 Labels: k8s-app=kube-dns pod-template-hash=7bc876b67f Annotations: cni.projectcalico.org/podIP: 10.233.90.198/32 cni.projectcalico.org/podIPs: 10.233.90.198/32 Status: Running IP: 10.233.90.198 IPs: IP: 10.233.90.198 Controlled By: ReplicaSet/coredns-7bc876b67f ... 回到上述问题：kubelet直接管理的Pod为什么还能通过apiserver获取到？ 因为kubelet会为每个它管理的静态pod，调用api-server创建一个对应的pod镜像。 由此以来，静态pod也能通过kubectl等方式进行访问，与其他控制器创建出来的pod看起来没有什么区别。 如何使用静态pod？ 通过kubelet进行配置: /var/lib/kubelet/config.yaml为kubelet配置文件（启动时通过--config=/var/lib/kubelet/config.yaml指定） staticPodPath字段为静态pod路径 $ cat /var/lib/kubelet/config.yaml apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.crt authorization: mode: Webhook webhook: cacheAuthorizedTTL: 0s cacheUnauthorizedTTL: 0s clusterDNS: - 169.254.25.10 clusterDomain: cluster.local cpuManagerReconcilePeriod: 0s evictionHard: memory.available: 5% evictionMaxPodGracePeriod: 120 evictionPressureTransitionPeriod: 30s evictionSoft: memory.available: 10% evictionSoftGracePeriod: memory.available: 2m featureGates: CSINodeInfo: true ExpandCSIVolumes: true RotateKubeletClientCertificate: true VolumeSnapshotDataSource: true fileCheckFrequency: 0s healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 0s imageMinimumGCAge: 0s kind: KubeletConfiguration kubeReserved: cpu: 200m memory: 250Mi maxPods: 300 nodeStatusReportFrequency: 0s nodeStatusUpdateFrequency: 0s rotateCertificates: true runtimeRequestTimeout: 0s staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 0s syncFrequency: 0s systemReserved: cpu: 200m memory: 250Mi volumeStatsAggPeriod: 0s 我们看下/etc/kubernetes/manifests路径下静态Pod配置 实际上就是我们前文例子中的：kube-apiserver、kube-controller-manager、kube-scheduler服务 $ ls /etc/kubernetes/manifests kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml kube-controller-manager.yaml文件内容: $ cat /etc/kubernetes/manifests/kube-controller-manager.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: component: kube-controller-manager tier: control-plane name: kube-controller-manager namespace: kube-system spec: containers: - command: - kube-controller-manager - --allocate-node-cidrs=true - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf - --bind-address=0.0.0.0 - --client-ca-file=/etc/kubernetes/pki/ca.crt - --cluster-cidr=10.233.64.0/18 - --cluster-name=cluster.local - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key - --controllers=*,bootstrapsigner,tokencleaner - --experimental-cluster-signing-duration=87600h - --feature-gates=CSINodeInfo=true,VolumeSnapshotDataSource=true,ExpandCSIVolumes=true,RotateKubeletClientCertificate=true - --kubeconfig=/etc/kubernetes/controller-manager.conf - --leader-elect=true - --node-cidr-mask-size=24 - --port=10252 - --profiling=False - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt - --root-ca-file=/etc/kubernetes/pki/ca.crt - --service-account-private-key-file=/etc/kubernetes/pki/sa.key - --service-cluster-ip-range=10.233.0.0/18 - --use-service-account-credentials=true image: kubesphere/kube-controller-manager:v1.18.6 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: path: /healthz port: 10257 scheme: HTTPS initialDelaySeconds: 15 timeoutSeconds: 15 name: kube-controller-manager resources: requests: cpu: 200m volumeMounts: - mountPath: /etc/ssl/certs name: ca-certs readOnly: true - mountPath: /etc/pki name: etc-pki readOnly: true - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec name: flexvolume-dir - mountPath: /etc/localtime name: host-time readOnly: true - mountPath: /etc/kubernetes/pki name: k8s-certs readOnly: true - mountPath: /etc/kubernetes/controller-manager.conf name: kubeconfig readOnly: true hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /etc/ssl/certs type: DirectoryOrCreate name: ca-certs - hostPath: path: /etc/pki type: DirectoryOrCreate name: etc-pki - hostPath: path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec type: DirectoryOrCreate name: flexvolume-dir - hostPath: path: /etc/localtime type: \"\" name: host-time - hostPath: path: /etc/kubernetes/pki type: DirectoryOrCreate name: k8s-certs - hostPath: path: /etc/kubernetes/controller-manager.conf type: FileOrCreate name: kubeconfig status: {} 参考文章 Create static Pods Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/workload/pod/Pod安全策略.html":{"url":"2.容器/k8s/workload/pod/Pod安全策略.html","title":"Pod安全策略","keywords":"","body":"Pod安全策略 pod-security-policy 特性状态: Kubernetes v1.21 [deprecated] PodSecurityPolicy在Kubernetes v1.21中已弃用，将在v1.25中被删除。有关弃用的更多信息，移步PodSecurityPolicy Deprecation: Past, Present, and Future Pod安全策略支持对Pod创建和更新的细粒度授权。 Pod安全策略是用来做什么的？ Pod安全策略是一个集群级资源，它控制Pod规范的安全方面。 PodSecurityPolicy(PSP)对象定义了pod要进入系统必须运行的一组条件，以及相关字段的默认值。它们允许管理员控制以下内容: 控制面 字段名称 运行特权容器 privileged 主机命名空间的使用 hostPID, hostIPC 主机网络和端口的使用 hostNetwork, hostPorts 卷类型的使用 volumes 主机文件系统的使用 allowedHostPaths 允许特定的FlexVolume驱动程序 allowedFlexVolumes 分配一个拥有pod卷的FSGroup fsGroup 需要使用只读的根文件系统 readOnlyRootFilesystem 容器的用户和组id runAsUser, runAsGroup, supplementalGroups 限制升级到根权限 allowPrivilegeEscalation, defaultAllowPrivilegeEscalation Linux capabilities defaultAddCapabilities, requiredDropCapabilities, allowedCapabilities 容器的SELinux上下文 seLinux 容器允许的Proc挂载类型 allowedProcMountTypes 容器使用的AppArmor配置文件 annotations 容器使用的seccomp配置文件 annotations 容器使用的sysctl配置文件 forbiddenSysctls,allowedUnsafeSysctls 策略解析 Privileged Privileged决定pod中的任何容器是否可以启用特权模式。默认情况下，容器不允许访问主机上的任何设备，但特权容器被授予访问主机上的所有设备的权限。 这允许容器与主机上运行的进程进行几乎相同的访问。这对于希望使用linux cap(如操作网络堆栈和访问设备)的容器(比如CSI容器)很有用 共享主机命名空间 HostPID: 控制pod是否可以共享主机Pid进程命名空间 HostIPC: 控制pod是否可以共享主机IPC命名空间 HostNetwork: 控制pod是否可以使用节点网络名称空间。这样做可以让pod访问环回设备、在本地主机上监听的服务，并且可以用来窥探同一节点上其他pod的网络活动。 HostPorts: 结合HostNetwork提供主机网络名称空间中允许的端口范围的列表。定义为HostPortRange列表，包含min(包括)和max(包括)。默认为不允许主机端口。 卷与文件系统 Volumes: 提供允许的卷类型列表。允许值对应于创建卷时定义的卷源。有关卷类型的完整列表，请参见卷类型 。 此外，*可用于允许所有卷类型。 推荐的最小允许卷集： configMap downwardAPI emptyDir persistentVolumeClaim secret projected 注意: PodSecurityPolicy不会限制可被persistentvolumecclaim引用的PersistentVolume对象类型， hostPath类型的PersistentVolumes不支持只读访问模式。 应该只向受信任的用户授予创建PersistentVolume对象的权限。 FSGroup: 控制应用于某些卷的补充组 MustRunAs: 要求至少指定一个范围。使用第一个范围的最小值作为默认值。针对所有范围进行验证。 MayRunAs: 要求至少指定一个范围。允许不设置FSGroups而不提供默认值。如果设置了FSGroups，则对所有范围进行验证。 RunAsAny: 没有提供默认。允许指定任意fsGroup ID。 AllowedHostPaths 指定hostPath卷允许使用的主机路径列表。空列表意味着对所使用的主机路径没有限制。 它被定义为一个带有单个pathPrefix字段的对象列表，该字段允许hostPath卷挂载以允许前缀开头的路径，并且readOnly字段指示它必须以只读方式挂载。 例如: allowedHostPaths: # This allows \"/foo\", \"/foo/\", \"/foo/bar\" etc., but # disallows \"/fool\", \"/etc/foo\" etc. # \"/foo/../\" is never valid. - pathPrefix: \"/foo\" readOnly: true # only allow read-only mounts 注意： 对主机文件系统具有无限制访问权限的容器可以通过多种方式升级特权，包括从其他容器读取数据，以及滥用系统服务(如Kubelet)的凭证。 可写hostPath目录卷允许容器以在pathPrefix之外遍历主机文件系统的方式写入文件系统。readOnly: true，在Kubernetes 1.11+中可用，必须在所有allowedHostPaths上使用，以有效地限制对指定pathPrefix的访问。 ReadOnlyRootFilesystem 要求容器必须在只读的根文件系统中运行(即没有可写层)。 用户和组 RunAsUser: 控制运行容器的用户ID MustRunAs: 要求至少指定一个范围。使用第一个范围的最小值作为默认值。针对所有范围进行验证 MustRunAsNonRoot: ``: 开启PSP Pod安全策略控制是作为可选的允许控制器实现的，PodSecurityPolicies是通过启用允许控制器来执行的 ，但是在不授权任何策略的情况下执行将会阻止在集群中创建任何pod。 由于pod安全策略API (policy/v1beta1/podsecuritypolicy)是独立于许可控制器启用的， 因此对于现有集群，建议在启用许可控制器之前添加并授权策略。 授权策略 PodSecurityPolicy通过以下两个步骤使用： 创建PodSecurityPolicy 授权请求用户或目标pod的服务帐户使用策略 大多数Kubernetes pod不是由用户直接创建的。 相反，它们通常是通过ControllerManager间接创建的，作为Deployment、ReplicaSet或其他模板化控制器的一部分。 授予控制器对策略的访问权将授予该控制器创建的所有pod的访问权，因此授权策略的首选方法是授予对pod服务帐户的访问权。 通过RBAC RBAC是一种标准的Kubernetes授权模式，可以方便地对策略的使用进行授权。 首先，Role或ClusterRole需要授予使用所需策略的访问权限。授予访问权限的规则如下: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: rules: - apiGroups: ['policy'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: - 然后(Cluster)角色绑定到授权用户: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: roleRef: kind: ClusterRole name: apiGroup: rbac.authorization.k8s.io subjects: # Authorize all service accounts in a namespace (recommended): - kind: Group apiGroup: rbac.authorization.k8s.io name: system:serviceaccounts: # Authorize specific service accounts (not recommended): - kind: ServiceAccount name: namespace: # Authorize specific users (not recommended): - kind: User apiGroup: rbac.authorization.k8s.io name: 如果使用了RoleBinding(不是ClusterRoleBinding)，它将只允许在与绑定相同的名称空间中运行pod。 这可以与系统组配对，以授予对名称空间中运行的所有pod的访问权限: # Authorize all service accounts in a namespace: - kind: Group apiGroup: rbac.authorization.k8s.io name: system:serviceaccounts # Or equivalently, all authenticated users in a namespace: - kind: Group apiGroup: rbac.authorization.k8s.io name: system:authenticated 最佳实践 PodSecurityPolicy正在被一个新的、简化的PodSecurity允许控制器所取代。遵循以下指导方针来简化从PodSecurityPolicy到新的允许控制器的迁移: 将Pod安全策略限制为Pod安全标准定义的策略: Privileged Baseline Restricted 通过system:serviceaccounts:将psp绑定到整个命名空间。例如: apiVersion: rbac.authorization.k8s.io/v1 # This cluster role binding allows all pods in the \"development\" namespace to use the baseline PSP. kind: ClusterRoleBinding metadata: name: psp-baseline-namespaces roleRef: kind: ClusterRole name: psp-baseline apiGroup: rbac.authorization.k8s.io subjects: - kind: Group name: system:serviceaccounts:development apiGroup: rbac.authorization.k8s.io - kind: Group name: system:serviceaccounts:canary apiGroup: rbac.authorization.k8s.io Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/workload/pod/pod摘流.html":{"url":"2.容器/k8s/workload/pod/pod摘流.html","title":"pod摘流","keywords":"","body":"pod摘流 需求背景 针对Deployment的某一故障pod，进行流量摘除并保留该pod用于后续异常排查 摘除流程 注册中心下线 适用于：服务间通过注册中心调用 调用注册中心接口，完成应用下线。如不下线注册中心，其他服务仍有调用到该故障节点的可能 endpoint列表剔除 适用于：服务间通过k8s service调用(dns寻址) 为方便说明我们新建三副本deployment $ cat 此时副本详情信息为 $ kubectl get pod -o wide -l app=nginx-sample -w NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-sample-866788fc6d-cdflv 1/1 Running 0 3m56s 10.233.94.226 node108 nginx-sample-866788fc6d-v6hr5 1/1 Running 0 9m35s 10.233.120.37 node110 nginx-sample-866788fc6d-w2dqr 1/1 Running 0 9m35s 10.233.254.69 node109 服务端点列表 $ kubectl describe svc nginx-sample-svc Name: nginx-sample-svc Namespace: default Labels: app=nginx-sample Annotations: Selector: app=nginx-sample Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.233.43.127 IPs: 10.233.43.127 Port: nginx 80/TCP TargetPort: 80/TCP Endpoints: 10.233.120.37:80,10.233.254.69:80,10.233.94.226:80 Session Affinity: None Events: 此时我们假设nginx-sample-866788fc6d-cdflv副本节点异常（即10.233.94.226），需要摘除流量 修改异常pod标签 $ kubectl label pod nginx-sample-866788fc6d-cdflv app=nginx-sample-debug --overwrite 修改标签由app=nginx-sample变为app=nginx-sample-debug 再次查看pod列表 $ kubectl get pod -o wide -l app=nginx-sample -w NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-sample-866788fc6d-v6hr5 1/1 Running 0 16m 10.233.120.37 node110 nginx-sample-866788fc6d-w2dqr 1/1 Running 0 16m 10.233.254.69 node109 nginx-sample-866788fc6d-w7p7t 1/1 Running 0 49s 10.233.94.36 node108 查看服务列表 $ kubectl describe svc nginx-sample-svc Name: nginx-sample-svc Namespace: default Labels: app=nginx-sample Annotations: Selector: app=nginx-sample Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.233.43.127 IPs: 10.233.43.127 Port: nginx 80/TCP TargetPort: 80/TCP Endpoints: 10.233.120.37:80,10.233.254.69:80,10.233.94.36:80 Session Affinity: None Events: 此时我们发现异常副本节点被剔除，并生成一个新的副本节点 debug异常pod节点 由于异常pod变更了标签，此时它变为了一个孤儿pod，即没有父控制器进行生命周期管理，我们可以通过标签过滤出来 $ kubectl get pod -l app=nginx-sample-debug debug完毕后，通过以下指令进行删除操作 $ kubectl delete pod -l app=nginx-sample-debug Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/workload/pod/Pod配置CAP.html":{"url":"2.容器/k8s/workload/pod/Pod配置CAP.html","title":"Pod配置CAP","keywords":"","body":"Linux CAP介绍与k8s下配置使用 关于capability 发音 美[keɪpəˈbɪləti] 英[keɪpə'bɪləti] 译为能力或功能，一般缩写CAP，以下我们简称Capabilities为CAP CAP历史回溯 从内核2.2开始，Linux将传统上与超级用户root关联的特权划分为不同的单元，称为CAP。 CAP作为线程(Linux并不真正区分进程和线程)的属性存在，每个单元可以独立启用和禁用。 如此一来，权限检查的过程就变成了： 在执行特权操作时，如果进程的有效身份不是root，就去检查是否具有该特权操作所对应的CAP，并以此决定是否可以进行该特权操作。 比如要向进程发送信号(kill())，就得具有CAP_KILL；如果设置系统时间，就得具有CAP_SYS_TIME。 在CAP出现之前，系统进程分为两种： 特权进程 非特权进程 特权进程可以做所有的事情: 进行管理级别的内核调用；而非特权进程被限制为标准用户的子集调用 某些可执行文件需要由标准用户运行，但也需要进行有特权的内核调用，它们需要设置suid位，从而有效地授予它们特权访问权限。(典型的例子是ping，它被授予进行ICMP调用的完全特权访问权。) 这些可执行文件是黑客关注的主要目标——如果他们可以利用其中的漏洞，他们就可以在系统上升级他们的特权级别。 由此内核开发人员提出了一个更微妙的解决方案:CAP。 意图很简单: 将所有可能的特权内核调用划分为相关功能组，赋予进程所需要的功能子集。 因此，内核调用被划分为几十个不同的类别，在很大程度上是成功的。 回到ping的例子，CAP的出现使得它仅被赋予一个CAP_NET_RAW功能，就能实现所需功能，这大大降低了安全风险。 注意： 比较老的操作系统上，会通过为ping添加SUID权限的方式，实现普通用户可使用。 这存在很大的安全隐患，笔者所用操作系统（CentOS7）上ping指令已通过CAP方式实现 $ ls -l /usr/bin/ping -rwxr-xr-x. 1 root root 66176 8月 4 2017 /usr/bin/ping $ getcap /usr/bin/ping /usr/bin/ping = cap_net_admin,cap_net_raw+p 设置容器的CAP Set capabilities for a Container 基于Linux capabilities ，您可以授予某个进程某些特权，而不授予root用户的所有特权。 要为容器添加或删除Linux功能，请在容器清单的securityContext部分中包含capability字段。 首先，看看未设置capability字段时会发生什么。下面是不添加或删除任何CAP的配置文件: apiVersion: v1 kind: Pod metadata: name: security-context-demo-3 spec: containers: - name: sec-ctx-3 image: centos:7 command: [\"tail\",\"-f\", \"/dev/null\"] 创建Pod $ kubectl apply -f https://k8s.io/examples/pods/security/security-context-3.yaml 查看Pod运行状态 $ kubectl get pod security-context-demo-3 在运行的容器中获取一个shell: kubectl exec -it security-context-demo-3 -- sh 在shell中，列出正在运行的进程: $ ps aux 输出显示了容器的进程id (pid): USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 4336 796 ? Ss 18:17 0:00 /bin/sh -c node server.js root 5 0.1 0.5 772124 22700 ? Sl 18:17 0:00 node server.js 在shell中，查看进程1的状态: $ cd /proc/1 $ cat status 输出显示了进程的能力位图: ... CapPrm: 00000000a80425fb CapEff: 00000000a80425fb ... 解码 $ capsh --decode=00000000a80425fb 0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap 记下能力位图，然后退出shell: $ exit 接下来，运行一个与前一个容器相同的容器，只是它有额外的功能集。 运行一个配置增加了CAP_NET_ADMIN和CAP_SYS_TIME功能的Pod: cat 在运行的容器中获取一个shell: kubectl exec -it security-context-demo-4 -- sh 在shell中，查看进程1的状态: $ cd /proc/1 $ cat status 进程的能力位图: ... CapPrm: 00000000aa0435fb CapEff: 00000000aa0435fb ... 进程的能力位图值解码 $ capsh --decode=00000000aa0435fb 0x00000000aa0435fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_admin,cap_net_raw,cap_sys_chroot,cap_sys_time,cap_mknod,cap_audit_write,cap_setfcap 对比两个进程的能力位图（解码后） # 未配置CAP 0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap # 配置CAP_NET_ADMIN和CAP_SYS_TIME 0x00000000aa0435fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_admin,cap_net_raw,cap_sys_chroot,cap_sys_time,cap_mknod,cap_audit_write,cap_setfcap 有关常capability数的定义，请参阅capability.h 。 注意: Linux capability常量的形式是CAP_XXX。 但是，当您在容器清单中列出功能时，必须忽略常量的CAP_部分。 例如，要添加CAP_SYS_TIME，请在功能列表中包含SYS_TIME。 关于进程状态值 这里我们介绍进程状态中与Capabilities相关的几个值: CapInh: 当前进程子进程可继承的能力 CapPrm: 当前进程可使用的能力（可以包含CapEff中没有的能力，CapEff是CapPrm的一个子集，进程放弃没有必要的能力有利于提高安全性） CapEff: 当前进程已使用/开启的能力 1.非容器特权进程CAP缺省值解析（共计35个） $ capsh --decode=000001ffffffffff 0x000001ffffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,35,36,37,38,39,40 cap_chown: 允许修改文件所有者权限 cap_dac_override: 忽略文件的DAC访问权限 cap_dac_read_search: 忽略文件读及目录检索的DAC访问权限 cap_fowner: 忽略文件属主ID必须与进程用户ID一致的权限 cap_fsetid: 确保在文件被修改后不修改setuid/setgid位 cap_kill: 允许对不属于自己的进程发送信号的权限 cap_setgid: 允许修改进程的GID权限 cap_setuid: 允许修改进程的UID权限 cap_setpcap: 允许对子进程进行CAP授权 cap_linux_immutable: 允许修改文件的IMMUTABLE与APPEND属性权限 cap_net_bind_service: 允许绑定小于1024端口的权限 cap_net_broadcast: 允许网络广播及多播访问的权限 cap_net_admin: 允许执行网络管理任务的权限 cap_net_raw: 允许使用原始套接字的权限 cap_ipc_lock: 允许锁定共享内存片段的权限 cap_ipc_owner: 忽略IPC所有权检查的权限 cap_sys_module: 允许插入和删除内核模块的权限 cap_sys_rawio: 允许直接访问/devport,/dev/mem,/dev/kmem及原始块设备的权限 cap_sys_chroot: 允许使用chroot()系统调用的权限 cap_sys_ptrace: 允许追踪任何进程的权限 cap_sys_pacct: 允许执行进程的BSD式审计的权限 cap_sys_admin: 允许执行系统管理任务(如加载或卸载文件系统、设置磁盘配额等)的权限 cap_sys_boot: 允许重启系统的权限 cap_sys_nice: 允许提升优先级及设置其他进程优先级的权限 cap_sys_resource: 忽略资源限制的权限 cap_sys_time: 允许改变系统时钟的权限 cap_sys_tty_config: 允许配置TTY设备的权限 cap_mknod: 允许使用mknod()系统调用的权限 cap_lease: 允许修改文件锁的FL_LEASE标志的权限 cap_audit_write: 允许将记录写入内核审计日志的权限 cap_audit_control: 启用和禁用内核审计、改变审计过滤规则、检索审计状态和过滤规则的权限 cap_setfcap: 允许为可执行文件设置CAP的权限 cap_mac_override: 可覆盖Mandatory Access Control(MAC)的权限 cap_mac_admin: 允许MAC配置或状态改变的权限 cap_syslog: 允许使用syslog()系统调用的权限 2.容器特权进程默认CAP缺省值解析（共计14个） 借用上述例子中未配置CAP的进程能力位图 0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap cap_chown: 允许修改文件所有者权限 cap_dac_override: 忽略文件的DAC访问权限 cap_fowner: 忽略文件属主ID必须与进程用户ID一致的权限 cap_fsetid: 允许设置文件setuid位的权限 cap_kill: 允许对不属于自己的进程发送信号的权限 cap_setgid: 允许修改进程的GID权限 cap_setuid: 允许修改进程的UID权限 cap_setpcap: 允许对子进程进行CAP授权 cap_net_bind_service: 允许绑定小于1024端口的权限 cap_net_raw: 允许使用原始套接字的权限 cap_sys_chroot: 允许使用chroot()系统调用的权限 cap_mknod: 允许使用mknod()系统调用的权限 cap_audit_write: 允许将记录写入内核审计日志的权限 cap_setfcap: 允许为可执行文件设置CAP的权限 对比发现，容器运行时内的root用户并非拥有全部权限，仅仅是默认拥有14条权限，其他权限如果使用需要额外开启。 3.查看容器非特权进程默认CAP缺省值（0个） $ id uid=1000 gid=0(root) groups=0(root) $ cat /proc/1/status|grep CapEff CapEff: 0000000000000000 思考一个问题: 当运行时为非特权用户，CAP配置是否生效？ Deployment配置如下（镜像以非特权USER运行） kind: Deployment apiVersion: apps/v1 metadata: name: eureka-app namespace: champ labels: app: eureka-app app.kubernetes.io/instance: eureka-app annotations: configmap.reloader.stakater.com/reload: eureka-app-cm deployment.kubernetes.io/revision: '25' spec: replicas: 1 selector: matchLabels: app: eureka-app template: metadata: labels: app: eureka-app annotations: kubesphere.io/containerSecrets: '' spec: containers: - name: eureka-app image: 'xxx.xxx.xxx/xxx/xxx:xxx' ports: - name: http-8080 containerPort: 8080 protocol: TCP - name: http-5005 containerPort: 5005 protocol: TCP securityContext: capabilities: add: - SYS_TIME ... 查看进程状态 $ cat /proc/1/status CapPrm: 0000000000000000 CapEff: 0000000000000000 显然当镜像指定USER为非特权用户运行时，CAP配置并不生效 结论 当镜像指定USER为非特权用户运行时，CAP配置并不生效 容器内特权进程默认拥有14条CAP权限配置，相对非容器特权进程要少的多 Linux CAP旨在将特权细粒度划分 参考文献 Linux Capabilities 简介 Linux Capabilities: Why They Exist and How They Work Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/workload/pod/为Pod配置服务账号.html":{"url":"2.容器/k8s/workload/pod/为Pod配置服务账号.html","title":"为Pod配置服务账号","keywords":"","body":"Configure Service Accounts for Pods 服务帐户为在Pod中运行的进程提供标识。 当访问k8s集群(例如，使用kubectl)时，kube-apiserver将对用户帐户(目前通常是admin帐户)进行认证鉴权。 而当Pod内容器中的进程访问kube-apiserver时，是通过特定的服务帐户(例如，default)进行身份验证。 使用默认的服务账号访问apiserver 在创建pod时，如果没有指定服务帐户，会在pod所在名称空间中自动为它分配默认的服务帐户。 可以通过以下命令查询pod的服务账号： $ kubectl get pod redis-0 -n ddd -o yaml|grep serviceAccountName f:serviceAccountName: {} serviceAccountName: default 您可以使用自动挂载的服务帐户凭据从pod内部访问API，如访问集群中所述。 服务帐户的API权限取决于使用的授权插件和授权策略。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/workload/pod/为容器设置Seccomp配置文件.html":{"url":"2.容器/k8s/workload/pod/为容器设置Seccomp配置文件.html","title":"为容器设置Seccomp配置文件","keywords":"","body":"为容器设置Seccomp配置文件 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/workload/endpoint.html":{"url":"2.容器/k8s/workload/endpoint.html","title":"endpoint","keywords":"","body":"endpoint 将外部服务映射为集群内布，便于配置应用路由等 创建endpoint对象 addresses: 数组类型，可以为多个，也可以为一个 namespace: 命名空间 cat 创建service对象 port: 与endpoint一致 metadata.name: 与endpoint一致 metadata.namespace: 与endpoint一致 cat Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/workload/job.html":{"url":"2.容器/k8s/workload/job.html","title":"job","keywords":"","body":"Job Job创建一个或多个pod，并将继续重试pod的执行，直到指定数量的pod成功终止。 当pods成功完成时，Job将跟踪成功的完成。 当达到指定的成功完成次数时，Job（即作业）即完成。 删除Job将清理它创建的pod，暂停Job将删除其活动pod，直到作业再次恢复。 一个简单的例子是创建一个作业对象，以便可靠地运行一个Pod以完成任务。如果第一个Pod失败或被删除（例如由于节点硬件故障或节点重新启动），作业对象将启动一个新的Pod。 还可以使用作业并行运行多个pod。 job类型 适合以Job形式来运行的任务主要有三种 1.非并行Job: 通常只启动一个Pod，除非该Pod失败 当Pod成功终止时，立即视Job为完成状态 2.具有确定完成计数(completions)的并行Job: .spec.completions字段设置为非0的正数值 Job用来代表整个任务，当成功的Pod个数达到.spec.completions时，Job被视为完成 3.带工作队列(parallelism)的并行Job: 不设置spec.completions，默认值为.spec.parallelism 多个Pod之间必须相互协调，或者借助外部服务确定每个Pod要处理哪个工作条目。 例如，任一Pod都可以从工作队列中取走最多N个工作条目 每个Pod都可以独立确定是否其它Pod都已完成，进而确定Job是否完成 当Job中任何Pod成功终止，不再创建新Pod 一旦至少1个Pod成功完成，并且所有Pod都已终止，即可宣告Job成功完成 一旦任何Pod成功退出，任何其它Pod都不应再对此任务执行任何操作或生成任何输出。 所有Pod都应启动退出过程 第三种Job适用于并行计算？ 对于非并行的Job，你可以不设置spec.completions和spec.parallelism。 这两个属性都不设置时，均取默认值1，即第一种类型Job 对于确定完成计数类型的Job，你应该设置.spec.completions为所需要的完成个数。 你可以设置.spec.parallelism，也可以不设置，其默认值为1 控制并行性 并行性请求（.spec.parallelism）可以设置为任何非负整数。如果未设置，则默认为1。 如果设置为0，则Job相当于启动之后便被暂停，直到此值被增加 实际并行性（在任意时刻运行状态的Pods个数）可能比并行性请求略大或略小，原因如下: 对于确定完成计数Job，实际上并行执行的Pods个数不会超出剩余的完成数。 如果.spec.parallelism值较高，会被忽略。 对于工作队列Job，有任何Job成功结束之后，不会有新的Pod启动。不过，剩下的Pods允许执行完毕 如果Job控制器没有时间作出反应 如果Job控制器因为任何原因（例如，缺少ResourceQuota或者没有权限）无法创建Pods。 Pods个数可能比请求的数目小 Job控制器可能会因为之前同一Job中Pod失效次数过多而抑制新Pod的创建 当Pod被优雅地关闭时，它需要一段时间才能停止 IndexedJob特性 特性状态 FEATURE STATE: Kubernetes v1.21 [alpha] 开启方式 API server与controller manager服务通过添加--feature-gates=\"IndexedJob=true\"开启IndexedJob特性 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/更新列表/v1.22/alpha-ReadWriteOncePod访问模式.html":{"url":"2.容器/k8s/更新列表/v1.22/alpha-ReadWriteOncePod访问模式.html","title":"alpha-ReadWriteOncePod访问模式","keywords":"","body":" 卷的ReadWriteOncePod访问模式 ReadWriteOncePod是什么？有何应用场景？ ReadWriteOncePod原理 对比ReadWriteOnce访问模式 我们如何使用ReadWriteOnce？ 使用样例 变更现有卷访问模式为ReadWriteOnce 哪些卷插件支持ReadWriteOncePod？ 作为CSI提供者，如何支持ReadWriteOncePod？ 卷的ReadWriteOncePod访问模式 Introducing Single Pod Access Mode for PersistentVolumes Author: Chris Henzie (Google) 随着Kubernetes v1.22版本的更新，k8s为我们带来了一个新的alpha特性：存储卷新的访问方式 -> ReadWriteOncePod（单Pod访问类型的pv与pvc），换句话来讲， 指定pvc访问类型为ReadWriteOncePod时，仅有一个Pod可以访问使用该pvc（持化卷声明） ReadWriteOncePod是什么？有何应用场景？ 当我们使用存储的时候，有很多不同的消费存储模式： 多节点读写：如通过网络共享的文件系统（NFS、Cephfs） 单节点读写：高度敏感的存储数据 多点只读 在k8s世界，可通过对存储卷（pv、pvc）指定Access Modes（访问模式），实现对存储的消费方式。 如多节点读写： kind: PersistentVolumeClaim apiVersion: v1 metadata: name: shared-cache spec: accessModes: - ReadWriteMany # Allow many pods to access shared-cache simultaneously. resources: requests: storage: 1Gi Kubernetes v1.22版本之前，对存储卷有三种方式： ReadWriteOnce：单节点读写 ReadOnlyMany ：多节点只读 ReadWriteMany：多节点读写 以上三种对存储卷访问方式的控制，是通过kube-controller-manager和kubelet组件实现。 ReadWriteOncePod原理 Kubernetes v1.22提供了第四种访问PV、PVC的访问模式：ReadWriteOncePod（单一Pod访问方式） 当你创建一个带有pvc访问模式为ReadWriteOncePod的Pod A时，Kubernetes确保整个集群内只有一个Pod可以读写该PVC。 此时如果你创建Pod B并引用了与Pod A相同的PVC(ReadWriteOncePod)时，那么Pod B会由于该pvc被Pod A引用而启动失败。 Pod B事件可能如下： Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 1s default-scheduler 0/1 nodes are available: 1 node has pod using PersistentVolumeClaim with the same name and ReadWriteOncePod access mode. 乍一看，是不是觉得与ReadWriteOnce访问模式很像？但其实并不一样。 对比ReadWriteOnce访问模式 ReadWriteOnce：该访问模式约束仅有一个node节点可以访问pvc。换句话来说，同一node节点的不同pod是可以对同一pvc进行读写的 这种访问模式对于一些应用是存在隐患的，特别是对数据有写入安全（同一时间仅有一个写操作）要求的应用。 ReadWriteOncePod的出现，解决了上述隐患。 我们如何使用ReadWriteOnce？ ReadWriteOncePod方式模式是Kubernetes v1.22版本的alpha特性，并且只支持CSI类型的卷 k8s版本需为v1.22+ 首先需要k8s集群需添加该特性门控（k8s中alpha功能特性默认关闭，beta功能特性默认开启） 涉及服务组件: kube-apiserver kube-scheduler kubelet --feature-gates=\"...,ReadWriteOncePod=true\" 升级csi边车，版本要求如下： csi-provisioner:v3.0.0+ csi-attacher:v3.3.0+ csi-resizer:v1.3.0+ 使用样例 pvc声明样例 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: single-writer-only spec: accessModes: - ReadWriteOncePod # Allow only a single pod to access single-writer-only. resources: requests: storage: 1Gi 如果您的存储插件支持动态配置（StorageClass），那么将使用ReadWriteOncePod访问模式创建新的PersistentVolumes 变更现有卷访问模式为ReadWriteOnce 您可以变更现有PVC访问模式为ReadWriteOncePod访问模式。接下来我们通过一个迁移样例，了解迁移的流程。 样例信息： pv: cat-pictures-pv pvc: cat-pictures-pvc Deployment: cat-pictures-writer 命名空间: default 三者关系为：名为cat-pictures-writer的Deployment，声明挂载了一个名为cat-pictures-pvc的pvc，该pvc对应的pv 为cat-pictures-pv step1: 变更pv回收策略 变更cat-pictures-pv的回收策略为Retain，确保删除pvc时，对应的pv不会被删除 kubectl patch pv cat-pictures-pv -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}' step2: 停止Deployment下所有工作负载（缩容至0） kubectl scale --replicas=0 deployment cat-pictures-writer step3: 删除cat-pictures-pvc kubectl delete pvc cat-pictures-pvc step4: 清理cat-pictures-pv的spec.claimRef.uid属性，确保重新创建pvc时可以绑定新的pvc kubectl patch pv cat-pictures-pv -p '{\"spec\":{\"claimRef\":{\"uid\":\"\"}}}' step5: 变更cat-pictures-pv的访问模式为ReadWriteOncePod kubectl patch pv cat-pictures-pv -p '{\"spec\":{\"accessModes\":[\"ReadWriteOncePod\"]}}' 注意：ReadWriteOncePod不能与其他访问模式结合使用，确保ReadWriteOncePod是PV的唯一访问模式，否则无法成功绑定。 step6: 变更cat-pictures-pvc的访问模式为ReadWriteOncePod(且唯一) 未配置StorageClass情况需手动创建pvc kubectl apply -f cat-pictures-pvc.yaml kubectl apply -f cat-pictures-writer-deployment.yaml 若配置StorageClass仅需变更Deployment中的pvc访问模式 step7: 变更PV回收方式由Retain为Delete kubectl patch pv cat-pictures-pv -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Delete\"}}' step8: 恢复cat-pictures-writer工作负载实例数 kubectl scale --replicas=1 deployment cat-pictures-writer 哪些卷插件支持ReadWriteOncePod？ 只有CSI类型存储驱动支持，原生卷插件（如Hostpath）并不支持ReadWriteOncePod模式， 因为原生卷插件作为CSI迁移的一部分正在被弃用， 当ReadWriteOncePod达到beta版本时，原生卷插件可能会被k8s原生支持。 绝大多数生产环境，都会使用第三方CSI插件（Ceph CSI），很少会使用原生卷插件类型。 作为CSI提供者，如何支持ReadWriteOncePod？ 请移步原文section Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/k8s/更新列表/v1.22/alpha-支持使用虚拟交换内存.html":{"url":"2.容器/k8s/更新列表/v1.22/alpha-支持使用虚拟交换内存.html","title":"alpha-支持使用虚拟交换内存","keywords":"","body":"Kubernetes v1.22 alpha特性: 支持使用交换内存 Author: Elana Hashman (Red Hat) 1.22版本引入了一个alpha特性支持: Kubernetes工作负载可以配置使用node节点上的交换内存 在之前版本中，Kubernetes不支持在Linux上使用交换内存，因为当涉及内存交换时，很难描述pod内存的使用情况。 并且如果在一个节点上检测到交换分区，该节点上的kubelet默认情况下将无法启动。 但是，交换内存有许多使用场景 ，并且可以改进节点稳定性、更好地支持具有高内存开销但工作集较小的应用程序、使用内存受限的设备和内存灵活性。 因此，在过去的两个版本中，SIG Node(k8s社区Node方向的兴趣小组)一直在收集关于交换内存的使用场景与社区反馈建议。 并提出了一种以可控的、可预测的方式向节点添加交换内存（swap）支持的设计， 以便Kubernetes用户可以进行测试swap并提供测试数据，从而可以基于具有swap的运行时构建集群功能。 对swap支持的第一个里程碑，便是该特性alpha阶段毕业。 初衷 swap有两种不同类型的用户，它们可能会重叠: 节点管理员: 他们可能希望交换可用来进行节点级性能调优和稳定性/减少嘈杂的邻居问题 应用程序开发人员: 他们编写的应用程序将从使用交换内存中受益 用户故事 通过使用swap提高系统稳定性 Cgroupsv2改进的内存管理算法，如oomd，强烈推荐使用swap。因此，在节点上使用少量的交换可以改善更好的资源压力处理和恢复. systemd-oomd.service cgroup-v2 节点的swap配置通过KubeletConfiguration 中的memorySwap字段对集群管理员可见 作为集群管理员，您可以通过设置memorySwap.swapBehavior来指定swap使用限制。 kubelet通过向容器运行时接口(CRI)添加memory_swap_limit_in_bytes字段， 实现容器对swap的使用限制，然后容器运行时将swap设置写入容器级别cgroup 使用方式 step1: kubelet开启该特性 --feature-gates=\"...,NodeMemorySwap=true\" step2: failSwapOn配置为false /var/lib/kubelet/config.yaml ... failSwapOn: false ... step3: 配置swap使用限制(可选) /var/lib/kubelet/config.yaml ... memorySwap: swapBehavior: LimitedSwap ... memorySwap.swapBehavior可选值 LimitedSwap(default): Kubernetes工作负载可以使用多少交换是有限的，工作负载使用的内存、交换内存总和 resource.limits.memory值 UnlimitedSwap: Kubernetes工作负载可以根据请求使用尽可能多的交换内存，直到达到系统swap最大限制 LimitedSwap设置的行为取决于节点运行的是v1还是v2的控制组(即cgroups): cgroups v1: Kubernetes工作负载可以使用多少交换是有限的，工作负载使用的内存、交换内存总和 resource.limits.memory值. cgroups v2: swap的配置独立于内存，因此，在这种情况下，容器运行时可以将memory.swap.max设置为0，并且不允许使用交换 当memorySwap.swapBehavior设置为UnlimitedSwap时 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/kubesphere/clientip.html":{"url":"2.容器/kubesphere/clientip.html","title":"clientip","keywords":"","body":" 获取 client ip 环境说明 all in one kubesphere 版本：v3.4.1 kubernetes 版本: v1.23.17 client_ip: 192.168.1.2 cni: cilium cilium_host ip 10.233.64.175 server ip 10.8.0.2 gateway ip 10.8.0.2 程序代码片段 非常简单的一个 RESTFUL 接口，返回调用者 Request 头相关值信息 package main import ( \"github.com/gin-gonic/gin\" ) func main() { r := gin.Default() r.GET(\"/ppp\", func(c *gin.Context) { c.JSON(200, gin.H{ \"remote_addr\": c.RemoteIP(), \"client_ip\": c.ClientIP(), \"X-Forwarded-For\": c.GetHeader(\"X-Forwarded-For\"), }) }) r.Run() // listen and serve on 0.0.0.0:8080 } 调用链路 browser -> ks gateway -> svc -> pod 默认方式访问: 获取的客户端地址为node节点ip Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/容器原理/":{"url":"2.容器/容器原理/","title":"容器原理","keywords":"","body":"容器原理概述 容器与虚拟化 传统虚拟化-虚拟机 实质：一套物理资源、多套Guest操作系统、系统级隔离 容器虚拟化 实质：一套物理资源、一套内核、进程级隔离 两者对比 容器的本质 一个视图被隔离、资源受限的进程，容器里PID=1的进程就是应用本身 容器核心技术 命名空间概念 什么是命名空间？ namespace是Linux内核用来隔离内核资源的实现方式 命名空间实质 进程视图隔离 常用命名空间（进程视图隔离内容） Cgroup命名空间: 隔离控制组根目录 (Linux 内核4.6新增) IPC命名空间: 隔离系统进程通信等 (Linux 内核2.6.19新增) MNT命名空间: 隔离挂载点(Linux 内核2.4.19新增) NET命名空间: 隔离网络设备、栈、端口等（Linux 内核2.6.24新增） PID命名空间: 隔离进程ID（Linux 内核2.6.24新增） USER命名空间: 隔离用户、用户组ID（Linux 内核2.6.23新增，Linux 内核3.8完善） UTS命名空间: 隔离主机名和NIS域名（Linux 内核2.6.19新增） 通过这七个命名空间，我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。 因此容器只能感知内部的进程，而对宿主机和其他容器一无所知。 命名空间管理 nsenter 查看当前命名空间 [root@localhost user]# lsns NS TYPE NPROCS PID USER COMMAND 4026531836 pid 258 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531837 user 289 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531838 uts 276 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531839 ipc 258 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531840 mnt 248 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531856 mnt 1 28 root kdevtmpfs 4026531956 net 282 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026532512 net 1 747 rtkit /usr/libexec/rtkit-daemon 4026532583 mnt 1 747 rtkit /usr/libexec/rtkit-daemon 4026532584 mnt 2 773 root /usr/sbin/NetworkManager --no-daemon 4026532585 mnt 1 778 root /usr/libexec/bluetooth/bluetoothd 4026532586 mnt 1 788 chrony /usr/sbin/chronyd 4026532587 mnt 1 17813 root /usr/local/bin/etcd 4026532588 uts 1 17813 root /usr/local/bin/etcd 4026532589 ipc 1 17813 root /usr/local/bin/etcd 4026532590 pid 1 17813 root /usr/local/bin/etcd ... 命名空间限制 /proc/sys/user/max_user_namespaces: 15511 /proc/sys/user/max_uts_namespaces: 15511 /proc/sys/user/max_pid_namespaces: 15511 /proc/sys/user/max_net_namespaces: 15511 /proc/sys/user/max_mnt_namespaces: 15511 /proc/sys/user/max_ipc_namespaces: 15511 /proc/sys/user/max_inotify_watches: 8192 /proc/sys/user/max_inotify_instances: 524288 /proc/sys/user/max_cgroup_namespaces: 15511 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/容器原理/命名空间/用户命名空间.html":{"url":"2.容器/容器原理/命名空间/用户命名空间.html","title":"用户命名空间","keywords":"","body":"USER命名空间 USER namespace有什么能力？ 提供用户隔离能力，隔离用户的用户ID与用户组ID 使用场景 在宿主机上以一个非root用户运行创建一个User namespace，然后在User namespace里面却映射成root用户 这样意味着，这个进程在User namespace里面有root权限，但是在User namespace外面却没有root的权限 重映射容器内用户Uid至宿主机 启动nginx docker run -itd --name nginx nginx 获取nginx容器pid $ ps -ef|grep nginx root 46991 46971 0 07:58 pts/0 00:00:00 nginx: master process nginx -g daemon off; 101 47051 46991 0 07:58 pts/0 00:00:00 nginx: worker process 101 47052 46991 0 07:58 pts/0 00:00:00 nginx: worker process 101 47053 46991 0 07:58 pts/0 00:00:00 nginx: worker process 101 47054 46991 0 07:58 pts/0 00:00:00 nginx: worker process root 48582 4609 0 07:59 pts/0 00:00:00 grep --color=auto nginx 进入nginx容器进程空间 $ cd /proc/46991 查看uid_map属性 $ cat uid_map 0 0 4294967295 第一列字段表示在容器显示的UID或GID 第二列字段表示容器外映射的真实的UID或GID 第三个字段表示映射的范围 如果为1，表示一一对应（内部与外部uid一一对应） 如果为4294967295，表示把namespace内部从0开始的uid映射到外部从0开始的uid， 其最大范围是无符号32位整形 上述nginx进程表示，容器内的nginx: master用户为root权限，即在容器外部也有root权限 docker启用用户命名空间 由上述步骤我们可知，默认docker未启用用户命名空间，容器内的uid与宿主机一致 如容器内使用root（uid=0）启动服务，有安全风险（宿主机视角也是root用户） 修改系统参数 $ sed -i \"/user.max_user_namespaces/d\" /etc/sysctl.conf $ echo \"user.max_user_namespaces=15511\" >> /etc/sysctl.conf $ sysctl -p 编辑配置文件 $ vi /etc/docker/daemon.json 添加参数\"userns-remap\": \"default\", 参考配置： { \"log-opts\": { \"max-size\": \"5m\", \"max-file\":\"3\" }, \"userns-remap\": \"default\", \"exec-opts\": [\"native.cgroupdriver=systemd\"] } 重载服务 $ systemctl daemon-reload $ systemctl restart docker 启动一个容器 $ docker rm -f nginx $ docker run -itd --name nginx nginx 查看容器内进程用户 $ ps -p $(docker inspect --format='{{.State.Pid}}' $(docker ps |grep ccc|awk '{print $1}')) -o pid,user PID USER 2535 100000 参考文献 DOCKER基础技术：LINUX NAMESPACE（下） Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/容器原理/控制组/cgroup.html":{"url":"2.容器/容器原理/控制组/cgroup.html","title":"cgroup","keywords":"","body":"cgroups 查看cgroup版本 [root@node1 ~]# grep cgroup /proc/filesystems nodev cgroup nodev cgroup2 如果包含有cgroup2的就是安装了的 简介 Cgroups用来提供对一组进程以及将来子进程的资源限制，包含三个组件： 控制组: 一个cgroups包含一组进程，并可以在这个cgroups上增加Linux subsystem的各种参数配置， 将一组进程和一组subsystem关联起来 subsystem 子系统是一组资源控制模块 可以通过lssubsys -a命令查看当前内核支持哪些subsystem 利用cgroup限制程序资源 安装cgroup管理工具 yum install libcgroup libcgroup-tools -y 限制cpu 创建cpu的cgroup控制组，控制组名为demo01 cgcreate -g cpu:/demo01 查看cpu控制组 [root@localhost ~]# cd /sys/fs/cgroup/cpu/demo01 [root@localhost demo01]# ls cgroup.clone_children cgroup.procs cpuacct.usage cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_release cgroup.event_control cpuacct.stat cpuacct.usage_percpu cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks cpu.cfs_period_us: cpu分配的周期(微秒），默认为100000 cpu.cfs_quota_us: 表示该control group限制占用的时间（微秒），默认为-1，表示不限制。 配置demo1控制组cpu参数 cpu.cfs_quota_us和cpu.cfs_period_us是控制cpu的两个属性， 可以通过设置它们的比值来设置某个组群的cpu使用率。在此，我们将cpu的使用率限制到30%。 # 设置cpu分配的周期(微秒），即为默认值 cgset -r cpu.cfs_period_us=100000 demo01 # 如果设为30000，表示占用30000/10000=30%的CPU cgset -r cpu.cfs_quota_us=30000 demo01 创建测试程序，测试无限制情况下cpu占用率 创建死循环脚本 cat > ~/demo.sh /dev/null done EOF 执行死循环脚本 [root@localhost ~]# nohup sh ~/demo.sh &> /dev/null & [1] 15319 查看进程占用系统资源 top - 23:38:34 up 20 min, 1 user, load average: 0.76, 0.20, 0.11 Tasks: 128 total, 2 running, 126 sleeping, 0 stopped, 0 zombie %Cpu(s): 19.7 us, 5.3 sy, 0.0 ni, 74.5 id, 0.0 wa, 0.0 hi, 0.6 si, 0.0 st KiB Mem : 3861280 total, 3284036 free, 203144 used, 374100 buff/cache KiB Swap: 2097148 total, 2097148 free, 0 used. 3425504 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 15319 root 20 0 113288 1184 1012 R 99.7 0.0 0:43.61 sh 9 root 20 0 0 0 0 S 1.0 0.0 0:01.43 rcu_sched 1037 root 20 0 157828 6668 5120 S 0.3 0.2 0:01.83 sshd kill该进程 kill -9 15319 通过指定cgroup进行cpu配额限制 [root@localhost ~]# nohup cgexec -g cpu:/demo1 sh ~/demo.sh &> /dev/null & [1] 16893 查看cpu占用情况 top - 23:58:27 up 40 min, 2 users, load average: 0.72, 0.28, 0.18 Tasks: 132 total, 2 running, 130 sleeping, 0 stopped, 0 zombie %Cpu(s): 7.1 us, 0.7 sy, 0.0 ni, 92.2 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 3861280 total, 3274444 free, 208332 used, 378504 buff/cache KiB Swap: 2097148 total, 2097148 free, 0 used. 3420156 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 22745 root 20 0 113288 1184 1008 R 30.0 0.0 0:01.22 sh /root/demo.sh Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/容器原理/ns-ipc.html":{"url":"2.容器/容器原理/ns-ipc.html","title":"ns-ipc","keywords":"","body":"ipc命名空间 概述 主要能力 提供进程间通信的隔离能力 ipc命名空间隔离性验证 获取当前进程ID [root@localhost ~]# echo $$ 49265 在这里为了方面解释，我们定义进程ID为49265的进程名称为PID-A 查看当前进程命名空间信息 [root@localhost ~]# ls -l /proc/$$/ns total 0 lrwxrwxrwx 1 root root 0 Jul 14 07:27 cgroup -> cgroup:[4026531835] lrwxrwxrwx 1 root root 0 Jul 14 07:27 ipc -> ipc:[4026531839] lrwxrwxrwx 1 root root 0 Jul 14 07:27 mnt -> mnt:[4026531840] lrwxrwxrwx 1 root root 0 Jul 14 07:27 net -> net:[4026531992] lrwxrwxrwx 1 root root 0 Jul 14 07:27 pid -> pid:[4026531836] lrwxrwxrwx 1 root root 0 Jul 14 07:27 pid_for_children -> pid:[4026531836] lrwxrwxrwx 1 root root 0 Jul 14 07:27 user -> user:[4026531837] lrwxrwxrwx 1 root root 0 Jul 14 07:27 uts -> uts:[4026531838] 查看PID-A进程ipc信息 [root@localhost ~]# ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x00000000 2 gdm 777 16384 1 dest 0x00000000 5 gdm 777 7372800 2 dest ------ Semaphore Arrays -------- key semid owner perms nsems 使用unshare隔离ipc namespace unshare --ipc /bin/bash 查看进程ID，发现已变更 [root@localhost ~]# echo $$ 62293 在这里为了方面解释，我们定义进程ID为62293的进程名称为PID-B 查看两个进程关系，显然PID-A与PID-B为父子关系的两个进程 [root@localhost ~]# ps -ef|grep 62293 root 62293 49265 0 07:33 pts/0 00:00:00 /bin/bash root 62430 62293 0 07:33 pts/0 00:00:00 ps -ef root 62431 62293 0 07:33 pts/0 00:00:00 grep --color=auto 62293 查看PID-B进程的ipc信息 [root@localhost ~]# ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status ------ Semaphore Arrays -------- key semid owner perms nsems 显然与PID-A进程不一致 测试: PID-B创建一个消息队列，是否PID-A中可以看到 [root@localhost ~]# ipcmk --queue Message queue id: 0 [root@localhost ~]# ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages 0x6c54b6c4 0 root 644 0 0 ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status ------ Semaphore Arrays -------- key semid owner perms nsems 新开一个ssh链接（会产生新的进程），查看是否可以看到PID-B中消息队列 [root@localhost ~]# echo $$ 49857 [root@localhost ~]# ipcs -q ------ Message Queues -------- key msqid owner perms used-bytes messages 显然无法查看，隔离验证成功！ 查看PID-B进程命名空间信息 [root@localhost ~]# ls -l /proc/62293/ns total 0 lrwxrwxrwx 1 root root 0 Jul 14 07:47 cgroup -> cgroup:[4026531835] lrwxrwxrwx 1 root root 0 Jul 14 07:47 ipc -> ipc:[4026532765] lrwxrwxrwx 1 root root 0 Jul 14 07:47 mnt -> mnt:[4026531840] lrwxrwxrwx 1 root root 0 Jul 14 07:47 net -> net:[4026531992] lrwxrwxrwx 1 root root 0 Jul 14 07:47 pid -> pid:[4026531836] lrwxrwxrwx 1 root root 0 Jul 14 07:47 pid_for_children -> pid:[4026531836] lrwxrwxrwx 1 root root 0 Jul 14 07:47 user -> user:[4026531837] lrwxrwxrwx 1 root root 0 Jul 14 07:47 uts -> uts:[4026531838] 对比PID-A，发现二者区别仅为ipc不同 IPC实现方式 Linux进程通信方式 信号量 共享内存 消息队列 管道 信号 套接字通信 其中信号量，共享内存，消息队列基于内核的IPC命名空间实现 [root@localhost ~]# ipcs ------ Message Queues -------- key msqid owner perms used-bytes messages 0x84300480 0 root 644 0 0 0xba58165a 1 root 644 0 0 0xb5be9e2a 2 root 644 0 0 ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x00000000 2 gdm 777 16384 1 dest 0x00000000 5 gdm 777 7372800 2 dest ------ Semaphore Arrays -------- key semid owner perms nsems 参考文档 Docker基础: Linux内核命名空间之（2） ipc namespace Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/容器原理/ns-mnt.html":{"url":"2.容器/容器原理/ns-mnt.html","title":"ns-mnt","keywords":"","body":"MNT命名空间 概念 mnt namespace有什么能力？ mnt(mount缩写) 命名空间提供了隔离mount point能力。 每个mnt namespace内的文件结构可以单独修改，互不影响。 隔离性验证 查看挂载信息 查看当前进程挂载点信息 [root@localhost ns]# ll /proc/$$/mount* -r--r--r-- 1 root root 0 Jul 14 22:21 /proc/88929/mountinfo -r--r--r-- 1 root root 0 Jul 14 22:21 /proc/88929/mounts -r-------- 1 root root 0 Jul 14 22:21 /proc/88929/mountstats 查看mountinfo内容 23 46 0:22 / /sys rw,nosuid,nodev,noexec,relatime shared:6 - sysfs sysfs rw 24 46 0:5 / /proc rw,nosuid,nodev,noexec,relatime shared:5 - proc proc rw 25 46 0:6 / /dev rw,nosuid shared:2 - devtmpfs devtmpfs rw,size=1985460k,nr_inodes=496365,mode=755 26 23 0:7 / /sys/kernel/security rw,nosuid,nodev,noexec,relatime shared:7 - securityfs securityfs rw 27 25 0:23 / /dev/shm rw,nosuid,nodev shared:3 - tmpfs tmpfs rw 28 25 0:24 / /dev/pts rw,nosuid,noexec,relatime shared:4 - devpts devpts rw,gid=5,mode=620,ptmxmode=000 29 46 0:25 / /run rw,nosuid,nodev shared:23 - tmpfs tmpfs rw,mode=755 30 23 0:26 / /sys/fs/cgroup ro,nosuid,nodev,noexec shared:8 - tmpfs tmpfs ro,mode=755 31 30 0:27 / /sys/fs/cgroup/systemd rw,nosuid,nodev,noexec,relatime shared:9 - cgroup cgroup rw,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 32 23 0:28 / /sys/fs/pstore rw,nosuid,nodev,noexec,relatime shared:21 - pstore pstore rw 33 30 0:29 / /sys/fs/cgroup/cpu,cpuacct rw,nosuid,nodev,noexec,relatime shared:10 - cgroup cgroup rw,cpu,cpuacct 34 30 0:30 / /sys/fs/cgroup/memory rw,nosuid,nodev,noexec,relatime shared:11 - cgroup cgroup rw,memory 35 30 0:31 / /sys/fs/cgroup/devices rw,nosuid,nodev,noexec,relatime shared:12 - cgroup cgroup rw,devices 36 30 0:32 / /sys/fs/cgroup/freezer rw,nosuid,nodev,noexec,relatime shared:13 - cgroup cgroup rw,freezer 37 30 0:33 / /sys/fs/cgroup/perf_event rw,nosuid,nodev,noexec,relatime shared:14 - cgroup cgroup rw,perf_event 38 30 0:34 / /sys/fs/cgroup/pids rw,nosuid,nodev,noexec,relatime shared:15 - cgroup cgroup rw,pids 39 30 0:35 / /sys/fs/cgroup/hugetlb rw,nosuid,nodev,noexec,relatime shared:16 - cgroup cgroup rw,hugetlb 40 30 0:36 / /sys/fs/cgroup/net_cls,net_prio rw,nosuid,nodev,noexec,relatime shared:17 - cgroup cgroup rw,net_cls,net_prio 41 30 0:37 / /sys/fs/cgroup/blkio rw,nosuid,nodev,noexec,relatime shared:18 - cgroup cgroup rw,blkio 42 30 0:38 / /sys/fs/cgroup/rdma rw,nosuid,nodev,noexec,relatime shared:19 - cgroup cgroup rw,rdma 43 30 0:39 / /sys/fs/cgroup/cpuset rw,nosuid,nodev,noexec,relatime shared:20 - cgroup cgroup rw,cpuset 44 23 0:40 / /sys/kernel/config rw,relatime shared:22 - configfs configfs rw 46 1 8:3 / / rw,relatime shared:1 - xfs /dev/sda3 rw,attr2,inode64,logbufs=8,logbsize=32k,noquota 22 24 0:21 / /proc/sys/fs/binfmt_misc rw,relatime shared:24 - autofs systemd-1 rw,fd=23,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=711 47 25 0:42 / /dev/hugepages rw,relatime shared:25 - hugetlbfs hugetlbfs rw,pagesize=2M 48 25 0:20 / /dev/mqueue rw,relatime shared:26 - mqueue mqueue rw 49 23 0:8 / /sys/kernel/debug rw,relatime shared:27 - debugfs debugfs rw 51 46 8:1 / /boot rw,relatime shared:28 - xfs /dev/sda1 rw,attr2,inode64,logbufs=8,logbsize=32k,noquota 52 46 0:44 / /var/lib/nfs/rpc_pipefs rw,relatime shared:29 - rpc_pipefs sunrpc rw 203 46 0:46 / /var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/merged rw,relatime shared:173 - overlay overlay rw,lowerdir=/var/lib/docker/231072.231072/overlay2/l/67CHEM5VH4RUKAJ7YJQWRBNVJE:/var/lib/docker/231072.231072/overlay2/l/GOFSE7LF6JYPQUVKRKRQFETASF,upperdir=/var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/diff,workdir=/var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/work 310 29 0:4 net:[4026532689] /run/docker/netns/bae6cec20525 rw shared:181 - nsfs nsfs rw 333 29 0:45 / /run/user/1000 rw,nosuid,nodev,relatime shared:217 - tmpfs tmpfs rw,size=400496k,mode=700,uid=1000,gid=1000 343 333 0:59 / /run/user/1000/gvfs rw,nosuid,nodev,relatime shared:227 - fuse.gvfsd-fuse gvfsd-fuse rw,user_id=1000,group_id=1000 353 23 0:60 / /sys/fs/fuse/connections rw,relatime shared:237 - fusectl fusectl rw 522 29 0:63 / /run/user/0 rw,nosuid,nodev,relatime shared:440 - tmpfs tmpfs rw,size=400496k,mode=700 查看mount内容 [root@localhost 88929]# cat /proc/$$/mounts sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0 proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0 devtmpfs /dev devtmpfs rw,nosuid,size=1985460k,nr_inodes=496365,mode=755 0 0 securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0 tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0 devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0 tmpfs /run tmpfs rw,nosuid,nodev,mode=755 0 0 tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0 cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 0 0 pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0 cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0 cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0 cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0 cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0 cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0 cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0 cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0 cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0 cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0 cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0 cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0 configfs /sys/kernel/config configfs rw,relatime 0 0 /dev/sda3 / xfs rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota 0 0 systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=23,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=711 0 0 hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0 mqueue /dev/mqueue mqueue rw,relatime 0 0 debugfs /sys/kernel/debug debugfs rw,relatime 0 0 /dev/sda1 /boot xfs rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota 0 0 sunrpc /var/lib/nfs/rpc_pipefs rpc_pipefs rw,relatime 0 0 overlay /var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/merged overlay rw,relatime,lowerdir=/var/lib/docker/231072.231072/overlay2/l/67CHEM5VH4RUKAJ7YJQWRBNVJE:/var/lib/docker/231072.231072/overlay2/l/GOFSE7LF6JYPQUVKRKRQFETASF,upperdir=/var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/diff,workdir=/var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/work 0 0 nsfs /run/docker/netns/bae6cec20525 nsfs rw 0 0 tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=400496k,mode=700,uid=1000,gid=1000 0 0 gvfsd-fuse /run/user/1000/gvfs fuse.gvfsd-fuse rw,nosuid,nodev,relatime,user_id=1000,group_id=1000 0 0 fusectl /sys/fs/fuse/connections fusectl rw,relatime 0 0 tmpfs /run/user/0 tmpfs rw,nosuid,nodev,relatime,size=400496k,mode=700 0 0 查看mountstats 挂载状态 [root@localhost ~]# cat /proc/$$/mountstats device sysfs mounted on /sys with fstype sysfs device proc mounted on /proc with fstype proc device devtmpfs mounted on /dev with fstype devtmpfs device securityfs mounted on /sys/kernel/security with fstype securityfs device tmpfs mounted on /dev/shm with fstype tmpfs device devpts mounted on /dev/pts with fstype devpts device tmpfs mounted on /run with fstype tmpfs device tmpfs mounted on /sys/fs/cgroup with fstype tmpfs device cgroup mounted on /sys/fs/cgroup/systemd with fstype cgroup device pstore mounted on /sys/fs/pstore with fstype pstore device cgroup mounted on /sys/fs/cgroup/cpu,cpuacct with fstype cgroup device cgroup mounted on /sys/fs/cgroup/memory with fstype cgroup device cgroup mounted on /sys/fs/cgroup/devices with fstype cgroup device cgroup mounted on /sys/fs/cgroup/freezer with fstype cgroup device cgroup mounted on /sys/fs/cgroup/perf_event with fstype cgroup device cgroup mounted on /sys/fs/cgroup/pids with fstype cgroup device cgroup mounted on /sys/fs/cgroup/hugetlb with fstype cgroup device cgroup mounted on /sys/fs/cgroup/net_cls,net_prio with fstype cgroup device cgroup mounted on /sys/fs/cgroup/blkio with fstype cgroup device cgroup mounted on /sys/fs/cgroup/rdma with fstype cgroup device cgroup mounted on /sys/fs/cgroup/cpuset with fstype cgroup device configfs mounted on /sys/kernel/config with fstype configfs device /dev/sda3 mounted on / with fstype xfs device systemd-1 mounted on /proc/sys/fs/binfmt_misc with fstype autofs device hugetlbfs mounted on /dev/hugepages with fstype hugetlbfs device mqueue mounted on /dev/mqueue with fstype mqueue device debugfs mounted on /sys/kernel/debug with fstype debugfs device /dev/sda1 mounted on /boot with fstype xfs device sunrpc mounted on /var/lib/nfs/rpc_pipefs with fstype rpc_pipefs device overlay mounted on /var/lib/docker/231072.231072/overlay2/6979cabf2db354126e61163ddc90b5738c600797fefbf0c41c9b56600f011d1f/merged with fstype overlay device nsfs mounted on /run/docker/netns/bae6cec20525 with fstype nsfs device tmpfs mounted on /run/user/1000 with fstype tmpfs device gvfsd-fuse mounted on /run/user/1000/gvfs with fstype fuse.gvfsd-fuse device fusectl mounted on /sys/fs/fuse/connections with fstype fusectl device tmpfs mounted on /run/user/0 with fstype tmpfs 隔离性验证 关于mount namespace记住以下三点: a.每个mount namespace都有一份自己的挂载点列表 b.当使用clone函数或unshare函数并传入CLONE_NEWNS标志创建新的mount namespace时， 新mount namespace中的挂载点其实是从调用者所在的mount namespace中拷贝的。 c.在新的mount namespace创建之后，这两个mount namespace及其挂载点基本无任何关系， 两个mount namespace是相互隔离的。 安装mkisofs演示mnt namespace yum install mkisofs -y 创建演示用iso文件 hostnamectl set-hostname vm mkdir -p ~/iso/{A,B} echo \"A\" > ~/iso/A/a.txt echo \"B\" > ~/iso/B/b.txt cd ~/iso mkisofs -o ./A.iso ./A mkisofs -o ./B.iso ./B exec bash 创建用于挂载的目录 mkdir -p /mnt/{isoA,isoB} 在当前mnt namepace下挂载~/iso/A.iso至/mnt/isoA 查看当前mnt namepace编号 [root@vm iso]# readlink /proc/$$/ns/mnt mnt:[4026531840] 挂载 [root@vm iso]# mount ~/iso/A.iso /mnt/isoA mount: /dev/loop0 is write-protected, mounting read-only [root@vm iso]# cat /mnt/isoA/a.txt A 查看挂载状态 [root@vm iso]# mount |grep A.iso /root/iso/A.iso on /mnt/isoA type iso9660 (ro,relatime,nojoliet,check=s,map=n,blocksize=2048) 创建并进入新的mount和uts namespace unshare --mount --uts /bin/bash hostnamectl set-hostname container-A exec bash 查看新的mount namespace挂载信息 查看mnt命名空间编号 [root@vm iso]# readlink /proc/$$/ns/mnt mnt:[4026532765] 查看挂载信息 [root@vm iso]# mount|grep A.iso /root/iso/A.iso on /mnt/isoA type iso9660 (ro,relatime,nojoliet,check=s,map=n,blocksize=2048) b.内容验证成功 新的mount namespace内挂载~/iso/B.iso至/mnt/isoB 挂载 [root@vm iso]# mount ~/iso/B.iso /mnt/isoB mount: /dev/loop1 is write-protected, mounting read-only 查看挂载状态 [root@vm iso]# mount |grep B.iso /root/iso/B.iso on /mnt/isoB type iso9660 (ro,relatime,nojoliet,check=s,map=n,blocksize=2048) 新的mount namespace内卸载/mnt/isoA [root@vm iso]# umount /mnt/isoA [root@vm iso]# ls /mnt/isoA 返回第一个mnt命名空间 通过新建session实现，并执行以下命令确认mnt命名空间(4026531840) readlink /proc/$$/ns/mnt 查看挂载信息 [root@container-a ~]# cat /mnt/isoA/a.txt A [root@container-a ~]# mount |grep iso /root/iso/A.iso on /mnt/isoA type iso9660 (ro,relatime,nojoliet,check=s,map=n,blocksize=2048) c.验证成功 参考文献 liunx mnt命名空间 Docker 学习笔记11 容器技术原理 Mount Namespace Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/容器原理/ns-net.html":{"url":"2.容器/容器原理/ns-net.html","title":"ns-net","keywords":"","body":" 网络命名空间 概念 veth-pair介绍 不同网络命名空间之间联通方案 直连 通过Bridge相连 通过OVS相连 docker网络浅析 容器网络互通原理 网络隔离原理 参考文献 网络命名空间 概念 网络命名空间有什么能力？ 隔离网络设备、协议栈、端口等 Linux中，网络命名空间可以被认为是隔离的拥有单独网络栈（网卡、路由转发表、iptables）的环境。 网络命名空间经常用来隔离网络设备和服务，只有拥有同样网络命名空间的设备，才能看到彼此。 从逻辑上说，网络命名空间是网络栈的副本，有自己的网络设备、路由选择表、邻接表、Netfilter表、网络套接字、网络procfs条目、网络sysfs条目和其他网络资源。 从系统的角度来看，当通过clone()系统调用创建新进程时，传递标志CLONE_NEWNET将在新进程中创建一个全新的网络命名空间。 从用户的角度来看，我们只需使用工具ip（package is iproute2）来创建一个新的持久网络命名空间 veth-pair介绍 veth-pair是什么? 顾名思义，veth-pair就是一对的虚拟设备接口，和tap/tun设备不同的是，它都是成对出现的。 一端连着协议栈，一端彼此相连着。如下所示: +-------------------------------------------------------------------+ | | | +------------------------------------------------+ | | | Newwork Protocol Stack | | | +------------------------------------------------+ | | ↑ ↑ ↑ | |.................|...............|...............|.................| | ↓ ↓ ↓ | | +----------+ +-----------+ +-----------+ | | | ens33 | | veth0 | | veth1 | | | +----------+ +-----------+ +-----------+ | |192.168.235.128 ↑ ↑ ↑ | | | +---------------+ | | | 10.10.10.2 10.10.10.3 | +-----------------|-------------------------------------------------+ ↓ Physical Network veth设备的特点 veth和其它的网络设备都一样，一端连接的是内核协议栈 veth设备是成对出现的，另一端两个设备彼此相连 一个设备收到协议栈的数据发送请求后，会将数据发送到另一个设备上去 正因为有这个特性，它常常充当着一个桥梁，连接着各种虚拟网络设备，典型的例子如下： 两个net namespace之间的连接 Bridge、OVS之间的连接 Docker容器之间的连接 不同网络命名空间之间联通方案 OVS是第三方开源的虚拟交换机，功能比Linux Bridge要更强大 veth-pair在虚拟网络中充当着桥梁的角色，连接多种网络设备构成复杂的网络。 veth-pair三个网络联通方案：直接相连、通过Bridge相连和通过OVS相连 直连 直接相连是最简单的方式，如下图，一对veth-pair直接将两个namespace连接在一起 创建测试用net命名空间 ip netns a ns0 ip netns a ns1 添加veth0和veth1设备，并配置veth0 IP地址，分别加入不同net命名空间 # 创建veth-pair对 ip link add veth0 type veth peer name veth1 # 分别加入不同命名空间 ip l s veth0 netns ns0 ip l s veth1 netns ns1 # 配置ip地址，并启用 ip netns exec ns0 ip a a 10.10.10.2/24 dev veth0 ip netns exec ns0 ip l s veth0 up ip netns exec ns1 ip a a 10.10.10.3/24 dev veth1 ip netns exec ns1 ip l s veth1 up 互ping [root@localhost ~]# ip netns exec ns0 ping -c 4 10.10.10.3 PING 10.10.10.3 (10.10.10.3) 56(84) bytes of data. 64 bytes from 10.10.10.3: icmp_seq=1 ttl=64 time=0.045 ms 64 bytes from 10.10.10.3: icmp_seq=2 ttl=64 time=0.090 ms 64 bytes from 10.10.10.3: icmp_seq=3 ttl=64 time=0.045 ms 64 bytes from 10.10.10.3: icmp_seq=4 ttl=64 time=0.106 ms --- 10.10.10.3 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3030ms rtt min/avg/max/mdev = 0.045/0.071/0.106/0.028 ms [root@localhost ~]# ip netns exec ns1 ping -c 4 10.10.10.2 PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=0.142 ms 64 bytes from 10.10.10.2: icmp_seq=2 ttl=64 time=0.118 ms 64 bytes from 10.10.10.2: icmp_seq=3 ttl=64 time=0.104 ms 64 bytes from 10.10.10.2: icmp_seq=4 ttl=64 time=0.054 ms --- 10.10.10.2 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3009ms rtt min/avg/max/mdev = 0.054/0.104/0.142/0.033 ms 清除net命名空间 ip netns del ns0 ip netns del ns1 通过Bridge相连 当必须连接两个以上的net命名空间(或KVM或LXC实例)时，应使用交换机。 Linux提供了著名的Linux网桥解决方案。 Linux Bridge相当于一台交换机，可以中转多个namespace的流量， 如下图，两对veth-pair分别将两个namespace连到Bridge上。 创建net命名空间 # add the namespaces ip netns add ns1 ip netns add ns2 ip netns add ns3 创建并启用网桥 # create the switch yum install bridge-utils -y BRIDGE=br-test brctl addbr $BRIDGE brctl stp $BRIDGE off ip link set dev $BRIDGE up 创建三对veth-pair ip l a veth0 type veth peer name br-veth0 ip l a veth1 type veth peer name br-veth1 ip l a veth2 type veth peer name br-veth2 分别将三对veth-pair加入三个net命名空间和br-test ip l s veth0 netns ns1 ip l s br-veth0 master br-test ip l s br-veth0 up ip l s veth1 netns ns2 ip l s br-veth1 master br-test ip l s br-veth1 up ip l s veth2 netns ns3 ip l s br-veth2 master br-test ip l s br-veth2 up 配置三个ns中的veth-pair的IP并启用 ip netns exec ns1 ip a a 10.10.10.2/24 dev veth0 ip netns exec ns1 ip l s veth0 up ip netns exec ns2 ip a a 10.10.10.3/24 dev veth1 ip netns exec ns2 ip l s veth1 up ip netns exec ns3 ip a a 10.10.10.4/24 dev veth2 ip netns exec ns3 ip l s veth2 up 互ping [root@localhost ~]# ip netns exec ns1 ping -c 1 10.10.10.3 PING 10.10.10.3 (10.10.10.3) 56(84) bytes of data. 64 bytes from 10.10.10.3: icmp_seq=1 ttl=64 time=0.103 ms --- 10.10.10.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.103/0.103/0.103/0.000 ms [root@localhost ~]# ip netns exec ns1 ping -c 1 10.10.10.4 PING 10.10.10.4 (10.10.10.4) 56(84) bytes of data. 64 bytes from 10.10.10.4: icmp_seq=1 ttl=64 time=0.118 ms --- 10.10.10.4 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.118/0.118/0.118/0.000 ms [root@localhost ~]# ip netns exec ns2 ping -c 1 10.10.10.2 PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=0.044 ms --- 10.10.10.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.044/0.044/0.044/0.000 ms [root@localhost ~]# ip netns exec ns2 ping -c 1 10.10.10.4 PING 10.10.10.4 (10.10.10.4) 56(84) bytes of data. 64 bytes from 10.10.10.4: icmp_seq=1 ttl=64 time=0.064 ms --- 10.10.10.4 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.064/0.064/0.064/0.000 ms [root@localhost ~]# ip netns exec ns3 ping -c 1 10.10.10.2 PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=0.042 ms --- 10.10.10.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.042/0.042/0.042/0.000 ms [root@localhost ~]# ip netns exec ns3 ping -c 1 10.10.10.3 PING 10.10.10.3 (10.10.10.3) 56(84) bytes of data. 64 bytes from 10.10.10.3: icmp_seq=1 ttl=64 time=0.096 ms --- 10.10.10.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.096/0.096/0.096/0.000 ms 清理测试用例 ip netns del ns1 ip netns del ns2 ip netns del ns3 ip l s br-test down brctl delbr br-test 通过OVS相连 OVS是第三方开源的Bridge，功能比Linux Bridge要更强大 OVS有两种方案实现多命名空间网络互通，一种方式为veth-pair方式，类似Linux Bridge实现 另一种解决方案是使用openvswitch，并利用openvswitch的内部端口。 这避免了在所有其他解决方案中必须使用的veth对的使用 关于第二种方式实现 关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 安装OVS编译依赖 yum install -y python-six selinux-policy-devel gcc make \\ python-devel openssl-devel kernel-devel graphviz kernel-debug-devel autoconf \\ automake rpm-build redhat-rpm-config libtool wget net-tools 编译安装OVS openvswitch-2.5.4.tar.gz mkdir -p ~/rpmbuild/SOURCES tar -zxvf openvswitch-2.5.4.tar.gz cp openvswitch-2.5.4.tar.gz ~/rpmbuild/SOURCES/ ls /lib/modules/$(uname -r) -ln rpmbuild -bb --without check openvswitch-2.5.4/rhel/openvswitch.spec cd rpmbuild/RPMS/x86_64/ yum localinstall -y openvswitch-2.5.4-1.x86_64.rpm systemctl enable openvswitch.service systemctl start openvswitch.service 创建net命名空间 ip netns add ns1 ip netns add ns2 ip netns add ns3 创建osv虚拟交换机 ovs-vsctl add-br ovs-br0 创建ovs port #### PORT 1 # create an internal ovs port ovs-vsctl add-port ovs-br0 tap1 -- set Interface tap1 type=internal # attach it to namespace ip link set tap1 netns ns1 # set the ports to up ip netns exec ns1 ip link set dev tap1 up # #### PORT 2 # create an internal ovs port ovs-vsctl add-port ovs-br0 tap2 -- set Interface tap2 type=internal # attach it to namespace ip link set tap2 netns ns2 # set the ports to up ip netns exec ns2 ip link set dev tap2 up #### PORT 3 # create an internal ovs port ovs-vsctl add-port ovs-br0 tap3 -- set Interface tap3 type=internal # attach it to namespace ip link set tap3 netns ns3 # set the ports to up ip netns exec ns3 ip link set dev tap3 up 配置ip并启用 ip netns exec ns1 ip a a 10.10.10.2/24 dev tap1 ip netns exec ns1 ip l s tap1 up ip netns exec ns2 ip a a 10.10.10.3/24 dev tap2 ip netns exec ns2 ip l s tap2 up ip netns exec ns3 ip a a 10.10.10.4/24 dev tap3 ip netns exec ns3 ip l s tap3 up 分别将三对veth-pair加入三个net命名空间和br-test ip l s veth0 netns ns1 ip l s br-veth0 master br-test ip l s br-veth0 up ip l s veth1 netns ns2 ip l s br-veth1 master br-test ip l s br-veth1 up ip l s veth2 netns ns3 ip l s br-veth2 master br-test ip l s br-veth2 up 互ping [root@localhost ~]# ip netns exec ns1 ping -c 1 10.10.10.3 PING 10.10.10.3 (10.10.10.3) 56(84) bytes of data. 64 bytes from 10.10.10.3: icmp_seq=1 ttl=64 time=0.103 ms --- 10.10.10.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.103/0.103/0.103/0.000 ms [root@localhost ~]# ip netns exec ns1 ping -c 1 10.10.10.4 PING 10.10.10.4 (10.10.10.4) 56(84) bytes of data. 64 bytes from 10.10.10.4: icmp_seq=1 ttl=64 time=0.118 ms --- 10.10.10.4 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.118/0.118/0.118/0.000 ms [root@localhost ~]# ip netns exec ns2 ping -c 1 10.10.10.2 PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=0.044 ms --- 10.10.10.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.044/0.044/0.044/0.000 ms [root@localhost ~]# ip netns exec ns2 ping -c 1 10.10.10.4 PING 10.10.10.4 (10.10.10.4) 56(84) bytes of data. 64 bytes from 10.10.10.4: icmp_seq=1 ttl=64 time=0.064 ms --- 10.10.10.4 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.064/0.064/0.064/0.000 ms [root@localhost ~]# ip netns exec ns3 ping -c 1 10.10.10.2 PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data. 64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=0.042 ms --- 10.10.10.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.042/0.042/0.042/0.000 ms [root@localhost ~]# ip netns exec ns3 ping -c 1 10.10.10.3 PING 10.10.10.3 (10.10.10.3) 56(84) bytes of data. 64 bytes from 10.10.10.3: icmp_seq=1 ttl=64 time=0.096 ms --- 10.10.10.3 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.096/0.096/0.096/0.000 ms 清除测试用例 ip netns del ns1 ip netns del ns2 ip netns del ns3 ip l s ovs-br0 down ovs-vsctl del-br ovs-br0 docker网络浅析 容器网络互通原理 启动测试容器 docker run -itd --name test1 busybox docker run -itd --name test2 busybox 查看容器ip地址 [root@localhost ~]# docker exec test1 ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 4: eth0@if5: mtu 1500 qdisc noqueue link/ether 02:42:ac:50:00:02 brd ff:ff:ff:ff:ff:ff inet 172.80.0.2/24 brd 172.80.0.255 scope global eth0 valid_lft forever preferred_lft forever [root@localhost ~]# docker exec test1 ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 4: eth0@if5: mtu 1500 qdisc noqueue link/ether 02:42:ac:50:00:02 brd ff:ff:ff:ff:ff:ff inet 172.80.0.2/24 brd 172.80.0.255 scope global eth0 valid_lft forever preferred_lft forever 互Ping [root@localhost ~]# docker exec test2 ping -c 2 172.80.0.2 PING 172.80.0.2 (172.80.0.2): 56 data bytes 64 bytes from 172.80.0.2: seq=0 ttl=64 time=0.080 ms 64 bytes from 172.80.0.2: seq=1 ttl=64 time=0.086 ms --- 172.80.0.2 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.080/0.083/0.086 ms [root@localhost ~]# docker exec test1 ping -c 2 172.80.0.3 PING 172.80.0.3 (172.80.0.3): 56 data bytes 64 bytes from 172.80.0.3: seq=0 ttl=64 time=0.055 ms 64 bytes from 172.80.0.3: seq=1 ttl=64 time=0.153 ms --- 172.80.0.3 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.055/0.104/0.153 ms 此时两个容器互通 ping内网其他主机 [root@localhost ~]# docker exec test1 ping -c 2 192.168.2.78 PING 192.168.2.78 (192.168.2.78): 56 data bytes 64 bytes from 192.168.2.78: seq=0 ttl=127 time=3.494 ms 64 bytes from 192.168.2.78: seq=1 ttl=127 time=3.007 ms --- 192.168.2.78 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 3.007/3.250/3.494 ms [root@localhost ~]# docker exec test2 ping -c 2 192.168.2.78 PING 192.168.2.78 (192.168.2.78): 56 data bytes 64 bytes from 192.168.2.78: seq=0 ttl=127 time=3.301 ms 64 bytes from 192.168.2.78: seq=1 ttl=127 time=3.442 ms --- 192.168.2.78 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 3.301/3.371/3.442 ms 连通原理解析 其实原理也是类似于上一节所说。实际上就是新建了一对veth，将网络打通了。 1、容器里能访问外网原理：是因为有一对veth,一端连着容器，一端连着主机的docker0， 这样容器就能共用主机的网络了，当容器访问外网时，就会通过NAT进行地址转换，实际是通过iptables来实现的。 2、容器间互通原理：每个容器创建时会生成一对veth,一端连着容器，一端连着docker0网络，这样两个容器都连着docker0，他们就可以互相通信了。 docker0相当于上述的Linux Bridge 如图所示： 清理测试用例 docker rm -f test1 docker rm -f test2 网络隔离原理 创建测试网络 docker network create net-1 docker network create net-2 启动容器 docker run -itd --name test1 --network=net-1 busybox docker run -itd --name test2 --network=net-2 busybox 查看IP [root@localhost ~]# docker exec test1 ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 10: eth0@if11: mtu 1500 qdisc noqueue link/ether 02:42:ac:50:01:02 brd ff:ff:ff:ff:ff:ff inet 172.80.1.2/24 brd 172.80.1.255 scope global eth0 valid_lft forever preferred_lft forever [root@localhost ~]# docker exec test2 ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 12: eth0@if13: mtu 1500 qdisc noqueue link/ether 02:42:ac:50:02:02 brd ff:ff:ff:ff:ff:ff inet 172.80.2.2/24 brd 172.80.2.255 scope global eth0 valid_lft forever preferred_lft forever 互Ping [root@localhost ~]# docker exec test1 ping -c 2 -i 1 172.80.2.2 ^C [root@localhost ~]# docker exec test1 ping -c 2 -W 1 172.80.2.2 PING 172.80.2.2 (172.80.2.2): 56 data bytes --- 172.80.2.2 ping statistics --- 2 packets transmitted, 0 packets received, 100% packet loss [root@localhost ~]# docker exec test2 ping -c 2 -W 1 172.80.1.2 PING 172.80.1.2 (172.80.1.2): 56 data bytes --- 172.80.1.2 ping statistics --- 2 packets transmitted, 0 packets received, 100% packet loss 由于归属不同网桥，网络不通 Ping外部主机 [root@localhost ~]# docker exec test2 ping -c 2 -W 1 192.168.2.78 PING 192.168.2.78 (192.168.2.78): 56 data bytes 64 bytes from 192.168.2.78: seq=0 ttl=127 time=3.282 ms 64 bytes from 192.168.2.78: seq=1 ttl=127 time=2.960 ms --- 192.168.2.78 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 2.960/3.121/3.282 ms [root@localhost ~]# docker exec test1 ping -c 2 -W 1 192.168.2.78 PING 192.168.2.78 (192.168.2.78): 56 data bytes 64 bytes from 192.168.2.78: seq=0 ttl=127 time=3.984 ms 64 bytes from 192.168.2.78: seq=1 ttl=127 time=3.429 ms --- 192.168.2.78 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 3.429/3.706/3.984 ms 由此可见，通过不同网桥实现了网络隔离 清理测试用例 docker rm -f test1 docker rm -f test2 关于docker网络具体实现，这里不做过多讨论。 参考文献 Linux Switching – Interconnecting Namespaces Linux网络命名空间 linux中的网络命名空间的使用 LINUX 内核网络设备——VETH 设备和 NETWORK NAMESPACE 初步 Linux 虚拟网络设备 veth-pair 详解，看这一篇就够了 为什么docker容器之间能互通？为什么容器里能访问外网？ 模拟 Docker网桥连接外网 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/容器原理/ns-uts.html":{"url":"2.容器/容器原理/ns-uts.html","title":"ns-uts","keywords":"","body":"UTS命名空间 概念 UTS namespace有什么能力？ UTS(UNIX Time-sharing System) 命名空间提供了主机名（hostname）和域名（/etc/hosts解析）的隔离。 能够使得子进程有独立的主机名和域名(hostname) 这一特性在Docker容器技术中被用到，使得docker容器在网络上被视作一个独立的节点，而不仅仅是宿主机上的一个进程 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/容器编排/kubesphere/":{"url":"2.容器/容器编排/kubesphere/","title":"kubesphere","keywords":"","body":"扩容数据节点 ES每个索引默认5个主分片, 而每个主分片都相应的有一个副本。 查看es索引 [root@ceph01 ~]# kubectl -n kubesphere-logging-system exec -it elasticsearch-logging-data-0 -- curl -XGET 'localhost:9200/_cat/indices?v&pretty' health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open ks-logstash-log-2021.03.15 Ch7ACUwWQlinSz8WB8GvWA 5 1 2538499 0 1.8gb 962.4mb green open ks-logstash-events-2021.03.15 EkPSa0acR4umfw7hbf_kFA 5 1 16049 0 19.9mb 9.9mb green open ks-logstash-log-2021.03.16 GRrg9bRsRQKG22ef5WrDUw 5 1 1466740 0 1gb 556.7mb green open ks-logstash-events-2021.03.16 wpDRAwpPQ1e1B7E6mfawSw 5 1 8612 0 15.7mb 8mb 查看索引分片 [root@ceph01 ~]# kubectl -n kubesphere-logging-system exec -it elasticsearch-logging-data-0 -- curl -XGET 'localhost:9200/ks-logstash-log-2021.03.16/_search_shards' {\"nodes\":{\"J-y4ZBHES1uQX9u63Wn4Qw\":{\"name\":\"elasticsearch-logging-data-1\",\"ephemeral_id\":\"1KL5L_HqTRe4R2jb5TnqSw\",\"transport_address\":\"10.233.111.39:9300\",\"attributes\":{}},\"kyrYoJtoQQmoHk479cQ5yw\":{\"name\":\"elasticsearch-logging-data-0\",\"ephemeral_id\":\"b1wbbdqNRiq9017XPhdgZg\",\"transport_address\":\"10.233.111.22:9300\",\"attributes\":{}}},\"indices\":{\"ks-logstash-log-2021.03.16\":{}},\"shards\":[[{\"state\":\"STARTED\",\"primary\":true,\"node\":\"J-y4ZBHES1uQX9u63Wn4Qw\",\"relocating_node\":null,\"shard\":0,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"oBpLdhR7Tb6_l9CxK3n94A\"}},{\"state\":\"STARTED\",\"primary\":false,\"node\":\"kyrYoJtoQQmoHk479cQ5yw\",\"relocating_node\":null,\"shard\":0,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"Wnn1wRUtTGiaQQ2jydjcGA\"}}],[{\"state\":\"STARTED\",\"primary\":true,\"node\":\"kyrYoJtoQQmoHk479cQ5yw\",\"relocating_node\":null,\"shard\":1,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"EctVniCpRAOiOhS7-h8ebQ\"}},{\"state\":\"STARTED\",\"primary\":false,\"node\":\"J-y4ZBHES1uQX9u63Wn4Qw\",\"relocating_node\":null,\"shard\":1,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"reEv3HA3RN-DI9NIllD6ew\"}}],[{\"state\":\"STARTED\",\"primary\":true,\"node\":\"kyrYoJtoQQmoHk479cQ5yw\",\"relocating_node\":null,\"shard\":2,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"b3s-F7yqTSKLWIGZBDIfJg\"}},{\"state\":\"STARTED\",\"primary\":false,\"node\":\"J-y4ZBHES1uQX9u63Wn4Qw\",\"relocating_node\":null,\"shard\":2,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"Fe0052TWRAyYhB1ZS9WQoA\"}}],[{\"state\":\"STARTED\",\"primary\":false,\"node\":\"kyrYoJtoQQmoHk479cQ5yw\",\"relocating_node\":null,\"shard\":3,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"CptaDh8wSK-HBWUt-lzIkw\"}},{\"state\":\"STARTED\",\"primary\":true,\"node\":\"J-y4ZBHES1uQX9u63Wn4Qw\",\"relocating_node\":null,\"shard\":3,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"m335EmbPRF-4xrn_kHMQyA\"}}],[{\"state\":\"STARTED\",\"primary\":false,\"node\":\"J-y4ZBHES1uQX9u63Wn4Qw\",\"relocating_node\":null,\"shard\":4,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"2YDSnbxQRz-WIhQpSxfIgg\"}},{\"state\":\"STARTED\",\"primary\":true,\"node\":\"kyrYoJtoQQmoHk479cQ5yw\",\"relocating_node\":null,\"shard\":4,\"index\":\"ks-logstash-log-2021.03.16\",\"allocation_id\":{\"id\":\"U08x7XvpTLS1aWXraD2VSg\"}}]]} 返回数据导入json在线视图查看 扩容elasticsearch-logging-data 扩容elasticsearch-logging-data至5副本（默认2），以保证每个节点1主分片1副本分片 kubectl scale sts elasticsearch-logging-data -n kubesphere-logging-system --replicas=5 修改数据节点存储卷 修改数据节点默认存储类 调整索引过期策略 kubesphere在开启日志系统功能下，默认在kubesphere-logging-system有个定时任务，每天凌晨1点执行 [root@ceph01 ~]# kubectl get CronJob -n kubesphere-logging-system NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE elasticsearch-logging-curator-elasticsearch-curator 0 1 * * * False 0 19h 32h 执行内容如下： [root@ceph01 ~]# kubectl -n kubesphere-logging-system get configmaps elasticsearch-logging-curator-elasticsearch-curator-config -o yaml 即为action_file.yml --- actions: 1: action: delete_indices description: \"Clean up ES by deleting old indices\" options: timeout_override: continue_if_exception: False disable_action: False ignore_empty_list: True filters: - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 7 field: stats_result: epoch: exclude: False elastisearch curator 即es索引管理器 ，主要用于管理索引生命周期 上述配置解析： 删除七天前的索引 调整为30天过期 kubectl -n kubesphere-logging-system edit configmaps elasticsearch-logging-curator-elasticsearch-curator-config 调整unit_count: 7为unit_count: 30 elasticsearch-logging-curator-elasticsearch-curator-config 更多高阶配置参考curator官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/开源容器化/oss-emulator.html":{"url":"2.容器/开源容器化/oss-emulator.html","title":"oss-emulator","keywords":"","body":"基于kubernetes使用阿里云OSS服务 背景介绍 离线开发环境需要连接阿里云 OSS 服务， 通过调研我们决定使用 oss-emulator 模拟 OSS 服务。 项目介绍 oss-emulator 是阿里开源的轻量级 OSS 服务模拟器，提供与 OSS 服务相同的 API 接口 但该项目仅提供基于 linux 部署方案，不支持容器化，无法满足容器化部署场景 因此，我将 oss-emulator 项目打包成镜像发布到 docker hub，并提供容器化部署方案，便于后续使用。 容器化过程及验证 需求依赖：docker hub 账号 fork项目 配置 docker hub 账号口令 用于后续 github action 自动构建使用 票据内容如下： 登录 docker hub 创建 oss-emulator 仓库 到此为止，我们准备工作已经完成，接下来就是将项目打包成镜像 项目根新增 Dockerfile ,内容如下 FROM ruby:alpine MAINTAINER weiliang-ms@github WORKDIR /work ADD . . RUN gem install thor builder webrick CMD [\"ruby\",\"/work/bin/emulator\", \"-r\", \"store\", \"-p\", \"8080\"] 项目根新增 github action 流水线配置 配置路径：.github/workflows/build.yml 配置内容如下： name: ci on: push: branches: - 'master' jobs: docker: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up QEMU uses: docker/setup-qemu-action@v1 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Login to DockerHub uses: docker/login-action@v1 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} - name: Build and push uses: docker/build-push-action@v2 with: context: ./ file: Dockerfile push: true tags: xzxwl/oss-emulator 提交变更后，github action 流水线基于配置进行镜像构建，最终将生成的 xzxwl/oss-emulator 镜像推送至 docker hub 启动 oss-emulator 容器 $ mkdir -p /work/oss-data $ docker run -idt --name oss-emulator -p 8080:8080 -v /work/oss-data:/work/store xzxwl/oss-emulator 下载 ossutil 测试管理 oss-emulator 下载授权 $ wget https://gosspublic.alicdn.com/ossutil/1.7.14/ossutil64 $ chmod 755 ossutil64 测试 OSS 可用性 测试创建 bucket 功能 $ ./ossutil64 -e http://127.0.0.1:8080 -i AccessKeyId -k AccessKeySecret mb oss://bucket-test 0.005875(s) elapsed 测试查询 bucket $ ./ossutil64 -e http://127.0.0.1:8080 -i AccessKeyId -k AccessKeySecret ls oss://bucket-test Object Number is: 0 0.006463(s) elapsed 测试上传文件 $ touch test.file $ ./ossutil64 -e http://127.0.0.1:8080 -i AccessKeyId -k AccessKeySecret cp test.file oss://bucket-test/ Succeed: Total num: 1, size: 0. OK num: 1(upload 1 files). average speed 0(byte/s) 0.014518(s) elapsed $ ./ossutil64 -e http://127.0.0.1:8080 -i AccessKeyId -k AccessKeySecret ls oss://bucket-test/ LastModifiedTime Size(B) StorageClass ETAG ObjectName 2023-01-11 21:56:51 -0500 EST 0 Standard D41D8CD98F00B204E9800998ECF8427E oss://bucket-test/test.file Object Number is: 1 0.008415(s) elapsed 该项目并非完全兼容 OSS 接口，仅支持以下接口内容，使用时注意 接口兼容性 oss-emulator 支持 put, get, list, copy, delete, multipart 等数据操作API接口，支持部分Bucket操作接口。 Bucket相关接口 支持 ListBuckets(GetService),PutBucket(CreateBucket),GetBucket,DeleteBucket, GetBucketLocation,GetBucketInfo,PutBucketACL,GetBucketACL 不支持 PutBucketLogging,PutBucketWebsite,PutBucketReferer,PutBucketLifecycle, GetBucketLogging,GetBucketWebsite,GetBucketReferer,GetBucketLifecycle, DeleteBucketLogging,DeleteBucketWebsite,DeleteBucketLifecycle Object相关接口 支持 PutObject,CopyObject,AppendObject,GetObject,DeleteObject,DeleteMultipleObjects, HeadObject,GetObjectMeta,PutObjectACL,GetObjectACL 不支持 PostObject,Callback,PutSymlink,GetSymlink,RestoreObject Multipart相关接口 支持 InitiateMultipartUpload,UploadPart,CompleteMultipartUpload 不支持 UploadPartCopy,AbortMultipartUpload,ListMultipartUpload,ListParts 容器化部署方式 docker 创建数据持久化目录 $ mkdir -p /oss-store 启动服务 $ docker run -idt --name oss-emulator -p 8080:8080 --restart=always -v /work/oss-data:/work/store xzxwl/oss-emulator kubernetes 创建 pvc $ cat 创建 deployment $ cat 创建 service $ cat 此时即可在 kubernetes 内部通过以下地址使用 oss 服务 oss-emulator-svc.test:80 AK, SK 可以配置如下 AK: AccessKeyId SK: AccessKeySecret 对于 xzxwl/oss-emulator 这个镜像，离线环境下可以上传至私有镜像库（例如harbor）进行管理使用 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:41 "},"2.容器/运行时/docker/cli/cli.html":{"url":"2.容器/运行时/docker/cli/cli.html","title":"cli","keywords":"","body":" Table of Contents generated with DocToc docker源码阅读笔记 命令行 cobra实现 docker源码阅读笔记 命令行 基于cobra开发 cobra实现 operationSubCommands方法 解析命令行入参，返回参数数组： 若命令存在（未被移除 && 非隐藏类型命令）且不含子命令，依次放入数组中 func operationSubCommands(cmd *cobra.Command) []*cobra.Command { var cmds []*cobra.Command for _, sub := range cmd.Commands() { if sub.IsAvailableCommand() && !sub.HasSubCommands() { cmds = append(cmds, sub) } } return cmds } hasSubCommands方法 判断docker命令是否含有子命令或参数 调用operationSubCommands对返回的数组进行容量判断，长度大于0返回true（即含有子命令） 如docker ps不含有子命令，而docker save含有子命令（-o） func hasSubCommands(cmd *cobra.Command) bool { return len(operationSubCommands(cmd)) > 0 } FlagErrorFunc方法 判断docker命令合法性： func FlagErrorFunc(cmd *cobra.Command, err error) error { if err == nil { return nil } usage := \"\" if cmd.HasSubCommands() { usage = \"\\n\\n\" + cmd.UsageString() } return StatusError{ Status: fmt.Sprintf(\"%s\\nSee '%s --help'.%s\", err, cmd.CommandPath(), usage), StatusCode: 125, } } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/cmd/cmd.html":{"url":"2.容器/运行时/docker/cmd/cmd.html","title":"cmd","keywords":"","body":" Table of Contents generated with DocToc 清理容器 清理镜像 普通用户访问docker sudo groupadd docker #添加docker用户组 usermod -aG docker $USER newgrp docker #更新用户组 docker ps 清理容器 方式一： 显示所有的容器，过滤出Exited状态的容器，取出这些容器的ID， sudo docker ps -a|grep Exited|awk '{print $1}' 查询所有的容器，过滤出Exited状态的容器，列出容器ID，删除这些容器 sudo docker rm `docker ps -a|grep Exited|awk '{print $1}'` 方式二： 删除所有未运行的容器（已经运行的删除不了，未运行的就一起被删除了） sudo docker rm $(sudo docker ps -a -q) 方式三： 根据容器的状态，删除Exited状态的容器 sudo docker rm $(sudo docker ps -qf status=exited) 清理docker垃圾 docker image prune -a -f Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/image/image.html":{"url":"2.容器/运行时/docker/image/image.html","title":"image","keywords":"","body":" Table of Contents generated with DocToc 定义转义符 环境变量 忽略文件 FROM RUN CMD RUN vs CMD LABEL MAINTAINER EXPOSE ENV ADD COPY COPY VS ADD ENTRYPOINT shell form vs exec form USER WORKDIR VOLUME SHELL HEALTHCHECK STOPSIGNAL ONBUILD 定义转义符 适用于windows平台 # escape=` FROM microsoft/nanoserver COPY testfile.txt c:\\ RUN dir c:\\ 环境变量 FROM busybox ENV foo /bar WORKDIR ${foo} # WORKDIR /bar ADD . $foo # ADD . /bar COPY \\$foo /quux # COPY $foo /quux ${variable_name}支持bash一些标准： ${variable:-word} variable为空则取word的值 ${variable:+word} variable非空则取word的值 支持环境变量得到docker指令如下： ADD COPY ENV EXPOSE FROM LABEL STOPSIGNAL USER VOLUME WORKDIR 忽略文件 当执行构建build时docker-cli会先在指定的上下文目录中，寻找.dockerignore文件，docker-cli根据文件内容，排除context的路基目录或文件，随后再将信息发送给docker-daemon 例子如下： # comment */temp* */*/temp* temp? 不忽略的话，会全部提交过去，如果当前上下文目录下文件较多/大，会影响镜像的build速度 FROM FROM可以在一个Dockerfile出现多次 ARG与FROM交互 ARG CODE_VERSION=latest FROM base:${CODE_VERSION} CMD /code/run-app FROM extras:${CODE_VERSION} CMD /code/run-extras ARG生命周期 在FROM之前声明的ARG位于构建阶段之外，因此不能在FROM之后的任何指令中使用它。若要使用第一个FROM之前声明的ARG的默认值，请使用构建阶段中没有值的ARG指令 ARG VERSION=latest FROM busybox:$VERSION ARG VERSION RUN echo $VERSION > image_version RUN 格式一： RUN (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows) 格式二： RUN [\"executable\", \"param1\", \"param2\"] /bin/sh替换为/bin/bash RUN [\"/bin/bash\", \"-c\", \"echo hello\"] docker-deamon执行与shell执行 shell执行并返回结果 Step 1/2 : FROM alpine ---> 961769676411 Step 2/2 : RUN [ \"sh\", \"-c\", \"cat ~/.bash_profile\" ] ---> Running in f6a08aee1953 cat: can't open '/root/.bash_profile': No such file or directory The command 'sh -c cat ~/.bash_profile' returned a non-zero code: 1 docker-deamon执行并返回结果 Step 1/2 : FROM alpine ---> 961769676411 Step 2/2 : RUN [ \"cat ~/.bash_profile\" ] ---> Running in 59d4ac8f5ff7 OCI runtime create failed: container_linux.go:345: starting container process caused \"exec: \\\"cat ~/.bash_profile\\\": stat cat ~/.bash_profile: no such file or directory\": unknown CMD 格式一： CMD [\"executable\",\"param1\",\"param2\"] (exec form, this is the preferred form) 格式二： CMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT) 格式三： CMD command param1 param2 (shell form) CMD只能出现一次，最后的CMD指令会覆盖之前的指令 解析规则 默认解析为shell指令 FROM ubuntu CMD echo \"This is a test.\" | wc - 实际解析为 FROM ubuntu CMD /bin/sh -c echo \"This is a test.\" | wc - 以下格式必须使绝对径，并且命令用 \"\"引用 FROM ubuntu CMD [\"/usr/bin/wc\",\"--help\"] RUN vs CMD RUN 生效于镜像build时 Step 1/2 : FROM alpine ---> 961769676411 Step 2/2 : RUN cat ~/.bash_profile ---> Running in 446a0b3c52ce cat: can't open '/root/.bash_profile': No such file or directory The command '/bin/sh -c cat ~/.bash_profile' returned a non-zero code: 1 CMD生效于镜像启动为容器时 Step 1/2 : FROM alpine ---> 961769676411 Step 2/2 : CMD cat ~/.bash_profile ---> Running in 8b4e2c4c810f Removing intermediate container 8b4e2c4c810f ---> 92382df2a644 Successfully built 92382df2a644 Successfully tagged alpine01:latest docker run -idt alpine01 返回容器ID 4542bb3760f4036c8044be744fbf5c948045bbfe4216aa7ae719e596a41dd859 docker logs -f 4542bb3760f4036c8044be744fbf5c948045bbfe4216aa7ae719e596a41dd859 cat: can't open '/root/.bash_profile': No such file or directory LABEL 格式： LABEL = = = ... 样例： LABEL \"com.example.vendor\"=\"ACME Incorporated\" LABEL com.example.label-with-value=\"foo\" LABEL version=\"1.0\" LABEL description=\"This text illustrates \\ that label-values can span multiple lines.\" 声明为一行： LABEL multi.label1=\"value1\" multi.label2=\"value2\" other=\"value3\" LABEL multi.label1=\"value1\" \\ multi.label2=\"value2\" \\ other=\"value3\" MAINTAINER 最新版已被移除，可以使用Label指令代替 LABEL maintainer=\"SvenDowideit@home.org.au\" EXPOSE 仅作说明用途，不会实际开放端口 ENV 格式一： ENV 格式二： ENV = ... ADD 格式一： ADD [--chown=:] ... 格式二： ADD [--chown=:] [\"\",... \"\"] 支持通配符： ADD hom* /mydir/ # adds all files starting with \"hom\" ADD hom?.txt /mydir/ # ? is replaced with any single character, e.g., \"home.txt\" The is an absolute path, or a path relative to WORKDIR, into which the source will be copied inside the destination container 转义特殊字符： ADD arr[[]0].txt /mydir/ # copy a file named \"arr[0].txt\" to /mydir/ 指定文件所属及权限： ADD --chown=55:mygroup files* /somedir/ ADD --chown=bin files* /somedir/ ADD --chown=1 files* /somedir/ ADD --chown=10:11 files* /somedir/ If the container root filesystem does not contain either /etc/passwd or /etc/group files and either user or group names are used in the --chown flag, the build will fail on the ADD operation. Using numeric IDs requires no lookup and will not depend on container root filesystem content. ADD规则： 必须位于build 上下文路径中，不可以使用相对路径（如ADD ../something /something） 尽量以\"/\"结尾（ADD http://192.168.1.2:80/file1 /root/file2,会解析为将file1下载到/root下并命名为file2） 当为目录时，则复制目录下的全部内容，包括文件系统元数据但不包含该目录 为可识别的压缩类型文件时（identity, gzip, bzip2 or xz，与文件名无关），会自动被解压 If doesn’t exist, it is created along with all missing directories in its path. COPY 格式一： COPY [--chown=:] ... 格式二： COPY [--chown=:] [\"\",... \"\"] COPY VS ADD COPY不会自动解压缩 COPY支持为URL类型 ENTRYPOINT 格式一： ENTRYPOINT [\"executable\", \"param1\", \"param2\"] 格式二： ENTRYPOINT command param1 param2 shell form vs exec form 两者的区别如下： shell form vs exec form shell form本质为交由shell执行（默认/bin/sh） exec form本质为交由docker-daemon执行 USER 指定镜像启动后的容器内程序所属用户,默认root启动 USER [:] or USER [:] WORKDIR 指定构建阶段的目录上下文 VOLUME 映射docker宿主机与容器目录 SHELL 多用于windows平台 The SHELL instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is [\"/bin/sh\", \"-c\"], and on Windows is [\"cmd\", \"/S\", \"/C\"]. The SHELL instruction must be written in JSON form in a Dockerfile. HEALTHCHECK 健康检测端点，判断容器内服务状态： HEALTHCHECK --interval=5m --timeout=3s \\ CMD curl -f http://localhost/ || exit 1 可选配置： // 每30s检测一次 --interval=DURATION (default: 30s) // 超时响应时间（超过30s未响应，代表不健康） --timeout=DURATION (default: 30s) // 容器启动多久后开启检测(取决于容器启动速度) --start-period=DURATION (default: 0s) // 重试次数 --retries=N (default: 3) 退出码： 0: success - the container is healthy and ready for use 1: unhealthy - the container is not working correctly STOPSIGNAL 退出信号，默认SIGTERM（强制退出） STOPSIGNAL 9 可修改该值实现程序的平滑退出，值可以为信号名也可以为数字 ONBUILD 相当于触发器，下次该镜像作为基础镜像时被触发 The ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. ONBUILD [INSTRUCTION] ONBUILD不可触发的指令如下： FROM ONBUILD MAINTAINER docker build image Dockerfile指令 创建目录 mkdir -p /docker/simple 创建dockerfile FROM alpine # 设置变量 ENV NGINX_VERSION 1.16.1 # 修改源 RUN echo \"http://mirrors.aliyun.com/alpine/latest-stable/main/\" > /etc/apk/repositories && \\ echo \"http://mirrors.aliyun.com/alpine/latest-stable/community/\" >> /etc/apk/repositories && \\ # 安装需要的软件 apk update && \\ apk add --no-cache ca-certificates && \\ apk add --no-cache curl bash tree tzdata && \\ cp -rf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && \\ # 编译安装nginx GPG_KEYS=B0F4253373F8F6F510D42178520A9993A1C052F8 \\ && CONFIG=\"\\ --prefix=/opt/nginx \\ --sbin-path=/usr/sbin/nginx \\ --modules-path=/usr/lib/nginx/modules \\ --conf-path=/opt/nginx/conf/nginx.conf \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --pid-path=/var/run/nginx.pid \\ --lock-path=/var/run/nginx.lock \\ --http-client-body-temp-path=/var/cache/nginx/client_temp \\ --http-proxy-temp-path=/var/cache/nginx/proxy_temp \\ --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp \\ --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp \\ --http-scgi-temp-path=/var/cache/nginx/scgi_temp \\ --user=nginx \\ --group=nginx \\ --with-http_ssl_module \\ --with-http_realip_module \\ --with-http_addition_module \\ --with-http_sub_module \\ --with-http_gunzip_module \\ --with-http_gzip_static_module \\ --with-http_random_index_module \\ --with-http_secure_link_module \\ --with-http_stub_status_module \\ --with-http_auth_request_module \\ --with-http_xslt_module=dynamic \\ --with-http_image_filter_module=dynamic \\ --with-http_geoip_module=dynamic \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_ssl_preread_module \\ --with-stream_realip_module \\ \" \\ && addgroup -S nginx \\ && adduser -D -S -h /var/cache/nginx -s /sbin/nologin -G nginx nginx \\ && apk add --no-cache --virtual .build-deps \\ gcc \\ libc-dev \\ make \\ openssl-dev \\ pcre-dev \\ zlib-dev \\ linux-headers \\ curl \\ gnupg \\ libxslt-dev \\ gd-dev \\ geoip-dev \\ && curl -fSL http://nginx.org/download/nginx-$NGINX_VERSION.tar.gz -o nginx.tar.gz \\ && mkdir -p /usr/src \\ && tar -zxC /usr/src -f nginx.tar.gz \\ && rm nginx.tar.gz \\ && cd /usr/src/nginx-$NGINX_VERSION \\ && ./configure $CONFIG --with-debug \\ && make -j$(getconf _NPROCESSORS_ONLN) \\ && mv objs/nginx objs/nginx-debug \\ && mv objs/ngx_http_xslt_filter_module.so objs/ngx_http_xslt_filter_module-debug.so \\ && mv objs/ngx_http_image_filter_module.so objs/ngx_http_image_filter_module-debug.so \\ && ./configure $CONFIG \\ && make -j$(getconf _NPROCESSORS_ONLN) \\ && make install \\ && rm -rf /opt/nginx/html/ \\ && mkdir /opt/nginx/conf/conf.d/ \\ && mkdir -p /usr/share/nginx/html/ \\ && install -m644 html/index.html /usr/share/nginx/html/ \\ && install -m644 html/50x.html /usr/share/nginx/html/ \\ && install -m755 objs/nginx-debug /usr/sbin/nginx-debug \\ && install -m755 objs/ngx_http_xslt_filter_module-debug.so /usr/lib/nginx/modules/ngx_http_xslt_filter_module-debug.so \\ && install -m755 objs/ngx_http_image_filter_module-debug.so /usr/lib/nginx/modules/ngx_http_image_filter_module-debug.so \\ && ln -s ../../usr/lib/nginx/modules /opt/nginx/modules \\ && strip /usr/sbin/nginx* \\ && strip /usr/lib/nginx/modules/*.so \\ && rm -rf /usr/src/nginx-$NGINX_VERSION \\ \\ # Bring in gettext so we can get `envsubst`, then throw # the rest away. To do this, we need to install `gettext` # then move `envsubst` out of the way so `gettext` can # be deleted completely, then move `envsubst` back. && apk add --no-cache --virtual .gettext gettext \\ && mv /usr/bin/envsubst /tmp/ \\ \\ && runDeps=\"$( \\ scanelf --needed --nobanner /usr/sbin/nginx /usr/lib/nginx/modules/*.so /tmp/envsubst \\ | awk '{ gsub(/,/, \"\\nso:\", $2); print \"so:\" $2 }' \\ | sort -u \\ | xargs -r apk info --installed \\ | sort -u \\ )\" \\ && apk add --no-cache --virtual .nginx-rundeps $runDeps \\ && apk del .build-deps \\ && apk del .gettext \\ && mv /tmp/envsubst /usr/local/bin/ \\ \\ # forward request and error logs to docker log collector && ln -sf /dev/stdout /var/log/nginx/access.log \\ && ln -sf /dev/stderr /var/log/nginx/error.log # 开放80端口 EXPOSE 80 STOPSIGNAL SIGTERM # 启动nginx命令 CMD [\"nginx\", \"-g\", \"daemon off;\"] 构建镜像 docker build -t nginx:1.16.1 . 11/16/2019 1:55:59 PM 批量导出 docker images |awk '{print $1}' |sed -n '2,$p' |xargs docker save -o k8s.tar docker multi-stage 创建目录 mkdir -p /docker/multi-stage 创建golang程序 cat > /docker/multi-stage/rancher.go 创建dockerfile 参考官方样例 cat > /docker/multi-stage/Dockerfile 构建镜像 docker build -t rancher-demo . 构建信息 镜像大小 运行容器 docker run --rm --name rancher -idt rancher-demo 查看日志 docker logs -f rancher 11/16/2019 1:19:36 PM Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/install/install.html":{"url":"2.容器/运行时/docker/install/install.html","title":"install","keywords":"","body":" 安装 升级内核 在线安装 配置yum源 配置yum代理 清理旧版本docker 安装docker 配置docker 关闭selinux 调整系统参数 配置阿里云加速 启动 安装 升级内核 建议升级内核 ，以便使用新特性 在线安装 配置yum源 yum可用跳过 CentOS 7 rm -f /etc/yum.repos.d/* curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo 配置yum代理 宿主机可直接访问互联网情况跳过 配置代理 vi /etc/yum.conf 文件最后添加以下内容 proxy=http://username:password@host:port username: http代理账号 password: http代理密码 host: http代理主机（ip或域名） port: http代理端口 清理旧版本docker yum remove docker -y \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine rm -rf /etc/systemd/system/docker.service.d rm -rf /var/lib/docker rm -rf /var/run/docker rm -rf /usr/local/docker rm -rf /etc/docker 安装docker 以下安装方式：二选一 安装最新版 yum -y install yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum -y install docker-ce 安装指定版本 查看可选版本 [root@localhost ~]# yum list docker-ce --showduplicates|sort -r * updates: mirrors.aliyun.com Loading mirror speeds from cached hostfile Loaded plugins: fastestmirror * extras: mirrors.aliyun.com docker-ce.x86_64 3:20.10.7-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.6-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.5-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.4-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.3-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.2-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.1-3.el7 docker-ce-stable docker-ce.x86_64 3:20.10.0-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.9-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.8-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.7-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.6-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.5-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.4-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.3-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.2-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.15-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.14-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.1-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.13-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.12-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.11-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.10-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.0-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.9-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.8-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.7-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.6-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.5-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.4-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.3-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.2-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stable docker-ce.x86_64 18.06.3.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.2.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.03.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.12.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.12.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.09.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.09.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.2.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.3.ce-1.el7 docker-ce-stable docker-ce.x86_64 17.03.2.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable 安装指定版本 ```bash yum install -y docker-ce-19.03.15-3.el7 配置docker 关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 调整系统参数 sed -i ':a;$!{N;ba};s@# docker limit BEGIN.*# docker limit END@@' /etc/security/limits.conf cat > /etc/security/limits.conf # docker limit BEGIN * soft nofile 65535 * hard nofile 65535 * soft nproc 65535 * hard nproc 65535 # docker limit END EOF sed -i \"/user.max_user_namespaces/d\" /etc/sysctl.conf echo \"user.max_user_namespaces=15000\" >> /etc/sysctl.conf sysctl -p ulimit -u 65535 ulimit -n 65535 echo 'net.ipv4.ip_forward = 1' >> /etc/sysctl.conf echo 'net.bridge.bridge-nf-call-arptables = 1' >> /etc/sysctl.conf echo 'net.bridge.bridge-nf-call-ip6tables = 1' >> /etc/sysctl.conf echo 'net.bridge.bridge-nf-call-iptables = 1' >> /etc/sysctl.conf echo 'vm.max_map_count = 262144' >> /etc/sysctl.conf echo 'vm.swappiness = 1' >> /etc/sysctl.conf echo 'fs.inotify.max_user_instances = 524288' >> /etc/sysctl.conf #See https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/ sed -r -i \"s@#{0,}?net.ipv4.tcp_tw_recycle ?= ?(0|1)@net.ipv4.tcp_tw_recycle = 0@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?net.ipv4.ip_forward ?= ?(0|1)@net.ipv4.ip_forward = 1@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?net.bridge.bridge-nf-call-arptables ?= ?(0|1)@net.bridge.bridge-nf-call-arptables = 1@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?net.bridge.bridge-nf-call-ip6tables ?= ?(0|1)@net.bridge.bridge-nf-call-ip6tables = 1@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?net.bridge.bridge-nf-call-iptables ?= ?(0|1)@net.bridge.bridge-nf-call-iptables = 1@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?vm.max_map_count ?= ?(0|1)@vm.max_map_count = 262144@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?vm.swappiness ?= ?(0|1)@vm.swappiness = 1@g\" /etc/sysctl.conf sed -r -i \"s@#{0,}?fs.inotify.max_user_instances ?= ?(0|1)@fs.inotify.max_user_instances = 524288@g\" /etc/sysctl.conf awk ' !x[$0]++{print > \"/etc/sysctl.conf\"}' /etc/sysctl.conf 配置docker daemon mkdir -p /etc/docker cat /etc/docker/daemon.json { \"log-opts\": { \"max-size\": \"10m\", \"max-file\":\"3\" }, \"userland-proxy\": false, \"live-restore\": true, \"default-ulimits\": { \"nofile\": { \"Hard\": 65535, \"Name\": \"nofile\", \"Soft\": 65535 } }, \"default-address-pools\": [ { \"base\": \"172.80.0.0/16\", \"size\": 24 }, { \"base\": \"172.90.0.0/16\", \"size\": 24 } ], \"no-new-privileges\": false, \"default-gateway\": \"\", \"default-gateway-v6\": \"\", \"default-runtime\": \"runc\", \"default-shm-size\": \"64M\", \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF 配置阿里云加速 可选 调整/etc/docker/daemon.json 添加如下内容 \"registry-mirrors\": [\"https://jz73200c.mirror.aliyuncs.com\"] 重启 sudo systemctl daemon-reload sudo systemctl restart docker 启动 systemctl daemon-reload systemctl enable docker --now Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/network/network.html":{"url":"2.容器/运行时/docker/network/network.html","title":"network","keywords":"","body":" Table of Contents generated with DocToc complete network communication between two containers via veth-paris & network namespaces. complete network communication between two containers via veth-paris & network namespaces. 1、安装pipework yum install git bridge-utils -y git clone https://github.com/jpetazzo/pipework cd pipework cp pipework /usr/bin/ 2、配置pipework 修改物理网卡配置 修改前： TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=static DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens33 UUID=5b8e5f27-6e21-4a48-97eb-05cca1056a98 DEVICE=ens33 IPADDR=192.168.40.136 NETMASK=255.255.255.0 GATEWAY=192.168.40.2 PREFIX=24 ONBOOT=yes ZONE= 修改后： TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no BOOTPROTO=none DEFROUTE=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_FAILURE_FATAL=no IPV6_ADDR_GEN_MODE=stable-privacy NAME=ens33 UUID=5b8e5f27-6e21-4a48-97eb-05cca1056a98 DEVICE=ens33 #IPADDR=192.168.40.136 #NETMASK=255.255.255.0 #GATEWAY=192.168.40.2 #PREFIX=24 ONBOOT=yes ZONE= BRIDGE=br0 创建网桥，宿主机IP为136（原物理网卡ens33 IP地址） cat >> /etc/sysconfig/network-scripts/ifcfg-br0 重启网络 systemctl restart network 查看宿主机网络 ip a 创建容器A，并配置独立IP 180 docker run -itd --net=none --name testA busybox /bin/sh pipework br0 testA 192.168.40.180/24@192.168.40.2 创建容器B，并配置独立IP 190 docker run -itd --net=none --name testB busybox /bin/sh pipework br0 testB 192.168.40.190/24@192.168.40.2 查看容器testA IP地址 docker exec -it testA ip a 查看容器testB IP地址 docker exec -it testB ip a 宿主机ping testA testB 网络 容器A、容器B测试网络连通性 docker exec -it testA ping 192.168.40.190 -c 3 docker exec -it testB ping 192.168.40.180 -c 3 11/18/2019 7:56:37 PM Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/setting/storage-quota.html":{"url":"2.容器/运行时/docker/setting/storage-quota.html","title":"storage-quota","keywords":"","body":"设置docker容器存储空间限额 适用 docker 场景 Storage Driver: overlay2 Backing Filesystem: xfs 环境准备 测试机安装 docker 测试机添加数据盘，用于模拟 docker 持久化目录 初始化环境信息如下: $ df -Th Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 7.8G 0 7.8G 0% /dev tmpfs tmpfs 7.8G 0 7.8G 0% /dev/shm tmpfs tmpfs 7.8G 8.9M 7.8G 1% /run tmpfs tmpfs 7.8G 0 7.8G 0% /sys/fs/cgroup /dev/mapper/centos-root xfs 49G 1.9G 48G 4% / /dev/sda1 xfs 1014M 195M 820M 20% /boot tmpfs tmpfs 1.6G 0 1.6G 0% /run/user/0 /dev/sdb1 xfs 100G 33M 100G 1% /xfs $ cat /etc/docker/daemon.json { \"registry-mirrors\":[ \"https://pee6w651.mirror.aliyuncs.com\", \"https://docker.mirrors.ustc.edu.cn\", \"http://hub-mirror.c.163.com\" ], \"insecure-registries\":[\"gcr.azk8s.cn\",\"dockerhub.azk8s.cn\",\"quay.azk8s.cn\",\"5twf62k1.mirror.aliyuncs.com\",\"registry.docker-cn.com\",\"registry-1.docker.io\"], \"max-concurrent-downloads\":3, \"log-driver\":\"json-file\", \"log-opts\":{ \"max-size\":\"100m\", \"max-file\":\"1\" }, \"max-concurrent-uploads\":3, \"storage-driver\":\"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"live-restore\": true } docker 持久化目录文件系统类型必须为 xfs 调整docker持久化目录 修改 /etc/docker/daemon.json 文件内容，调整 docker 持久化目录为 xfs 分区下，调整后配置如下 $ cat /etc/docker/daemon.json { \"registry-mirrors\":[ \"https://pee6w651.mirror.aliyuncs.com\", \"https://docker.mirrors.ustc.edu.cn\", \"http://hub-mirror.c.163.com\" ], \"insecure-registries\":[\"gcr.azk8s.cn\",\"dockerhub.azk8s.cn\",\"quay.azk8s.cn\",\"5twf62k1.mirror.aliyuncs.com\",\"registry.docker-cn.com\",\"registry-1.docker.io\"], \"max-concurrent-downloads\":3, \"log-driver\":\"json-file\", \"log-opts\":{ \"max-size\":\"100m\", \"max-file\":\"1\" }, \"data-root\": \"/xfs/docker\", \"max-concurrent-uploads\":3, \"storage-driver\":\"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"live-restore\": true } 即以下配置内容 \"data-root\": \"/xfs/docker\", 重载配置以生效 $ systemctl daemon-reload $ systemctl restart docker 启动测试容器，创建40G文件 $ docker run -idt --name ddd alpine:latest $ docker exec -it ddd sh / # dd if=/dev/zero of=test.file bs=1M count=40960 40960+0 records in 40960+0 records out / # du -sh test.file 40.0G test.file 查看容器磁盘使用量 $ docker system df -v |grep \"alpine:latest\" a8c777259823 alpine:latest \"/bin/sh\" 0 42.9GB 27 minutes ago Up 27 minutes ddd 清理掉测试容器 $ docker rm -f ddd 修改 /etc/fstab 挂载参数，开启磁盘配额功能 否则配置容器存储限额后，启动会报如下异常 Jan 03 10:14:01 localhost dockerd[52920]: failed to start daemon: error initializing graphdriver: Storage option overlay2.size not supported. Filesystem does not support Project Quota: Filesystem does not support, or has not enabled quotas 修改前，挂载参数 $ mount |grep sdb1 /dev/sdb1 on /xfs type xfs (rw,relatime,attr2,inode64,noquota) 修改 /etc/fstab 内 /xfs 挂载点挂载参数，以开启磁盘配额功能 $ cat /etc/fstab|grep sdb1 /dev/sdb1 /xfs xfs defaults,usrquota,prjquota 0 0 刷新挂载信息 $ mount -av / : ignored /boot : already mounted /xfs : successfully mounted 此时挂载参数（已生效） $ mount |grep sdb1 /dev/sdb1 on /xfs type xfs (rw,relatime,attr2,inode64,usrquota,prjquota) 配置 docker 容器存储配额为 20G $ cat /etc/docker/daemon.json { \"registry-mirrors\":[ \"https://pee6w651.mirror.aliyuncs.com\", \"https://docker.mirrors.ustc.edu.cn\", \"http://hub-mirror.c.163.com\" ], \"insecure-registries\":[\"gcr.azk8s.cn\",\"dockerhub.azk8s.cn\",\"quay.azk8s.cn\",\"5twf62k1.mirror.aliyuncs.com\",\"registry.docker-cn.com\",\"registry-1.docker.io\"], \"max-concurrent-downloads\":3, \"log-driver\":\"json-file\", \"log-opts\":{ \"max-size\":\"100m\", \"max-file\":\"1\" }, \"data-root\": \"/xfs/docker\", \"max-concurrent-uploads\":3, \"storage-driver\":\"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\", \"overlay2.size=20G\" ], \"live-restore\": true } 即添加如下内容 \"storage-opts\": [ \"overlay2.size=20G\" ], 重载以生效 $ systemctl daemon-reload $ systemctl restart docker 再次启动测试容器，创建 40G 文件 $ docker run -idt --name ddd alpine:latest $ docker exec -it ddd sh / # dd if=/dev/zero of=test.file bs=1M count=40960 40960+0 records in 40960+0 records out / # du -sh test.file 20.0G test.file 通过添加容量配额后，当我们尝试创建 40G 文件，只能创建出 20G 大小的文件，即我们配置里所限制的大小。 上面结果表明容器配额已生效。 清理掉测试容器 $ docker rm -f ddd Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/storage/storage.html":{"url":"2.容器/运行时/docker/storage/storage.html","title":"storage","keywords":"","body":" Table of Contents generated with DocToc volums 管理方式 使用场景 管理volume Bind mounts volums与Bind mounts对比 关于volums与Bind mounts使用说明 tmpfs 存储驱动 可写入层 写时拷贝 overlay2 几种数据持久方案区别 volume：对宿主机文件系统之上的 容器可写入层 进行读写操作 Bind mounts：宿主机与容器以映射的方式共享目录，对宿主机文件系统进行读写操作 tmpfs：数据存储于内存中，不持久化 volums Volumes方式的数据存储于docker宿主机的/var/lib/docker/volumes/下（Linux系统下），由于linux严格的权限管理，非docker进程无法修改该目录下数据，具有很好的隔离性，volums为目前最好的docker容器数据持久化的方式。 管理方式 由docker进行创建管理，创建方式为两种： 1、手动创建 docker volume create 2、自动创建 随容器/服务的创建而被创建 使用场景 1、多容器运行时共享数据 2、当Docker主机不能保证具有给定的目录或文件结构时。卷可以帮助您将Docker主机的配置与容器运行时解耦。（例如A环境具有/data目录，当迁移至B环境不具有/data目录） 3、存储容器数据于远程主机 4、备份、迁移容器数据等至远程主机，备份目录 /var/lib/docker/volumes/ 管理volume #1、手动创建 docker volume create my-volume #查看volume docker inspect my-volume #2、创建启动容器时指定volume docker run -d --name test-nginx --mount source=myvol2,target=/app nginx:latest #查看volume列表(默认在创建启动容器时自动创建了myvol2) docker volume ls #接下来，测试删除容器后volume是否会随之被删除(很显然删除容器对volume并未产生影响，需手动删除) #手动删除volume volume指定为只读方式 #创建启动容器，指定volumew名为nginx-vol（实际路径为/var/lib/docker/volumes/nginx-vol），对应容器内/usr/share/nginx/html目录，并且为只读状态（容器内禁止写操作） docker run -d --name=nginxtest --mount source=nginx-vol,destination=/usr/share/nginx/html,readonly nginx:latest #查看本地volume情况(nginx容器内的数据已挂载出来) #访问容器内，验证可读属性（） docker container exec -it nginxtest bash cd /usr/share/nginx/html echo 1 >> 50x.html Bind mounts Bind mounts方式理论上可以在宿主机任意位置持久化数据，显然非docker进程可以修改这部分数据，隔离性较差。 使用场景 1、容器与宿主机共享配置文件等（如dns、时区等） 2、共享源代码或打包后的应用程序（例如：宿主机maven打包java程序，只需挂载target/目录，容器内即可访问到打包好的应用程序） 当然，该方式仅适用于开发环境，安全考虑并不推荐使用于生产环境 3、Docker主机的文件或目录结构与容器所需的绑定挂载一致时。（例如容器内读取配置文件目录为/etc/redis.conf,而宿主机/etc/redis.conf并不存在，则需要匹配路径进行挂载） volums与Bind mounts对比 Volumes方式更容易备份、迁移 Volumes可以通过docker命令行指令或api进行管理 Volumes适用于windows与linux环境 Volumes多容器分享数据更安全（非docker进程无法修改该部分数据） Volume drivers可以实现加密传输数据持久化至远程主机。 新volume的内容可以由容器预先填充 关于volums与Bind mounts使用说明 如果你挂载一个空的volums到容器的/data目录，并且容器内/data下数据非空，则容器内的/data数据会拷贝到新挂载的卷上；相似的，如果你挂载了宿主机不存在的volums至容器内部，这个不存在的目录则会自动创建 如果你使用bind mount或挂载一个非空的volum到容器的/data目录，并且容器内/data下数据非空，则容器内的/data部分数据会被覆盖（相同目录/文件名） tmpfs tmpfs方式数据存储宿主机系统内存中，并且不会持久化到宿主机的文件系统中（磁盘） 使用场景 存储敏感数据（swarm利用tmpfs存放secrets于内存中） 数据共享 与volume、bind mounts方式不同，tmpfs无法跨容器共享数据，即仅适用于单机模式。 环境依赖说明 tmpfs仅支持Linux环境下docker使用 使用方式 #nginx容器内/app下的数据将写入至宿主机内存之中。 #1、方式一 docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app \\ nginx:latest #2、方式二 docker run -d \\ -it \\ --name tmptest \\ --tmpfs /app \\ nginx:latest 验证数据是否存储于内存中 #1、分配虚拟机8G内存 #2、启动nginx容器，指定--tmpfs方式存储数据 #3、查看内存占用 docker ps #bf5605056391为容器ID ps -ef|grep bf5605056391 #11298为容器进程PID top -p 11298 #4、拷贝918M大小文件至容器/app下 ，观察内存变化 docker cp CentOS-7-x86_64-Minimal-1810.iso bf5605056391:/app/ #5、查看内存占用 free -h #6、多次拷贝观察宿主机内存变化（文件名注意修改，避免被覆盖） docker cp CentOS-7-x86_64-Minimal-1810.iso bf5605056391:/app/CentOS-7-x86_64-Minimal-1810.iso2 docker cp CentOS-7-x86_64-Minimal-1810.iso bf5605056391:/app/CentOS-7-x86_64-Minimal-1810.iso2 docker cp CentOS-7-x86_64-Minimal-1810.iso bf5605056391:/app/CentOS-7-x86_64-Minimal-1810.iso4 #7、内存变化（大约900M左右的递减，与文件大小相匹配） #8、让我们看下容器内部/app下存储了什么(nothing,说明容器内写入/app的数据都被写入到了内存里，/app下也并不会持久化) #9、如果我们通过不断向容器内/app下写入数据会怎样？(竟然没有出现crash相关情况发生，初步猜测最开始存放的数据被回收释放) #10、测试删除容器，内存是否会被释放 docker container rm -f bf5605056391 验证tmpfs存储大小限制 #1、启动nginx容器，1073741824为字节数（等于1GB） 换算地址 http://www.elecfans.com/tools/zijiehuansuan.html 官方配置说明 docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app,tmpfs-size=1073741824 \\ nginx:latest #2、不断拷贝数据到容器内部，观察内存大小 竟然没有生效，难道是配置错了？ #3、删除容器，重启配置(官方提供信息指出默认单位是字节，可实际看来怎么是MB，难道理解有误？) docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app,tmpfs-size=1024 \\ nginx:latest #4、删除容器，重启配置(1GB大小限制未生效，难道单位问题？) docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app,tmpfs-size=1GB \\ nginx:latest #5、删除容器，重启配置(1G大小限制未生效) docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app,tmpfs-size=1G \\ nginx:latest #6、删除容器，重启配置，限制大小10M(？？？竟然也未生效？，难道第三步测试有误) docker run -d \\ -it \\ --name tmptest \\ --mount type=tmpfs,destination=/app,tmpfs-size=10 \\ nginx:latest 仔细观察第三步测试，拷贝到容器内的文件名一样，果然测试有误 #7、看来这个配置项并未生效 存储驱动 以下内容基于overlay2 overlay2为docker存储驱动的一种，负责容器读写（不会修改镜像原始数据）镜像数据 可写入层 writable layer 官方文档描述如下： When you start a container, a thin writable container layer is added on top of the other layers. Any changes the container makes to the filesystem are stored here. Any files the container does not change do not get copied to this writable layer. This means that the writable layer is as small as possible. 假设现在有镜像A，B构建文件如下 #A镜像构建文件（构建镜像名为：imageA，拷贝内容为hello.sh、app.py） FROM ubuntu:18.04 COPY ./hello.py ./hello.sh /app/ CMD python /app/app.py #B镜像构建文件（构建镜像名为：imageB） FROM imageB CMD /bin/bash /app/hello.sh #hello.sh内容 #!/bin/sh echo \"Hello world\" #hello.py内容 print('hello python!') 目录结构及文件内容如下 构建镜像ab docker build -t imagea -f Dockerfile.A . docker build -t imageb -f Dockerfile.B . 查看两者层级 很显然，A镜像红框内的3个层级分别由以下构建命令生成，其他层级由基础镜像ubuntu:18.04生成 FROM ubuntu:18.04 COPY ./hello.py ./hello.sh /app/ CMD python /app/app.py 而B镜像对比A镜像之多出一个层级313438ff3ff4，即以下构建指令生成的层，该层即为可写入层，镜像B与镜像A的区别存储于该层 而镜像B与镜像A相同的层级指向同一系统存储地址， 由于该层仅为一条shell指令故大小可以忽略不计，即构建B镜像理论上对宿主机磁盘的占用忽略不计，由此可看见docker的分层结构相当节省存储空间 Any changes the container makes to the filesystem are stored here CMD /bin/bash /app/hello.sh 测试以上结论是否正确 构建镜像CD，配置文件内容为如下： #Dockerfile.C FROM imageb COPY CentOS-7-x86_64-Minimal-1804.iso / #Dockerfile.D FROM imagec CMD /bin/bash /app/hello.py 此时磁盘空间大小 文件大小为906M，理论上构建CD镜像后，磁盘剩余空间在14031-906=13125M左右 docker build -t imagec -f Dockerfile.C . docker build -t imaged -f Dockerfile.D . 很显然测试结果验证了以上的结论（当然官方已经说明了，这里仅仅是测试而已），我们再看下镜像cd的层级 测试删除镜像C，对磁盘空间有何影响 分析：由于镜像C与镜像D有相同分层，且镜像D比镜像C多一个层级，即可以理解为D内数据包含C数据，删除镜像C对宿主机磁盘空间无任何影响 由此可见，验证了我们的猜测：删除镜像C对宿主机磁盘空间无任何影响（存在镜像D包含镜像C所有数据内容） 测试生成镜像E，构建内容为镜像C的内容，宿主机磁盘空间是否变化。 分析：因为E的所有数据内容都已存在（镜像D持有），故磁盘空间不会发生变化 #Dockerfile.E FROM imageb COPY CentOS-7-x86_64-Minimal-1804.iso / docker build -t imagee -f Dockerfile.E . 显然猜测成立 测试删除镜像D、E，对磁盘空间有何影响 分析：由于镜像C、镜像D、镜像E均持有COPY CentOS-7-x86_64-Minimal-1804.iso /层级，镜像C已经删除，此时再删除镜像D、镜像E， COPY CentOS-7-x86_64-Minimal-1804.iso /层级无其他镜像引用，导致磁盘空间释放906M 猜测成立！ 结论如下： docker分层架构在很大程度上节省了磁盘存储开销（镜像文件一般较大），相同层级只存储一份 删除镜像时，只会删除与其他镜像非同层级数据 copy-on-write 写时拷贝 官方对Copy-on-write的说明: Copy-on-write is a strategy of sharing and copying files for maximum efficiency. If a file or directory exists in a lower layer within the image, and another layer (including the writable layer) needs read access to it, it just uses the existing file. The first time another layer needs to modify the file (when building the image or running the container), the file is copied into that layer and modified. This minimizes I/O and the size of each of the subsequent layers. These advantages are explained in more depth below 总结为：容器需要读写底层（镜像的层级）数据时，会将文件或目录拷贝到容器`可写层`进行读写，而非全部拷贝（显然镜像的某些层级包含的文件在容器整个运行周期中可能并不会用到）。 测试写时拷贝 #构建镜像F #Dockerfile.F FROM centos COPY CentOS-7-x86_64-Minimal-1804.iso / docker build -t imagef -f Dockerfile.F . 分析：运行时并未读写其他层级数据，容器大小忽略不计 官方对Copy-on-write优势的说明: Not only does copy-on-write save space, but it also reduces start-up time. When you start a container (or multiple containers from the same image), Docker only needs to create the thin writable container layer 总结：节省存储空间、容器启动快 overlay2 overlay2可理解为连接container (upperdir)与image (lowerdir)的纽带，类比显卡驱动等 容器读取文件 官方列举三个场景 The file does not exist in the container layer: If a container opens a file for read access and the file does not already exist in the container (upperdir) it is read from the image (lowerdir). This incurs very little performance overhead. #如果容器层不存在该文件，将从镜像层读取，官方表示性能损耗较小。 The file only exists in the container layer: If a container opens a file for read access and the file exists in the container (upperdir) and not in the image (lowerdir), it is read directly from the container. #如果容器层存在该文件，将直接从容器层读取 The file exists in both the container layer and the image layer: If a container opens a file for read access and the file exists in the image layer and the container layer, the file’s version in the container layer is read. Files in the container layer (upperdir) obscure files with the same name in the image layer (lowerdir). #如果容器层、镜像层均存在该文件，优先读取容器层文件的版本 容器修改文件|目录 官方列举几个场景： Writing to a file for the first time: The first time a container writes to an existing file, that file does not exist in the container (upperdir). The overlay/overlay2 driver performs a copy_up operation to copy the file from the image (lowerdir) to the container (upperdir). The container then writes the changes to the new copy of the file in the container layer. #第一次修改容器层不存在的文件时，overlay驱动执行`copy_up`操作，将文件从镜像层拷贝到容器层，然后容器将\"更改\"写入容器层中文件的新副本 However, OverlayFS works at the file level rather than the block level. This means that all OverlayFS copy_up operations copy the entire file, even if the file is very large and only a small part of it is being modified. This can have a noticeable impact on container write performance. However, two things are worth noting: 1、The copy_up operation only occurs the first time a given file is written to. Subsequent writes to the same file operate against the copy of the file already copied up to the container. 2、OverlayFS only works with two layers. This means that performance should be better than AUFS, which can suffer noticeable latencies when searching for files in images with many layers. This advantage applies to both overlay and overlay2 drivers. overlayfs2 is slightly less performant than overlayfs on initial read, because it must look through more layers, but it caches the results so this is only a small penalty. #只有第一次文件拷贝为全量拷贝，以后读写操作均操作容器层上的文件副本 Deleting files and directories: 1、When a file is deleted within a container, a whiteout file is created in the container (upperdir). The version of the file in the image layer (lowerdir) is not deleted (because the lowerdir is read-only). However, the whiteout file prevents it from being available to the container. 2、When a directory is deleted within a container, an opaque directory is created within the container (upperdir). This works in the same way as a whiteout file and effectively prevents the directory from being accessed, even though it still exists in the image (lowerdir). #当删除容器内的某一文件（假如/usr/bin/telnet）或目录（/boot）时，对应镜像层的文件不会被删除（只读性），虽然镜像层依然存在该文件，但容器层已服务对其进行读写 Renaming directories: Calling rename(2) for a directory is allowed only when both the source and the destination path are on the top layer. Otherwise, it returns EXDEV error (“cross-device link not permitted”). Your application needs to be designed to handle EXDEV and fall back to a “copy and unlink” strategy. #重命名容器内目录：（不太理解） 只有当源路径和目标路径都位于顶层时，才允许为目录调用rename(2)。 否则，它将返回EXDEV错误(“不允许跨设备链接”)。您的应用程序需要设计成能够处理EXDEV并返回到“复制和断开链接”策略。 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/thin/thin.html":{"url":"2.容器/运行时/docker/thin/thin.html","title":"thin","keywords":"","body":"镜像体积小的优势：传输快、加载快、 1、根据场景选取基础镜像 jdk -> 选取openjdk镜像作为基础镜像 非不是centos ubuntu等操作系统镜像 2、利用multistage-build Use multistage builds. For instance, you can use the maven image to build your Java application, then reset to the tomcat image and copy the Java artifacts into the correct location to deploy your app, all in the same Dockerfile. This means that your final image doesn’t include all of the libraries and dependencies pulled in by the build, but only the artifacts and the environment needed to run them. #以下整个流程在一个Dockerfile内实现 1、选取maven基础镜像进行打包JAVA程序 2、拷贝Jar至tomcat基础镜像内（spring boot的话直接jdk基础镜像） 3、发布 multistage-build样例 FROM golang:1.7.3 WORKDIR /go/src/github.com/alexellis/href-counter/ RUN go get -d -v golang.org/x/net/html COPY app.go . RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=0 /go/src/github.com/alexellis/href-counter/app . CMD [\"./app\"] 显然，只是把多个步骤合并到同一Dockfile呢，降低构造镜像成本。 3、减少层级 场景一 合并命令，每一行命令均产生一个层级 #合并前 RUN apt-get -y update RUN apt-get install -y python #合并后 RUN apt-get -y update && apt-get install -y python 场景二 #制作适合自己的基础镜像（适用于多个application场景，并且基础层级相同较多的） Docker only needs to load the common layers once, and they are cached. This means that your derivative images use memory on the Docker host more efficiently and load more quickly 场景 由于测试镜像的话，可能需要安装一些测试软件等，保证两者的区别处于镜像最高层级（还是为了充分复用相同层级） To keep your production image lean but allow for debugging, consider using the production image as the base image for the debug image. Additional testing or debugging tooling can be added on top of the production image 场景四 制作镜像时，打上tag标签 When building images, always tag them with useful tags which codify version information, intended destination (prod or test, for instance), stability, or other information that is useful when deploying the application in different environments. Do not rely on the automatically-created latest tag 4、程序数据持久化问题 #避免将数据写入容器内部，这样不仅会增加容器体积，并且I/O读写效率比挂载模式要低 Avoid storing application data in your container’s writable layer using storage drivers. This increases the size of your container and is less efficient from an I/O perspective than using volumes or bind mounts. Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/主机安全配置/01为docker挂载单独存储目录.html":{"url":"2.容器/运行时/docker/安全加固/主机安全配置/01为docker挂载单独存储目录.html","title":"01为docker挂载单独存储目录","keywords":"","body":"为docker挂载单独存储目录 描述 默认安装情况下，所有Docker容器及数据、元数据存储于/var/lib/docker下 审计方式 Docker依赖于/var/lib/docker作为默认数据目录，该目录存储所有相关文件，包括镜像文件。 该目录可能会被恶意的写满，导致Docker、甚至主机可能无法使用。因此，建议为Docker存储目录配置独立挂载点（最好为独立数据盘） 修复建议 docker宿主机增加数据盘/dev/sdb [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 19G 0 part ├─centos-root 253:0 0 17G 0 lvm / └─centos-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 30G 0 disk sr0 11:0 1 4.4G 0 rom 格式化数据盘 [root@localhost ~]# mkfs.ext4 /dev/sdb mke2fs 1.42.9 (28-Dec-2013) /dev/sdb is entire device, not just one partition! Proceed anyway? (y,n) y Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 1966080 inodes, 7864320 blocks 393216 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=2155872256 240 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000 Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done 配置/dev/sdb挂载点为/var/lib/docker 该步骤建议安装docker之后进行 echo \"/dev/sdb /var/lib/docker ext4 defaults 0 0\" >> /etc/fstab 重启主机测试是否生效 [root@localhost ~]# reboot [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 19G 0 part ├─centos-root 253:0 0 17G 0 lvm / └─centos-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 30G 0 disk /var/lib/docker sr0 11:0 1 4.4G 0 rom [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE harbor.wl.com/public/alpine latest d6e46aa2470d 6 months ago 5.57MB 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/主机安全配置/02容器宿主机加固.html":{"url":"2.容器/运行时/docker/安全加固/主机安全配置/02容器宿主机加固.html","title":"02容器宿主机加固","keywords":"","body":"容器宿主机加固 分析 容器在Linux主机上运行，容器宿主机可以运行一个或多个容器。 加强主机以缓解主机安全配置错误是非常重要的 审计方式 确保遵守主机的安全规范。询问系统管理员当前主机系统符合哪个安全标准。确保主机系统实际符合主机制定的安全规范 修复建议 参考Linux主机安全加固规范，建议使用easyctl 对主机进行一键加固 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/主机安全配置/03更新Docker到最新版本.html":{"url":"2.容器/运行时/docker/安全加固/主机安全配置/03更新Docker到最新版本.html","title":"03更新Docker到最新版本","keywords":"","body":"描述 Docker软件频繁发布更新，旧版本可能存在安全漏洞 审计 查看release 与本地版本比较 $ docker version 隐患分析 不要盲目升级docker版本，评估升级是否会对现有系统产生影响，充分测试其兼容性（如与k8s kubeadm兼容性） 修复建议 #安装一些必要的系统工具 $ yum -y install yum-utils device-mapper-persistent-data lvm2 #添加软件源信息 $ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #更新 yum 缓存 $ yum makecache fast #安装docker-ce $ yum -y install docker-ce # 或更新 $ yum -y update docker-ce 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/主机安全配置/04只有受信任的用户才能控制Docker守护进程.html":{"url":"2.容器/运行时/docker/安全加固/主机安全配置/04只有受信任的用户才能控制Docker守护进程.html","title":"04只有受信任的用户才能控制Docker守护进程","keywords":"","body":"只有受信任的用户才能控制Docker守护进程 描述 Docker守护进程需要root权限。对于添加到Docker组的用户， 为其提供了完整的root访问权限。 隐患分析 Docker允许在宿主机和访客容器之间共享目录，而不会限制容器的访问权限。 这意味着可以启动容器并将主机上的/目录映射到容器。 容器将能够不受任何限制地更改您的主机文件系统。 简而言之，这意味着您只需作为Docker组的成员即可获得较高的权限，然后在主机上启动具有映射/目录的容器。 审计方式 [root@localhost ~]# yum install glibc-common -y -q [root@localhost ~]# getent group docker docker:x:994: 结果判定 查看审计步骤中的返回值是否含有非信任用户 修复建议 从docker组中删除任何不受信任的用户。另外，请勿在主机上创建敏感目录到容器卷的映射 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护程序文件配置/01设置docker文件的所有权为root.html":{"url":"2.容器/运行时/docker/安全加固/守护程序文件配置/01设置docker文件的所有权为root.html","title":"01设置docker文件的所有权为root","keywords":"","body":"设置docker文件的所有权为root:root 隐患分析 docker.service文件包含可能会改变Docker守护进程行为的敏感参数。 因此，它应该由root拥有和归属，以保持文件的完整性。 审计方式 ```shell script $ systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 ls -l 返回值应为 -rw-r--r-- 1 root root 1157 Apr 26 08:04 /etc/systemd/system/docker.service ### 修复建议 若所属用户非`root:root`，修改授权 ```shell script $ systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 chown root:root 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护程序文件配置/02设置docker.service文件权限为644或更多限制性.html":{"url":"2.容器/运行时/docker/安全加固/守护程序文件配置/02设置docker.service文件权限为644或更多限制性.html","title":"02设置docker.service文件权限为644或更多限制性","keywords":"","body":"设置docker.service文件权限为644或更多限制性 描述 验证docker.service文件权限是否正确设置为644或更多限制 隐患分析 docker.service文件包含可能会改变Docker守护进程行为的敏感参数。 因此，它应该由root拥有和归属，以保持文件的完整性。 审计方式 ```shell script [root@localhost ~]# systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 stat -c %a 644 ### 修复建议 若权限非`644`，修改授权 ```shell script $ systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 chmod 644 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护程序文件配置/03设置docker.socket文件的所有权为root.html":{"url":"2.容器/运行时/docker/安全加固/守护程序文件配置/03设置docker.socket文件的所有权为root.html","title":"03设置docker.socket文件的所有权为root","keywords":"","body":"设置docker.socket文件的所有权为root:root 描述 验证docker.socket文件所有权和组所有权是否正确设置为root 隐患分析 docker.socket文件包含可能会改变Docker远程API行为的敏感参数。 因此，它应该拥有root权限，以保持文件的完整性。 审计方式 ```shell script systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 ls -l 返回值应为 -rw-r--r-- 1 root root 197 Mar 10 2020 /usr/lib/systemd/system/docker.socket ### 修复建议 若所属用户非`root:root`，修改授权 ```shell script $ systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 chown root:root 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护程序文件配置/04设置docker.socket文件权限为644或更多限制性.html":{"url":"2.容器/运行时/docker/安全加固/守护程序文件配置/04设置docker.socket文件权限为644或更多限制性.html","title":"04设置docker.socket文件权限为644或更多限制性","keywords":"","body":"设置docker.socket文件权限为644或更多限制性 描述 验证docker.socket文件权限是否正确设置为644或更多限制 隐患分析 docker.socket文件包含可能会改变Docker远程API行为的敏感参数。 因此，它应该拥有root权限，以保持文件的完整性。 审计方式 ```shell script [root@localhost ~]# systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 stat -c %a 644 ### 修复建议 若权限非`644`，修改授权 ```shell script $ systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 chmod 644 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护程序文件配置/05设置docker配置目录权限.html":{"url":"2.容器/运行时/docker/安全加固/守护程序文件配置/05设置docker配置目录权限.html","title":"05设置docker配置目录权限","keywords":"","body":"设置/etc/docker目录所有权为root:root 描述 验证/etc/docker目录所有权和组所有权是否正确设置为root:root 隐患分析 除了各种敏感文件之外，/etc/docker目录还包含证书和密钥。 因此，它应该由root:root拥有和归组来维护目录的完整性。 审计方式 ```shell script [root@localhost ~]# stat -c %U:%G /etc/docker root:root ### 修复建议 若所属用户非`root:root`，修改授权 ```shell script $ chown root:root /etc/docker 设置/etc/docker目录权限为755或更多限制性 描述 验证/etc/docker目录权限是否正确设置为755 隐患分析 除了各种敏感文件之外，/etc/docker目录还包含证书和密钥。 因此，它应该由root:root拥有和归组来维护目录的完整性。 审计方式 ```shell script [root@localhost ~]# stat -c %a /etc/docker 755 ### 修复建议 若所属用户非`root:root`，修改授权 ```shell script $ chmod 755 /etc/docker 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护程序文件配置/06仓库证书文件加固.html":{"url":"2.容器/运行时/docker/安全加固/守护程序文件配置/06仓库证书文件加固.html","title":"06仓库证书文件加固","keywords":"","body":"设置仓库证书文件权限为444或更多限制性 描述 验证所有仓库证书文件（通常位于/etc/docker/certs.d/ 目录下）所有权限是否正确设置为444 隐患分析 /etc/docker/certs.d/目录包含Docker镜像仓库证书。 这些证书文件必须具有444权限，以维护证书的完整性。 审计方式 ```shell script [root@localhost ~]# stat -c %a /etc/docker/certs.d/* 755 ### 修复建议 若权限非`444`，修改授权 ```shell script $ chmod 444 /etc/docker/certs.d/* 设置TLS CA证书文件所有权为root:root 描述 验证TLS CA证书文件均由root拥有并归组所有 隐患分析 TLS CA证书文件应受到保护，不受任何篡改。它用于指定的CA证书验证。 因此，它必须由root拥有，以维护CA证书的完整性。 审计方式 ```shell script [root@localhost ~]# ls /etc/docker/certs.d// |xargs -n1 stat -c %U:%G root:root root:root root:root ### 修复建议 若所属用户非`root:root`，修改授权 ```shell script $ chown root:root /etc/docker/certs.d/*/* 设置TLS CA证书文件权限为444或更多限制性 描述 验证所有仓库证书文件（通常位于/etc/docker/certs.d/ 目录下）所有权限是否正确设置为444 隐患分析 TLS CA证书文件应受到保护，不受任何篡改。它用于指定的CA证书验证。 这些证书文件必须具有444权限，以维护证书的完整性。 审计方式 ```shell script [root@localhost ~]# stat -c %a /etc/docker/certs.d// 644 644 644 ###- 修复建议 若权限非`444`，修改授权 ```shell script $ chmod 444 /etc/docker/certs.d/*/* 设置docker服务器证书文件所有权为root:root 描述 验证Docker服务器证书文件（与--tlscert参数一起传递的文件）是否由root和其组拥有 隐患分析 Docker服务器证书文件应受到保护，不受任何篡改。它用于验证Docker服务器。 因此，它必须由root拥有以维护证书的完整性。 审计方式 注意: /root/docker替换为docker服务端实际证书存放目录 ```shell script [root@localhost ~]# ls -l /root/docker total 44 -rw-r--r-- 1 root root 3326 Apr 26 02:55 ca-key.pem -rw-r--r-- 1 root root 1980 Apr 26 02:56 ca.pem -rw-r--r-- 1 root root 17 Apr 26 02:57 ca.srl -rw-r--r-- 1 root root 1801 Apr 26 02:57 cert.pem -rw-r--r-- 1 root root 1582 Apr 26 02:57 client.csr -rw-r--r-- 1 root root 30 Apr 26 02:57 extfile-client.cnf -rw-r--r-- 1 root root 86 Apr 26 02:56 extfile.cnf -rw-r--r-- 1 root root 3243 Apr 26 02:57 key.pem -rw-r--r-- 1 root root 1862 Apr 26 02:56 server-cert.pem -rw-r--r-- 1 root root 1594 Apr 26 02:56 server.csr -rw-r--r-- 1 root root 3243 Apr 26 02:56 server-key.pem ### 修复建议 若所属用户非`root:root`，修改授权 ```shell script $ chown root:root /root/docker/* 设置Docker服务器证书文件权限为400或更多限制 描述 验证Docker服务器证书文件（与--tlscert参数一起传递的文件）权限是否为400 隐患分析 Docker服务器证书文件应受到保护，不受任何篡改。它用于验证Docker服务器。 因此，它必须由root拥有以维护证书的完整性。 审计方式 注意: /root/docker替换为docker服务端实际证书存放目录 ```shell script [root@localhost ~]# ls -l /root/docker total 44 -rw-r--r-- 1 root root 3326 Apr 26 02:55 ca-key.pem -rw-r--r-- 1 root root 1980 Apr 26 02:56 ca.pem -rw-r--r-- 1 root root 17 Apr 26 02:57 ca.srl -rw-r--r-- 1 root root 1801 Apr 26 02:57 cert.pem -rw-r--r-- 1 root root 1582 Apr 26 02:57 client.csr -rw-r--r-- 1 root root 30 Apr 26 02:57 extfile-client.cnf -rw-r--r-- 1 root root 86 Apr 26 02:56 extfile.cnf -rw-r--r-- 1 root root 3243 Apr 26 02:57 key.pem -rw-r--r-- 1 root root 1862 Apr 26 02:56 server-cert.pem -rw-r--r-- 1 root root 1594 Apr 26 02:56 server.csr -rw-r--r-- 1 root root 3243 Apr 26 02:56 server-key.pem ### 修复建议 若权限非`400`，修改授权 ```shell script $ chmod 400 /root/docker/* 设置仓库证书文件所有权为root:root 描述 验证所有仓库证书文件（通常位于/etc/docker/certs.d/ 目录下）均由root拥有并归组所有 隐患分析 /etc/docker/certs.d/目录包含Docker镜像仓库证书。 这些证书文件必须由root和其组拥有，以维护证书的完整性 审计方式 ```shell script [root@localhost ~]# stat -c %U:%G /etc/docker/certs.d/* root:root ### 修复建议 若所属用户非`root:root`，修改授权 ```shell script $ chown root:root /etc/docker/certs.d/* 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护程序文件配置/07socket监听文件加固.html":{"url":"2.容器/运行时/docker/安全加固/守护程序文件配置/07socket监听文件加固.html","title":"07socket监听文件加固","keywords":"","body":"设置docker.sock文件所有权为root:docker 描述 验证docker.sock文件由root拥有，而用户组为docker。 隐患分析 Docker守护进程以root用户身份运行。 因此，默认的Unix套接字必须由root拥有。 如果任何其他用户或进程拥有此套接字，那么该非特权用户或进程可能与Docker守护进程交互。 另外，这样的非特权用户或进程可能与容器交互，这样非常不安全。 另外，Docker安装程序会创建一个名为docker的用户组。 可以将用户添加到该组，然后这些用户将能够读写默认的Docker Unix套接字。 docker组成员由系统管理员严格控制。 如果任何其他组拥有此套接字，那么该组的成员可能会与Docker守护进程交互。。 因此，默认的Docker Unix套接字文件必须由docker组拥有权限，以维护套接字文件的完整性 审计 ```shell script [root@localhost ~]# stat -c %U:%G /var/run/docker.sock root:docker ### 修复建议 若所属用户非`root:docker`，修改授权 ```shell script $ chown root:docker /var/run/docker.sock 设置docker.sock文件权限为660或更多限制性 描述 验证docker套接字文件是否具有660或更多限制的权限 隐患分析 只有root和docker组的成员允许读取和写入默认的Docker Unix套接字。 因此，Docker套接字文件必须具有660或更多限制的权限 审计 ```shell script [root@localhost ~]# stat -c %a /var/run/docker.sock 660 ### 修复建议 若权限非`660`，修改授权 ```shell script $ chmod 660 /var/run/docker.sock 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护程序文件配置/08配置文件加固.html":{"url":"2.容器/运行时/docker/安全加固/守护程序文件配置/08配置文件加固.html","title":"08配置文件加固","keywords":"","body":"设置docker.json文件所有权为root:root 描述 验证docker.json文件由root归属。 隐患分析 docker.json文件包含可能会改变Docker守护程序行为的敏感参数。 因此，它应该由root拥有，以维护文件的完整性 审计 ```shell script [root@localhost ~]# stat -c %U:%G /etc/docker/daemon.json root:root ### 修复建议 若所属用户非`root:root`，修改授权 ```shell script $ chown root:root /etc/docker/daemon.json 设置docker.json文件权限为644或更多限制性 描述 验证docker.json文件权限是否正确设置为644或更多限制 隐患分析 docker.json文件包含可能会改变Docker守护程序行为的敏感参数。 因此，它应该由root拥有，以维护文件的完整性 审计方式 ```shell script [root@localhost ~]# stat -c %a /etc/docker/daemon.json 644 ### 修复建议 若权限非`644`，修改授权 ```shell script $ chmod 644 /etc/docker/daemon.json 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/01不使用不安全的镜像仓库.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/01不使用不安全的镜像仓库.html","title":"01不使用不安全的镜像仓库","keywords":"","body":"不使用不安全的镜像仓库 描述 Docker在默认情况下，私有仓库被认为是安全的 隐患分析 镜像仓库建议使用TLS。 在/etc/docker/certs.d//目录下，将镜像仓库的CA证书副本放置在Docker主机上。 不安全的镜像仓库是没有有效的镜像仓库证书或不使用TLS的镜像仓库。不应该在生产环境中使用任何不安全的镜像仓库。 不安全的镜像仓库中的镜像可能会被篡改，从而导致生产系统可能受到损害。 此外，如果镜像仓库被标记为不安全，则docker pull，docker push和docker push命令并不能发现， 那样用户可能无限期地使用不安全的镜像仓库而不会发现。 审计方式 [root@localhost ~]# cat /etc/docker/daemon.json |grep insecure-registries \"insecure-registries\":[\"gcr.azk8s.cn\",\"dockerhub.azk8s.cn\",\"quay.azk8s.cn\",\"5twf62k1.mirror.aliyuncs.com\",\"registry.docker-cn.com\",\"registry-1.docker.io\"], 修复建议 使用ssl签名的镜像仓库（如配置ssl证书的harbor） 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/02不使用aufs存储驱动程序.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/02不使用aufs存储驱动程序.html","title":"02不使用aufs存储驱动程序","keywords":"","body":"不使用aufs存储驱动程序 描述 aufs存储驱动程序是较旧的存储驱动程序。 它基于Linux内核补丁集，不太可能合并到主版本Linux内核中。 aufs驱动会导致一些严重的内核崩溃。aufs在Docker中只是保留了历史遗留支持,现在主要使用overlay2和devicemapper。 而且最重要的是，在许多使用最新Linux内核的发行版中，aufs不再被支持 审计方式 [root@node105 ~]# docker info |grep \"Storage Driver:\" Storage Driver: overlay2 修复建议 默认安装情况下存储驱动为overlay2，避免使用aufs作为存储驱动 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/03Docker守护进程配置TLS身份认证.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/03Docker守护进程配置TLS身份认证.html","title":"03Docker守护进程配置TLS身份认证","keywords":"","body":"Docker守护进程配置TLS身份认证 描述 可以让Docker守护进程监听特定的IP和端口以及除默认Unix套接字以外的任何其他Unix套接字。 配置TLS身份验证以限制通过IP和端口访问Docker守护进程。 隐患分析 默认情况下，Docker守护程序绑定到非联网的Unix套接字，并以root权限运行。 如果将默认的Docker守护程序更改为绑定到TCP端口或任何其他Unix套接字，那么任何有权访问该端口或套接字的人都可以完全访问Docker守护程序，进而可以访问主机系统。 因此，不应该将Docker守护程序绑定到另一个IP/端口或Unix套接字。 如果必须通过网络套接字暴露Docker守护程序，请为守护程序配置TLS身份验证 审计方法 [root@localhost ~]# systemctl status docker|grep /usr/bin/dockerd ├─1061 /usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix://var/run/docker.sock 修复建议 生产环境下避免开启tcp监听，若避免不了，执行以下操作。 生成CA私钥和公共密钥 $ mkdir -p /root/docker $ cd /root/docker $ openssl genrsa -aes256 -out ca-key.pem 4096 $ openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem 创建一个服务端密钥和证书签名请求(CSR) 192.168.235.128为当前主机IP地址 $ openssl genrsa -out server-key.pem 4096 $ openssl req -subj \"/CN=192.168.235.128\" -sha256 -new -key server-key.pem -out server.csr 用CA来签署公共密钥 $ echo subjectAltName = DNS:192.168.235.128,IP:192.168.235.128 >> extfile.cnf $ echo extendedKeyUsage = serverAuth >> extfile.cnf 生成key $ openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \\ -CAcreateserial -out server-cert.pem -extfile extfile.cnf 创建客户端密钥和证书签名请求 $ openssl genrsa -out key.pem 4096 $ openssl req -subj '/CN=client' -new -key key.pem -out client.csr 修改extfile.cnf $ echo extendedKeyUsage = clientAuth > extfile-client.cnf 生成签名私钥 $ openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \\ -CAcreateserial -out cert.pem -extfile extfile-client.cnf 将Docker服务停止，然后修改docker服务文件 停服务 $ systemctl stop docker 编辑配置文件 $ vi /etc/systemd/system/docker.service 替换ExecStart=/usr/bin/dockerd为以下 ExecStart=/usr/bin/dockerd --tlsverify --tlscacert=/root/docker/ca.pem --tlscert=/root/docker/server-cert.pem --tlskey=/root/docker/server-key.pem -H unix:///var/run/docker.sock -H tcp://192.168.235.128:2375 重启 $ systemctl daemon-reload $ systemctl start docker 测试tls $ docker --tlsverify --tlscacert=/root/docker/ca.pem --tlscert=/root/docker/cert.pem --tlskey=/root/docker/key.pem -H=192.168.235.128:2375 version 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/04配置合适的ulimit.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/04配置合适的ulimit.html","title":"04配置合适的ulimit","keywords":"","body":"配置合适的ulimit 描述 什么是ulimit ulimit主要是用来限制进程对资源的使用情况的，它支持各种类型的限制，常用的有： 内核文件的大小限制 进程数据块的大小限制 Shell进程创建文件大小限制 可加锁内存大小限制 常驻内存集的大小限制 打开文件句柄数限制 分配堆栈的最大大小限制 CPU占用时间限制用户最大可用的进程数限制 Shell进程所能使用的最大虚拟内存限制 隐患分析 ulimit提供对shell可用资源的控制。设置系统资源控制可以防止资源耗尽带来的问题，如fork炸弹。 有时候合法的用户和进程也可能过度使用系统资源，导致系统资源耗尽。 为Docker守护程序设置默认ulimit将强制执行所有容器的ulimit。 不需要单独为每个容器设置ulimit。 但默认的ulimit可能在容器运行时被覆盖。 因此，要控制系统资源，需要自定义默认的ulimit 审计 确保含有--default-ulimit参数 [root@localhost ~]# ps -ef|grep dockerd root 65353 1 0 03:02 ? 00:00:00 /usr/bin/dockerd --tlsverify --tlscacert=/root/docker/ca.pem --tlscert=/root/docker/server-cert.pem --tlskey=/root/docker/server-key.pem -H unix:///var/run/docker.sock -H tcp://192.168.235.128:2375 修复建议 调整参数LimitNOFILE、LimitNPROC $ sed -i \"s#LimitNOFILE=infinity#LimitNOFILE=20480:40960#g\" /etc/systemd/system/docker.service $ sed -i \"s#LimitNPROC=infinity#LimitNPROC=1024:2048#g\" /etc/systemd/system/docker.service 重启 $ systemctl daemon-reload $ systemctl restart docker 启动一个容器测试 [root@localhost ~]# docker run -idt --name ddd harbor.wl.com/public/alpine sh 15eebdabbb8bd59366348ae95a89d79100370b9c9381b070fdfbb0119b516400 查看容器PID [root@localhost ~]# ps -ef|grep 15eebdabbb8bd59366348ae95a89d79100370b9c9381b070fdfbb0119b516400|grep -v grep|awk '{print $2}' 80060 查看limit [root@localhost ~]# cat /proc/80060/limits Limit Soft Limit Hard Limit Units Max cpu time unlimited unlimited seconds Max file size unlimited unlimited bytes Max data size unlimited unlimited bytes Max stack size 8388608 unlimited bytes Max core file size unlimited unlimited bytes Max resident set unlimited unlimited bytes Max processes 1024 2048 processes Max open files 20480 40960 files Max locked memory 65536 65536 bytes Max address space unlimited unlimited bytes Max file locks unlimited unlimited locks Max pending signals 3795 3795 signals Max msgqueue size 819200 819200 bytes Max nice priority 0 0 Max realtime priority 0 0 Max realtime timeout unlimited unlimited us 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/05启用用户命名空间.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/05启用用户命名空间.html","title":"05启用用户命名空间","keywords":"","body":"启用用户命名空间 描述 在Docker守护程序中启用用户命名空间支持，可对用户进行重新映射。该建议对镜像中没有指定用户是有帮助的。如果在容器镜像中已经 定义了非root运行，可跳过此建议。 隐患分析 Docker守护程序中对Linux内核用户命名空间支持为Docker主机系统提供了额外的安全性。 它允许容器具有独特的用户和组ID，这些用户和组ID在主机系统所使用的传统用户和组范围之外。 例如，root用户希望有容器内的管理权限，可映射到主机系统上的非root的UID上 审计 如果容器进程以root身份运行，则不符合安全要求 [root@localhost ~]# ps -ef|grep 15eebdabbb8b root 80060 73608 0 04:03 ? 00:00:00 containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/15eebdabbb8bd59366348ae95a89d79100370b9c9381b070fdfbb0119b516400 -address /var/run/docker/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc -systemd-cgroup root 111259 1482 0 07:08 pts/0 00:00:00 grep --color=auto 15eebdabbb8b 修复建议 修改系统参数 $ sed -i \"/user.max_user_namespaces/d\" /etc/sysctl.conf $ echo \"user.max_user_namespaces=15000\" >> /etc/sysctl.conf $ sysctl -p 编辑配置文件 $ vi /etc/systemd/system/docker.service ExecStart=/usr/bin/dockerd添加参数--userns-remap=default 重载服务 $ systemctl daemon-reload $ systemctl restart docker 启动一个容器 [root@localhost ~]# docker run -idt --name ccc alpine 查看容器内进程用户 [root@localhost ~]# ps -p $(docker inspect --format='{{.State.Pid}}' $(docker ps |grep ccc|awk '{print $1}')) -o pid,user PID USER 2535 100000 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/06使用默认cgroup.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/06使用默认cgroup.html","title":"06使用默认cgroup","keywords":"","body":"使用默认cgroup 描述 查看--cgroup-parent选项允许设置用于所有容器的默认cgroup parent。 如果没有特定用例,则该设置应保留默认值。 隐患分析 系统管理员可定义容器应运行的cgroup。 若系统管理员没有明确定义cgroup，容器也会在docker cgroup下运行。 应该监测和确认使用情况。通过加到与默认不同的cgroup，导致不合理地共享资源，从而可能会主机资源耗尽 审计方式 shell script $ ps -ef|grep dockerd 确保--cgroup-parent参数未设置或设置为适当的非默认cgroup 修复建议 如无特殊需求，默认值即可 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/07设置容器的默认空间大小.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/07设置容器的默认空间大小.html","title":"07设置容器的默认空间大小","keywords":"","body":"设置容器的默认空间大小 描述 在某些情况下，可能需要大于10G（容器默认存储大小）的容器空间。需要仔细选择空间的大小 隐患分析 守护进程重启时可以增加容器空间的大小。用户可以通过设置默认容器空间值来进行扩大，但不允许缩小。 设立该值的时候需要谨慎，防止设置不当带来空间耗尽的情况 审计方式 shell script $ ps -ef|grep dockerd 执行上述命令，它不应显示任何--storage-opt dm.basesize参数 修复建议 如无特殊需求，默认值即可 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/08启用docker客户端命令的授权.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/08启用docker客户端命令的授权.html","title":"08启用docker客户端命令的授权","keywords":"","body":"启用docker客户端命令的授权 描述 使用本机Docker授权插件或第三方授权机制与Docker守护程序来管理对Docker客户端命令的访问。 隐患分析 Docker默认是没有对客户端命令进行授权管理的功能。 任何有权访问Docker守护程序的用户都可以运行任何Docker客户端命令。 对于使用Docker远程API来调用守护进程的调用者也是如此。 如果需要细粒度的访问控制，可以使用授权插件并将其添加到Docker守护程序配置中。 使用授权插件，Docker管理员可以配置更细粒度访问策略来管理对Docker守护进程的访问。 Docker的第三方集成可以实现他们自己的授权模型，以要求Docker的本地授权插件 （即Kubernetes，Cloud Foundry，Openshift）之外的Docker守护进程的授权。 审计方式 shell script $ ps -ef|grep dockerd 或 $ cat /etc/docker/daemon.json|grep userland-proxy 如果使用Docker本地授权，可使用--authorization-plugin参数加载授权插件。 修复建议 如无特殊需求，默认值即可 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/09配置集中和远程日志记录.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/09配置集中和远程日志记录.html","title":"09配置集中和远程日志记录","keywords":"","body":"配置集中和远程日志记录 描述 Docker现在支持各种日志驱动程序。存储日志的最佳方式是支持集中式和远程日志记录 审计方式 运行docker info并确保日志记录驱动程序属性被设置为适当的。 ```shell script [root@localhost ~]# docker info --format '{{.LoggingDriver}}' json-file ### 修复建议 > 配置`json-file`驱动 ```shell script [root@localhost ~]# cat /etc/docker/daemon.json { \"log-driver\":\"json-file\", \"log-opts\":{ \"max-size\":\"50m\", \"max-file\":\"3\" } } 重启 shell script $ systemctl daemon-reload $ systemctl restart docker 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/10禁用旧仓库版本（v1）上的操作.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/10禁用旧仓库版本（v1）上的操作.html","title":"10禁用旧仓库版本（v1）上的操作","keywords":"","body":"禁用旧仓库版本（v1）上的操作 描述 最新的Docker镜像仓库是v2。遗留镜像仓库版本v1上的所有操作都应受到限制 隐患分析 Docker镜像仓库v2在v1中引入了许多性能和安全性改进。 它支持容器镜像来源验证和其他安全功能。因此，对Docker v1仓库的操作应该受到限制 审计方式 ```shell script $ ps -ef|grep dockerd 上面的命令应该列出`--disable-legacy-registry`作为传递给`Docker`守护进程的选项。 ### 修复建议 **注意：**`17.12+`版本已移除，无需配置 > 编辑配置文件 ```shell script $ vi /etc/systemd/system/docker.service ExecStart=/usr/bin/dockerd添加参数--userns-remap=default 重载服务 shell script $ systemctl daemon-reload $ systemctl restart docker 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/11启用实时恢复.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/11启用实时恢复.html","title":"11启用实时恢复","keywords":"","body":"启用实时恢复 描述 live-restore参数可以支持无守护程序的容器运行。 它确保Docker在关闭或恢复时不会停止容器，并在重新启动后重新连接到容器。 隐患分析 可用性作为安全一个重要的属性。 在Docker守护进程中设置--live-restore标志可确保当Docker守护进程不可用时容器执行不会中断。 这也意味着当更新和修复Docker守护进程而不会导致容器停止工作。 审计方式 ```shell script [root@localhost ~]# docker info --format '{{.LiveRestoreEnabled}}' false ### 修复建议 > 编辑文件 ```shell script $ mkdir -p /etc/docker/ $ vi /etc/docker/daemon.json 添加如下内容 \"live-restore\": true 重载服务 shell script $ systemctl daemon-reload $ systemctl restart docker 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/12禁用userland代理.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/12禁用userland代理.html","title":"12禁用userland代理","keywords":"","body":"禁用userland代理 描述 当容器端口需要被映射时，Docker守护进程都会启动用于端口转发的userland-proxy方式。如果使用了DNAT方式，该功能可以被禁用 隐患分析 Docker引擎提供了两种机制将主机端口转发到容器,DNAT和userland-proxy。 在大多数情况下，DNAT模式是首选，因为它提高了性能，并使用本地Linux iptables功能而需要附加组件。 如果DNAT可用，则应在启动时禁用userland-proxy以减少安全风险。 审计方法 ```shell script $ ps -ef|grep dockerd 或 $ cat /etc/docker/daemon.json|grep userland-proxy 确保`userland-proxy`配置为`false` ### 修复建议 > 编辑文件 ```shell script $ mkdir -p /etc/docker/ $ vi /etc/docker/daemon.json 添加如下内容 \"userland-proxy\": false, 重载服务 shell script $ systemctl daemon-reload $ systemctl restart docker 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/13应用守护进程范围的自定义seccomp配置文件.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/13应用守护进程范围的自定义seccomp配置文件.html","title":"13应用守护进程范围的自定义seccomp配置文件","keywords":"","body":"应用守护进程范围的自定义seccomp配置文件 描述 如果需要，您可以选择在守护进程级别自定义seccomp配置文件，并覆盖Docker的默认seccomp配置文件 隐患分析 大量系统调用暴露于每个用户级进程，其中许多系统调用在整个生命周期中都未被使用。 大多数应用程序不需要所有的系统调用，因此可以通过减少可用的系统调用来增加安全性。 可自定义seccomp配置文件，而不是使用Docker的默认seccomp配置文件。 如果Docker的默认配置文件够用的话，则可以选择忽略此建议 审计 shell script [root@localhost ~]# docker info --format '{{.SecurityOptions}}' 修复建议 错误配置的seccomp配置文件可能会中断的容器运行。Docker默认的策略兼容性很好，可以解决一些基本的安全问题。 所以，在重写默认值 时，你应该非常小心 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/14生产环境中关闭实验性功能.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/14生产环境中关闭实验性功能.html","title":"14生产环境中关闭实验性功能","keywords":"","body":"生产环境中关闭实验性功能 描述 避免生产环境中的实验性功-Experimental 隐患分析 Docker实验功能现在是一个运行时Docker守护进程标志, 其作为运行时标志传递给Docker守护进程，激活实验性功能。 实验性功能现在虽然比较稳定，但是一些功能可能没有大规模经使用，并不能保证API的稳定性，所以不建议在生产环境中使用 审计方法 shell script [root@localhost ~]# docker version --format '{{.Server.Experimental}}' false 修复建议 不要将--Experimental作为运行时参数传递给Docker守护进程 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/守护进程安全配置/15限制容器获取新的权限.html":{"url":"2.容器/运行时/docker/安全加固/守护进程安全配置/15限制容器获取新的权限.html","title":"15限制容器获取新的权限","keywords":"","body":"限制容器获取新的权限 描述 默认情况下，限制容器通过suid或sgid位获取附加权限 隐患分析 一个进程可以在内核中设置no_new_priv。 它支持fork，clone和execve。 no_new_priv确保进程或其子进程不会通过suid或sgid位获得任何其他特权。 这样，很多危险的操作就降低安全风险。在守护程序级别进行设置可确保默认情况下，所有新容器不能获取新的权限。 审计方法 ```shell script ps -ef|grep dockerd 或 cat /etc/docker/daemon.json|grep no-new-privileges 确保`no-new-privileges`配置为`false` ### 修复建议 > 编辑文件 ```shell script $ mkdir -p /etc/docker/ $ vi /etc/docker/daemon.json 添加如下内容 \"no-new-privileges\": false 重载服务 shell script $ systemctl daemon-reload $ systemctl restart docker 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/01Dockerfile指定USER.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/01Dockerfile指定USER.html","title":"01Dockerfile指定USER","keywords":"","body":"Dockerfile指定USER 描述 为容器镜像的Dockerfile中的容器创建非root用户 隐患分析 如果可能，指定非root用户身份运行容器是个很好的做法。 虽然用户命名空间映射可用，但是如果用户在容器镜像中指定了用户，则默认情况下容器将作为该用户运行，并且不需要特定的用户命名空间重新映射。 审计方式 ```shell script [root@localhost ~]# docker ps |grep ccc|awk '{print $1}'|xargs -n1 docker inspect --format='{{.Id}}:User={{.Config.User}}' 4e53c86daf89a1bac0ed178d043663d2af162ca813ff17864ebdb964d8233459:User= 上述命令应该返回容器用户名或用户`ID`。 如果为空，则表示容器以`root`身份运行 ### 修复建议 确保容器镜像的`Dockerfile`包含以下指令：`USER ` 其中用户名或`ID`是指可以在容器基础镜像中找到的用户。 如果在容器基础镜像中没有创建特定用户，则在`USER`指令之前添加`useradd`命令以添加特定用户。 例如，在`Dockerfile`中创建用户： RUN useradd -d /home/username -m -s /bin/bash username USER username ``` 注意: 如果镜像中有容器不需要的用户，请考虑删除它们。 删除这些用户后，提交镜像，然后生成新的容器实例以供使用。 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/02使用可信的基础镜像.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/02使用可信的基础镜像.html","title":"02使用可信的基础镜像","keywords":"","body":"使用可信的基础镜像 描述 确保容器镜像是从头开始编写的，或者是基于通过安全仓库下载的另一个已建立且可信的基本镜像 隐患分析 官方存储库是由Docker社区或供应商优化的Docker镜像。 可能还存在其他不安全的公共存储库。 在从Docker和第三方获取容器镜像时，需谨慎使用。 审计方式 1.检查Docker主机以查看执行以下命令使用的Docker镜像： ```shell script $ docker images 这将列出当前可用于`Docker`主机的所有容器镜像。 访谈系统管理员并获取证据，证明镜像列表是通过安全的镜像仓库获到的，也可简单的从镜像的`TAG`名称来判断是否为可信镜像。 > 2.检查镜像信息 对于在`Docker`主机上找到的每个`Docker`镜像，检查镜像的构建方式，以验证是否来自可信来源： ```shell script $ docker history 修复建议 中间件等应用使用官方镜像 构建镜像时选用alpine、CentOS等官方镜像 从源头杜绝不安全镜像 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/03容器中不安装没有必要的软件包.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/03容器中不安装没有必要的软件包.html","title":"03容器中不安装没有必要的软件包","keywords":"","body":"容器中不安装没有必要的软件包 描述 容器往往是操作系统的最简的版本，不要安装任何不需要的软件。 隐患分析 安装不必要的软件可能会增加容器的攻击风险。因此，除了容器的真正需要的软件之外，不要安装其他多余的软件。 审计方式 1.通过执行以下命令列出所有运行的容器实例： ```shell script $ docker ps > 对于每个容器实例，执行以下或等效的命令 ```shell script $ docker exec rpm -qa rpm -qa命令可根据容器镜像系统类型进行相应变更 修复建议 中间件等应用使用官方镜像 构建镜像时选用alpine、CentOS等官方精简后的镜像 从源头杜绝安装没有必要的软件包 仅安装已经验证的软件包 描述 在将软件包安装到镜像中之前，验证软件包可靠性 隐患分析 验证软件包的可靠性对于构建安全的容器镜像至关重要。不合法的软件包可能具有恶意或者存在一些可能被利用的已知漏洞 审计方式 第 1 步：运行以下命令以获取镜像列表： ```shell script $ docker images 第 2 步：对上面列表中的每个镜像运行以下命令，并查看软件包的合法性 ```shell script $ docker history 若可以访问镜像的Dockerfile，请验证是否检查了软件包的合法性 修复建议 使用GPG密钥下载和验证您所选择的软件包或任何其他安全软件包分发机制 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/04扫描镜像漏洞并且构建包含安全补丁的镜像.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/04扫描镜像漏洞并且构建包含安全补丁的镜像.html","title":"04扫描镜像漏洞并且构建包含安全补丁的镜像","keywords":"","body":"扫描镜像漏洞并且构建包含安全补丁的镜像 描述 应该经常扫描镜像以查找漏洞。重建镜像安装最新的补丁。 隐患分析 安全补丁可以解决软件的安全问题。可以使用镜像漏洞扫描工具来查找镜像中的任何类型的漏洞，然后检查可用的补丁以减轻这些漏洞。 修补程序将系统更新到最新的代码库。此外，如果镜像漏洞扫描工具可以执行二进制级别分析，而不仅仅是版本字符串匹配，则会更好 审计方式 1.通过执行以下命令列出所有运行的容器实例 ```shell script $ docker ps --quiet > 2.对于每个容器实例，执行下面的或等效的命令来查找容器中安装的包的列表,确保安装各种受影响软件包的安全更新。 ```shell script $ docker exec rpm -qa 修复建议 定期更新基础镜像版本tag（或使用latest版本镜像，每日执行构建）及镜像内必须软件版本 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/05镜像添加HEALTHCHECK.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/05镜像添加HEALTHCHECK.html","title":"05镜像添加HEALTHCHECK","keywords":"","body":"镜像添加HEALTHCHECK 描述 在Docker容器镜像中添加HEALTHCHECK指令以对正在运行的容器执行运行状况检查。 安全出发点 安全性最重要的一个特性就是可用性。将HEALTHCHECK指令添加到容器镜像可确保Docker引擎定期检查运行的容器实例是否符合该指令， 以确保实例仍在运行。根据报告的健康状况，Docker引擎可以退出非工作容器并实例化新容器。 审计 运行以下命令，并确保Docker镜像对HEALTHCHECK指令设置 ```shell script [root@localhost ~]# docker inspect --format='{{.Config.Healthcheck}}' 8a2fb25a19f5 ``` 应当返回设置值而非nil 修复建议 按照Docker文档，并使用HEALTHCHECK指令重建容器镜像。 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/06COPY代替ADD.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/06COPY代替ADD.html","title":"06COPY代替ADD","keywords":"","body":"在Dockerfile中使用COPY而不是ADD 描述 在Dockerfile中使用COPY指令而不是ADD指令 隐患分析 COPY指令只是将文件从本地主机复制到容器文件系统。 ADD指令可能会从远程URL下载文件并执行诸如解包等操作。 因此，ADD指令增加了从URL添加恶意文件的风险 审计 步骤 1：运行以下命令获取镜像列表 ```shell script $ docker images 步骤 2：对上述列表中的每个镜像执行以下命令，并查找任何`ADD`指令： for i in docker images --quiet;do docker history $i |grep ADD > /dev/null if [ $? -eq 0 ];then echo \"imageID: $i has 'ADD' direct...\" fi done ``` 修复建议 在Dockerfile中使用COPY指令 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/07Dockerfile不声明涉密信息.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/07Dockerfile不声明涉密信息.html","title":"07Dockerfile不声明涉密信息","keywords":"","body":"Dockerfile不声明涉密信息 描述 不要在Dockerfile中存储任何涉密信息 隐患分析 通过使用Docker历史命令，可以查看各种工具和实用程序。 通常情况，镜像发布者提供Dockerfile来构建镜像。所以，Dockerfile中的涉密信息可能会被暴露并被恶意利用。 审计方式 第 1 步：运行以下命令以获取镜像列表： ```shell script $ docker images 第 2 步：对上面列表中的每个镜像运行以下命令，并查找是否有涉密信息： ```shell script $ docker history 如果有权访问镜像的Dockerfile，请确认没有涉密信息（不应该有涉密的信息，如用户账号，私钥证书等。） 修复建议 不要在Dockerfile中存储任何类型的涉密信息 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/08不使用特权容器.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/08不使用特权容器.html","title":"08不使用特权容器","keywords":"","body":"不使用特权容器 描述 使用--privileged标志将所有Linux内核功能提供给容器，从而覆盖-cap-add和-cap-drop标志。若无必须请不要使用 隐患分析 --privileged标志给容器提供所有功能,并且还提升了cgroup控制器执行的所有限制。 换句话说，容器可以做几乎主机可以做的一切。这个标志存在允许特殊用例,就像在Docker中运行Docker一样 审计方式 ```shell script [root@localhost ~]# docker ps --quiet |xargs docker inspect --format '{{.Id}}:Privileged={{.HostConfig.Privileged}}' 7121e891641679fda571e67a0e9953d263feca2508b013c70ae2546f6336b1a0:Privileged=false bb3875c107daa062f2eccb10bd48ad54954cecd7d51a5eba385335f377b7aae9:Privileged=false 7a3a2c9e524a9d44ae857abd52447f86940dd49e1947291e7985b98e3c6a309a:Privileged=false 0780c27f8eb858e172e6a7458d2b2221130e6dde0f64887d396ad5bc350a4a64:Privileged=false 确保`Privileged`为`false` ### 修复措施 不要运行带有`--privileged`标志的容器。例如，不要启动如下容器： ```shell script docker run -idt --privileged centos /bin/bash 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/09端口映射加固.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/09端口映射加固.html","title":"09端口映射加固","keywords":"","body":"特权端口禁止映射到容器内 描述 低于1024的TCP/IP端口号被认为是特权端口，由于各种安全原因，普通用户和进程不允许使用它们。 隐患分析 默认情况下，如果用户没有明确声明容器端口进行主机端口映射，Docker会自动地将容器端口映射到主机上的49153-65535中。 但是，如果用户明确声明它，Docker可以将容器端口映射到主机上的特权端口。 这是因为容器使用不限制特权端口映射的NET_BIND_SERVICE Linux内核功能来执行。 特权端口接收和发送各种敏感和特权的数据。允许Docker使用它们可能会带来严重的影响 审计方式 通过执行以下命令列出容器的所有运行实例及其端口映射 ```shell script [root@localhost ~]# docker ps --quiet |xargs docker inspect --format '{{.Id}}:Ports={{.NetworkSettings.Ports}}' 7121e891641679fda571e67a0e9953d263feca2508b013c70ae2546f6336b1a0:Ports=map[6060/tcp:[map[HostIp:0.0.0.0 HostPort:6060]] 6061/tcp:] bb3875c107daa062f2eccb10bd48ad54954cecd7d51a5eba385335f377b7aae9:Ports=map[5432/tcp:[map[HostIp:0.0.0.0 HostPort:5432]]] 7a3a2c9e524a9d44ae857abd52447f86940dd49e1947291e7985b98e3c6a309a:Ports=map[3000/tcp:[map[HostIp:0.0.0.0 HostPort:4000]]] 0780c27f8eb858e172e6a7458d2b2221130e6dde0f64887d396ad5bc350a4a64:Ports=map[3306/tcp:[map[HostIp:0.0.0.0 HostPort:3316]]] 查看列表，并确保容器端口未映射到低于`1024`的主机端口号 ### 修复建议 启动容器时，不要将容器端口映射到特权主机端口。另外，确保没有容器在`Docker`文件中特权端口映射声明 ## 只映射必要的端口 ### 描述 容器镜像的`Dockerfile`定义了在容器实例上默认要打开的端口。端口列表可能与在容器内运行的应用程序相关 ### 隐患分析 一个容器可以运行在`Dockerfile`文件中为其镜像定义的端口，也可以任意传递运行时参数以打开一个端口列表。 此外，`Dockerfile`文件可能会进行各种更改，暴露的端口列表可能与在容器内运行的应用程序不相关。 推荐做法是不要打开不需要的端口 ### 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:Ports={{.NetworkSettings.Ports}}' 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377:Ports=map[80/tcp:[map[HostIp:192.168.235.128 HostPort:18080]]] 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:Ports=map[80/tcp:[map[HostIp:0.0.0.0 HostPort:8080]]] 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:Ports=map[] 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:Ports=map[] 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:Ports=map[] cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:Ports=map[] 查看列表，并确保映射的端口是容器真正需要的端口 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/10命名空间加固.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/10命名空间加固.html","title":"10命名空间加固","keywords":"","body":"不共享主机的UTS命令空间 描述 UTS命名空间提供两个系统标识符的隔离：主机名和NIS域名。 它用于设置在该名称空间中运行进程可见的主机名和域名。 在容器中运行的进程通常不需要知道主机名和域名。因此，名称空间不应与主机共享 隐患分析 与主机共享UTS命名空间提供了容器可更改主机的主机名。这是不安全的 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all |xargs docker inspect --format '{{.Id}}:UTSMode={{.HostConfig.UTSMode}}' 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:UTSMode= 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:UTSMode= 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:UTSMode= d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:UTSMode= cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:UTSMode= 如果上述命令返回`host`，则意味着主机`UTS`名称空间与容器共享，是不符合要求的。 如果上述命令不返回任何内容，则主机的`UTS`名称空间不共享 ### 修复建议 不要使用`--uts=host`参数启动容器。例如，不要启动如下容器： ```shell script $ docker run -idt --uts=host alpine 不共享主机的进程命名空间 描述 进程ID（PID）命名空间隔离进程ID空间，这意味着不同PID命名空间中的进程可以具有相同的PID。这就是容器和主机之间的进程级隔离 隐患分析 PID名称空间提供了进程的隔离。PID命名空间删除了系统进程的视图，并允许重用包括PID的进程ID。 如果主机的PID名称空间与容器共享，它基本上允许容器内的进程查看主机上的所有进程。 这就打破了主机和容器之间进程级别隔离的优点。若访问容器最终可以知道主机系统上运行的所有进程，甚至可以从容器内杀死主机系统进程。 这可能是灾难性的。因此，不要将容器与主机的进程名称空间共享 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:PidMode={{.HostConfig.PidMode}}' 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:PidMode= 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:PidMode= 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:PidMode= d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:PidMode= cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:PidMode= 如果上述命令返回`host`，则表示主机`PID`名称空间与容器共享，存在安全风险 ### 修复建议 不要使用`--pid=host`参数启动容器。例如，不要启动一个容器，如下所示 ````shell script $ docker run -idt --pid=host centos ` 不共享主机的IPC命令空间 描述 IPC（POSIX / Sys IPC）命名空间提供命名共享内存段，信号量和消息队列的分离。因此主机上的IPC命名空间不应该与容器共享，并且应该保持独立。 隐患分析 IPC命名空间提供主机和容器之间的IPC分离。 如果主机的IPC名称空间与容器共享，它允许容器内的进程查看主机系统上的所有IPC。 这打破了主机和容器之间IPC级别隔离的好处。可通过访问容器操纵主机IPC。 这可能是灾难性的。 因此，不要将主机的IPC名称空间与容器共享 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:IpcMode={{.HostConfig.IpcMode}}' 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:IpcMode=private 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:IpcMode=private 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:IpcMode=private d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:IpcMode=private cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:IpcMode=private 如果上述命令返回`host`，则意味着主机`IPC`命名空间与容器共享。 ### 修复建议 不要使用`--ipc=host`参数启动容器。 例如，不要启动如下容器 ```shell script $ docker run -idt --ipc=host centos 说明 共享内存段用于加速进程间通信。 它通常被高性能应用程序使用。 如果这些应用程序被容器化为多个容器，则可能需要共享容器的IPC名称空间以实现高性能。 在这种情况下，您仍然应该共享容器特定的IPC命名空间而不是整个主机IPC命名空间。 可以将容器的IPC名称空间与另一个容器共享，如下所示： shell script $ docker run -idt --ipc=container:e43299eew043243284 centos 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/11Docker套接字不安装在容器内.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/11Docker套接字不安装在容器内.html","title":"11Docker套接字不安装在容器内","keywords":"","body":"Docker套接字不安装或挂载到容器内 描述 docker socket不应该安装在容器内 隐患分析 如果Docker套接字安装在容器内，它将允许在容器内运行的进程执行Docker命令，这有效地允许完全控制主机 审计方式 shell script [root@localhost ~]# docker ps --quiet | xargs docker inspect --format='{{.Id}}:Volumes={{.Mounts}}'|grep docker.sock 上述命令将返回docker.sock作为卷映射到容器的任何实例 修复建议 确保没有容器将docker.sock作为卷挂载 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/12不使用Docker的默认网桥docker0.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/12不使用Docker的默认网桥docker0.html","title":"12不使用Docker的默认网桥docker0","keywords":"","body":"不使用Docker的默认网桥docker0 描述 不要使用Docker的默认bridge docker0。 使用Docker的用户定义的网络进行容器联网 隐患分析 Docker将以桥模式创建的虚拟接口连接到名为docker0的公共桥。 这种默认网络模型易受ARP欺骗和MAC洪泛攻击的攻击，因为没有应用过滤 审计方式 运行以下命令，并验证容器是否在用户定义的网络上，而不是默认的docker0网桥 ```shell script [root@localhost ~]# docker network ls --quiet|xargs docker network inspect --format='{{.Name}}.{{.Options}}'|grep docker0 bridge.map[com.docker.network.bridge.default_bridge:true com.docker.network.bridge.enable_icc:true com.docker.network.bridge.enable_ip_masquerade:true com.docker.network.bridge.host_binding_ipv4:0.0.0.0 com.docker.network.bridge.name:docker0 com.docker.network.driver.mtu:1500] 若返回值不为空，说明使用`docker0`网桥 ### 修复建议 **使用自定义网桥** > 关于自定义网桥与默认docker0网桥的主要区别 1. 自定义网桥自动提供容器间的`DNS`解析 默认网桥通过`IP`地址实现容器间的寻址，也可通过`--link`参数实现容器`DNS`解析（容器A名称->容器A IP地址），但不推荐`--link`方式 2. 自定义网桥提供更好的隔离 如果宿主机上所有容器没有指定`--network`参数，那它们将使用默认网桥`docker0`，并可以无限制的互相通信，存在一定安全隐患。 而自定义网桥提供了的网络隔离，只有相同网络域（network）内的容器才能相互访问 > 创建自定义网桥 ```shell script docker network create nginx-net 运行测试用例 shell script [root@localhost ~]# docker run -idt --name=nginx --network=nginx-net nginx:1.14-alpine [root@localhost ~]# docker run -idt --name=box --network=nginx-net busybox:1.31.1 [root@localhost ~]# docker exec box wget nginx -S Connecting to nginx (172.18.0.2:80) HTTP/1.1 200 OK Server: nginx/1.14.2 Date: Sat, 01 May 2021 07:06:59 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Wed, 10 Apr 2019 01:08:42 GMT Connection: close ETag: \"5cad421a-264\" Accept-Ranges: bytes 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/13容器中禁止运行SSH服务.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/13容器中禁止运行SSH服务.html","title":"13容器中禁止运行SSH服务","keywords":"","body":"容器中禁止运行SSH服务 描述 SSH服务不应该在容器内运行 隐患分析 在容器内运行SSH可以增加安全管理的复杂性 难以管理SSH服务器的访问策略和安全合规性 难以管理各种容器的密钥和密码 难以管理SSH服务器的安全升级 可以在不使用SSH情况下对容器进行shell访问，避免不必要地增加安全管理的复杂性。 审计方式 ``shell script for i indocker ps --quiet`;do docker exec $i ps -el|grep sshd >/dev/null if [ $? -eq 0 ]; then echo \"container : $i run sshd...\" fi done 返回值如下，说明下面几个容器内部运行`ssh`服务 ```shell script container : 0781479bef1b run sshd... container : fea9d4d5708a run sshd... container : 38bb65479056 run sshd... container : 212fec812c01 run sshd... 修复建议 卸载容器内部ssh服务或重新构建不含有ssh的镜像，运行容器 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/构建时与运行时安全/14使用docker-exec指令加固.html":{"url":"2.容器/运行时/docker/安全加固/构建时与运行时安全/14使用docker-exec指令加固.html","title":"14使用docker-exec指令加固","keywords":"","body":"docker exec命令不能使用特权选项 描述 不要使用--privileged选项来执行docker exec 隐患分析 在docker exec中使用--privileged选项可为命令提供扩展的Linux功能。这可能会造成不安全的情况 修复建议 在docker exec命令中不要使用--privileged选项 docker exec命令不能与user选项一起使用 描述 不要使用--user选项执行docker exec 隐患分析 在docker exec中使用--user选项以该用户身份在容器内执行该命令。这可能会造成不安全的情况。 例如，假设你的容器是以tomcat用户（或任何其他非root用户）身份运行的， 那么可以使用--user=root选项以root用户身份运行命令，这是非常危险的 修复建议 在docker exec命令中不要使用--user选项 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/运行时/docker/安全加固/security.html":{"url":"2.容器/运行时/docker/安全加固/security.html","title":"security","keywords":"","body":"4.10 正确设置容器上的CPU优先级 描述 默认情况下，Docker主机上的所有容器均可共享资源。通过使用Docker主机的资源管理功能（如CPU共享），可以控制容器可能占用的主机CPU资源 隐患分析 默认情况下CPU时间在容器间平均分配。 如果需要，为了控制容器实例之间的CPU时间，可以使用CPU共享功能。 CPU共享允许将一个容器优先于另一个容器，并禁止较低优先级的容器更频繁占用CPU资源。可确保高优先级的容器更好地运行 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:CpuShares={{.HostConfig.CpuShares}}' 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377:CpuShares=0 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:CpuShares=0 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:CpuShares=0 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:CpuShares=0 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:CpuShares=0 cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:CpuShares=0 如果上述命令返回`0`或`1024`，则表示`CPU`无限制。 如果上述命令返回非`1024`值以外的非零值，则表示`CPU`已经限制。 - 修复建议 管理容器之间的`CPU`份额。为此，请使用`--cpu-shares`参数启动容器 ### 4.11 `Linux`内核功能在容器内受限 - 描述 默认情况下，`Docker`使用一组受限制的`Linux`内核功能启动容器。 这意味着可以将任何进程授予所需的功能，而不是`root`访问。 使用`Linux`内核功能，这些进程不必以`root`用户身份运行。 - 隐患分析 `Docker`支持添加和删除功能，允许使用非默认配置文件。 这可能会使`Docker`通过移除功能更加安全，或者通过增加功能来减少安全性。 因此，建议除去容器进程明确要求的所有功能。 例如，容器进程通常不需要如下所示的功能：`NET_ADMIN`、`SYS_ADMIN`、 `SYS_MODULE` - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet |xargs docker inspect --format '{{.Id}}:CapAdd={{.HostConfig.CapAdd}} CapDrop={{.HostConfig.CapDrop}}' 7121e891641679fda571e67a0e9953d263feca2508b013c70ae2546f6336b1a0:CapAdd= CapDrop= bb3875c107daa062f2eccb10bd48ad54954cecd7d51a5eba385335f377b7aae9:CapAdd= CapDrop= 7a3a2c9e524a9d44ae857abd52447f86940dd49e1947291e7985b98e3c6a309a:CapAdd= CapDrop= 0780c27f8eb858e172e6a7458d2b2221130e6dde0f64887d396ad5bc350a4a64:CapAdd= CapDrop= 验证添加和删除的Linux内核功能是否符合每个容器实例的容器进程所需的功能 修复建议 只添加必须功能特性 ```shell script docker run -dit --cap-drop=all --cap-add={\"NET_ADMIN\", \"SYS_ADMIN\"} centos /bin/bash 默认情况下，以下功能可用于容器: `AUDIT_WRITE`、`CHOWN`、`DAC_OVERRIDE`、`FOWNER`、`FSETID`、`KILL`、`MKNOD`、`NET_BIND_SERVICE`、`NET_RAW`、 `SETFCAP`、`SETGID`、`SETPCAP`、`SETUID`、`SYS_CHROOT` > Linux kernel capabilities机制介绍 默认情况下，`Docker`启动具有一组受限功能的容器。 `capabilities`机制将二进制`root/no-root`二分法转换为细粒度的访问控制系统。 只需要绑定`1024`以下端口的进程(比如`web`服务器)不需要以`root`身份运行:它们只需要被授予`net_bind_service`能力即可。 对于通常需要根特权的几乎所有特定领域，还有许多其他功能。 典型的服务器以`root`身份运行多个进程，包括`SSH`守护进程、`cron`守护进程、日志守护进程、内核模块、网络配置工具等。 容器是不同的，因为几乎所有这些任务都由容器周围的基础设施处理: - `SSH`访问通常由运行在`Docker`宿主机管理 - 必要时，`cron`应该作为一个用户进程运行，专门为需要调度服务的应用程序定制，而不是作为一个平台范围的工具 - 日志管理通常也交给`Docker`，或者像`Loggly`或`Splunk`这样的第三方服务 - 硬件管理是不相关的，这意味着您永远不需要在容器中运行`udevd`或等效的守护进程 - 网络管理也都在宿主机上设置，除非特殊需求。这意味着容器不需要执行`ifconfig`、`route`或`ip`命令（当然，除非容器被专门设计成路由器或防火墙） 这意味这大部分情况下，容器完全不需要真正的`root`权限。因此，容器可以运行一个减少的`capabilities`集，容器中的`root`也比真正的`root`拥有更少的`capabilities`,比如： - 完全禁止任何`mount`操作 - 禁止访问络`socket` - 禁止访问一些文件系统的操作，比如创建新的设备`node`等等 - 禁止模块加载 这意味这就算攻击者在容器中取得了`root`权限，也很难造成严重破坏 这不会影响到普通的`web`应用程序，但会大大减少恶意用户的攻击。默认情况下，`Docker`会删除所有需要的功能，使用`allowlist`而不是`denylist`方法 运行`Docker`容器的一个主要风险是，给容器的默认功能集和挂载可能会提供不完全的隔离 `Docker`支持添加和删除`capabilities`功能，允许使用非默认配置文件。这可能会使`Docker`通过删除功能而变得更安全，或者通过增加功能而变得更不安全。对于用户来说，最佳实践是删除除其流程显式需要的功能之外的所有功能 **简言之：`Linux Kernel capabilities`提供更细粒度的`root`权限控制** ### 4.13 敏感的主机系统目录未挂载在容器上 - 描述 不应允许将敏感的主机系统目录（如下所示）作为容器卷进行挂载，特别是在读写模式下。 ```shell script boot dev etc lib lib64 proc run sbin sys usr var 隐患分析 如果敏感目录以读写方式挂载，则可以对这些敏感目录中的文件进行更改。 这些更改可能会降低安全性，且直接影响Docker宿主机 审计方式 ```shell script [root@localhost ~]# docker ps --quiet |xargs docker inspect --format '{{.Id}}:Volumes={{.Mounts}}' 7121e891641679fda571e67a0e9953d263feca2508b013c70ae2546f6336b1a0:Volumes=[map[Destination:/config Driver:local Mode: Name:800e943d52c78312b2d6dd53bed41999fd5f7780af5098f688a894fb74f4360f Propagation: RW:true Source:/var/lib/docker/volumes/800e943d52c78312b2d6dd53bed41999fd5f7780af5098f688a894fb74f4360f/_data Type:volume]] bb3875c107daa062f2eccb10bd48ad54954cecd7d51a5eba385335f377b7aae9:Volumes=[map[Destination:/var/lib/postgresql/data Driver:local Mode: Name:774546bf5c3dcfe5f90a60012c5f1f2bdeb57a5908cdc1922b3dc75550ceeaa4 Propagation: RW:true Source:/var/lib/docker/volumes/774546bf5c3dcfe5f90a60012c5f1f2bdeb57a5908cdc1922b3dc75550ceeaa4/_data Type:volume]] 7a3a2c9e524a9d44ae857abd52447f86940dd49e1947291e7985b98e3c6a309a:Volumes=[map[Destination:/usr/src/redmine/config/configuration.yml Mode: Propagation:rprivate RW:true Source:/cephfs/redmine/config/configuration.yml Type:bind] map[Destination:/usr/src/redmine/files Mode: Propagation:rprivate RW:true Source:/cephfs/redmine/files Type:bind] map[Destination:/usr/src/redmine/app/models/attachment.rb Mode: Propagation:rprivate RW:true Source:/cephfs/redmine/config/attachment.rb Type:bind] map[Destination:/usr/src/redmine/config.ru Mode: Propagation:rprivate RW:true Source:/cephfs/redmine/config/config.ru Type:bind]] 0780c27f8eb858e172e6a7458d2b2221130e6dde0f64887d396ad5bc350a4a64:Volumes=[map[Destination:/var/lib/mysql Mode: Propagation:rprivate RW:true Source:/cephfs/redmine/mysql Type:bind]] - 修复建议 不要将主机敏感目录挂载在容器上，尤其是在读写模式下 ### 4.17 确保容器的内存使用合理 - 描述 默认情况下，`Docker`主机上的所有容器均等共享资源。 通过使用`Docker`主机的资源管理功能，例如内存限制，您可以控制容器可能消耗的内存量 - 隐患分析 默认情况下，容器可以使用主机上的所有内存。 您可以使用内存限制机制来防止由于一个容器消耗了所有主机资源而导致拒绝服务，以致同一主机上的其他容器无法执行预期功能 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:Memory={{.HostConfig.Memory}}' 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377:Memory=0 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:Memory=0 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:Memory=0 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:Memory=0 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:Memory=0 cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:Memory=0 如果上述命令返回0，则表示内存无限制。如果上述命令返回非零值，则表示已有内存限制策略 修复建议 建议使用--momery参数运行容器，例如可以如下运行一个容器 ```shell script docker run -idt --memory 256m centos ### 4.18 设置容器的根文件系统为只读 - 描述 通过使用`Docker`运行的只读选项，容器的根文件系统应被视为`只读镜像`。 这样可以防止在容器运行时写入容器的根文件系统 - 隐患分析 启用此选项会迫使运行时的容器明确定义其数据写入策略，可减少安全风险， 因为容器实例的文件系统不能被篡改或写入，除非它对文件系统文件夹和目录具有明确的读写权限。 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:ReadonlyRootfs={{.HostConfig.ReadonlyRootfs}}' 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377:ReadonlyRootfs=false 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:ReadonlyRootfs=false 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:ReadonlyRootfs=false 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:ReadonlyRootfs=false 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:ReadonlyRootfs=false cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:ReadonlyRootfs=false 如果上述命令返回true，则表示容器的根文件系统是只读的。 如果上述命令返回false，则意味着容器的根文件系统是可写的 修复建议 在容器的运行时添加一个只读标志以强制容器的根文件系统以只读方式装入 ```shell script docker run -read-only 在容器的运行时启用只读选项，包括但不限于如下： > 1.使用`--tmpfs` 选项为非持久数据写入临时文件系统 ```shell script docker run -idt --read-only --tmpfs \"/run\" --tmpfs \"/tmp\" centos bash 2.启用Docker rw在容器的运行时载入，以便将容器数据直接保存在Docker主机文件系统上 ```shell script docker run -idt --read-only -v /opt/app/data:/run/app/data:rw centos > 3.在容器运行期间，将容器数据传输到容器外部，以便保持容器数据。包括托管数据库，网络文件共享和 API。 ### 4.19 确保进入容器的流量绑定到特定的主机接口 - 描述 默认情况下，`Docker`容器可以连接到外部，但外部无法连接到容器。 每个传出连接都源自主机自己的`IP`地址。所以只允许通过主机上的特定外部接口访问容器服务 - 隐患分析 如果主机上有多个网络接口，则容器可以接受任何网络接一上公开端口的连接，这可能不安全。 很多时候，特定的端口暴露在外部，并且在这些端口上运行诸如入侵检测，入侵防护，防火墙，负载均衡等服务以筛选传入的公共流量。 因此，只允许来自特定外部接口的传入连接 - 审计方式 通过执行以下命令列出容器的所有运行实例及其端口映射 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:Ports={{.NetworkSettings.Ports}}' 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:Ports=map[80/tcp:[map[HostIp:0.0.0.0 HostPort:8080]]] 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:Ports=map[] 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:Ports=map[] 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:Ports=map[] cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:Ports=map[] 查看列表并确保公开的容器端口与特定接口绑定，而不是通配符IP地址- 0.0.0.0 例如，如果上述命令返回是不安全的，并且容器可以接受指定端口8080上的任何主机接口上的连接 修复建议 将容器端口绑定到所需主机端口上的特定主机接口。 ```shell script [root@localhost ~]# docker run -idt --name=nginx2 -p 192.168.235.128:18080:80 --network=nginx-net nginx:1.14-alpine 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377 [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:Ports={{.NetworkSettings.Ports}}' 83243cce85b85f9091b4c3bd7ff981762ff91c50e42ca36f2a5f47502ff00377:Ports=map[80/tcp:[map[HostIp:192.168.235.128 HostPort:18080]]] 748901568eafe1d3c21bb8e544278ed36af019281d485eb74be39b41ca549605:Ports=map[80/tcp:[map[HostIp:0.0.0.0 HostPort:8080]]] 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:Ports=map[] 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:Ports=map[] 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:Ports=map[] cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:Ports=map[] ### 4.20 容器重启策略`on-failure`设置为`5` - 描述 在`docker run`命令中使用`--restart`标志，可以指定重启策略，以便在廿出时确定是否重启容器。 基于安全考虑，应该设置重启尝试次数限制为5次 - 隐患分析 如果无限期地尝试启动容器，可能会导致主机上的拒绝服务。 这可能是一种简单的方法来执行分布式拒绝服务攻击，特别是在同一主机上有多个容器时。 此外，忽略容器的廿出状态并始终尝试重新启动容器导致未调查容器终止的根本原因。 如果一个容器被终止，应该做的是去调查它重启的原因，而不是试图无限期地重启它。 因此，建议使用故障重启策略并将其限制为最多 5 次重启尝试 - 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:RestartPolicyName={{.HostConfig.RestartPolicy.Name}} MaximumRetryCount={{.HostConfig.RestartPolicy.MaximumRetryCount}}' 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:RestartPolicyName=no MaximumRetryCount=0 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:RestartPolicyName=no MaximumRetryCount=0 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:RestartPolicyName=no MaximumRetryCount=0 d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:RestartPolicyName=no MaximumRetryCount=0 cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:RestartPolicyName=no MaximumRetryCount=0 修复建议 如果一个容器需要自己重新启动，可以如下设置： ```shell script docker run -idt --restart=on-failure:5 nginx ### 4.23 主机设备不直接共享给容器 - 描述 主机设备可以在运行时直接共享给容器。 不要将主机设备直接共享给容器，特别是对不受信任的容器 - 隐患分析 选项`--device` 将主机设备共享给容器，因此容器可以直接访问这些主机设备。 不允许容器以特权模式运行以访问和操作主机设备默认情况下，容器将能够读取，写入和`mknod`这些设备。 此外，容器可能会从主机中删除设备。 因此，不要直接将主机设备共享给容器。如果必须的将主机设备共享给容器，请适当地使用共享权限： ```shell script w -> write r -> read m -> mknod 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{{.Id}}:Devices={{.HostConfig.Devices}}' 3b8b371f5e800e25d85e7426020cb7088e6cccb5bd950ad269a185cadf6f7adc:Devices=[] 5bf74b6014405acad5f724cb005b320a864528ac2dd48de1fbb0e37165befc71:Devices=[] 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:Devices=[] d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:Devices=[] cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:Devices=[] 验证是否需要从容器中访问主机设备，并且正确设置所需的权限。如果上述命令返回[]，则容器无权访问主机设备 - 修复建议 不要将主机设备直接共享于容器。如果必须将主机设备共享给容器，请使用正确的一组权限，以下为错误示范 ```shell script docker run --interactive --tty --device=/dev/tty0:/dev/tty0:rwm centos bash 4.24 设置装载传播模式不共享 描述 装载传播模式允许在容器上以shared、private和slave模式挂载数据卷。只有必要的时候才使用共享模式 隐患分析 共享模式下挂载卷不会限制任何其他容器的安装并对该卷进行更改。 如果使用的数据卷对变化比较敏感，则这可能是灾难性的。最好不要将安装传播模式设置为共享 审计方式 ```shell script [root@localhost ~]# docker ps --quiet --all|xargs docker inspect --format '{.Id}:Propagation={{range $mnt:=.Mounts}}{{json $mnt.Propagation}}{{end}}' {.Id}:Propagation= {.Id}:Propagation= {.Id}:Propagation= {.Id}:Propagation= {.Id}:Propagation= 上述命令将返回已安装卷的传播模式。除非需要，不应将传播模式设置为共享。 - 修复建议 不建议以共享模式传播中安装卷。例如，不要启动容器，如下所示 ```shell script docker run --volume=/hostPath:/containerPath:shared 4.29 限制使用PID cgroup 描述 在容器运行时使用--pids-limit标志 隐患分析 攻击者可以在容器内发射fork炸弹。 这个fork炸弹可能会使整个系统崩溃，并需要重新启动主机以使系统重新运行。 PIDs cgroup --pids-limit将通过限制在给定时间内可能发生在容器内的fork数来防止这种攻击 审计分析 运行以下命令并确保PidsLimit未设置为0或-1。 PidsLimit为0或-1意味着任何数量的进程可以同时在容器内分叉。 ```shell script [root@localhost ~]# docker ps --quiet | xargs docker inspect --format='{{.Id}}:PidsLi mit={{.HostConfig.PidsLimit}}' 0aede0130fd30b8cb40200aa9b61e84f0d911740617dda3dd707037655419854:PidsLimit= d35fd7bd5e90e6aebc237368453361f632f775490da3c1d28011b9f7e43ff75c:PidsLimit= cff4f40d63e7ba39cb013706f0c73351c3a99325adf606c715df63b8c81001be:PidsLimit= - 修复建议 升级内核至`4.3+`，添加`--pids-limit参数`，如 ```shell script docker run -idt --name=box --pids-limit=100 busybox:1.31.1 5.Docker安全操作 5.1 避免镜像泛滥 描述 不要在同一主机上保留大量容器镜像，根据需要仅使用标记的镜像。 隐患分析 标记镜像有助于从latest退回到生产中镜像的特定版本。 如果实例化了未使用或旧标签的镜像，则可能包含可能被利用的漏洞。 此外，如果您无法从系统中删除未使用的镜像，并且存在各种此类冗余和未使用的镜像，主机文件空间可能会变满，从而导致拒绝服务。 审计方式 1.通过执行以下命令列出当前实例化的所有镜像ID ```shell script [root@localhost ~]# docker images --quiet | xargs docker inspect --format='{{.Id}}:Image={{.Config.Image}}' sha256:d6e46aa2470df1d32034c6707c8041158b652f38d2a9ae3d7ad7e7532d22ebe0:Image=sha256:3543079adc6fb5170279692361be8b24e89ef1809a374c1b4429e1d560d1459c > 2.通过执行以下命令列出系统中存在的所有镜像 ```shell script [root@localhost ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE harbor.wl.com/public/alpine latest d6e46aa2470d 6 months ago 5.57MB 3.比较步骤1和步骤2中的镜像ID列表，找出当前未实例化的镜像。如果发现未使用或旧镜像，请与系统管理员讨论是否需要在系统上保留这些镜像 修复建议 保留您实际需要的一组镜像，并建立工作流程以从主机中删除陈旧的镜像。 此外，使用诸如按摘要的功能从镜像仓库中获取特定镜像。 对于无用镜像，应予以删除 5.2 避免容器泛滥 描述 不要在同一主机上保留大量无用容器 隐患分析 容器的灵活性使得运行多个应用程序实例变得很容易，并间接导致存在于不同安全补丁级别的Docker镜像。 因此，避免容器泛滥，并将主机上的容器数量保持在可管理的总量上 审计方式 1.查找主机上的容器总数 ```shell script [root@localhost ~]# docker info --format '{{.Containers}}' 1 > 2.执行以下命令以查找主机上实际正在运行或处于停止状态的容器总数。 ```shell script [root@localhost ~]# docker info --format '{{.ContainersStopped}}' 0 [root@localhost ~]# docker info --format '{{.ContainersRunning}}' 1 如果主机上保留的容器数量与主机上实际运行的容器数量之间的差异很大（比如说 25 或更多）， 那么请清理无用容器（确保stopped无用再进行清理）。 修复建议 定期检查每个主机的容器清单，并使用以下命令清理已停止的容器 [root@localhost ~]# docker container prune WARNING! This will remove all stopped containers. Are you sure you want to continue? [y/N] y Total reclaimed space: 0B 最佳实践 安装 安装更新CentOS 7最新稳定版 降低低版本操作系统可能存在的安全漏洞 安装更新最新稳定版内核 香港镜像源 更新高版本内核以支持docker新特性、降低内核导致的安全漏洞风险 安装最新稳定版docker-ce docker-ce 二进制下载地址 docker-ce 镜像源 配置 配置limit参数 ```shell script ulimit -HSn 65536 cat >/etc/security/limits.conf soft nofile 65536 hard nofile 65536 soft noproc 10240 hard noproc 10240 EOF ``` 配置内核参数 ```shell script cat >/etc/sysctl.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-arptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 user.max_user_namespaces=15000 EOF sysctl -p > 配置`docker daemon` ```shell script mkdir -p /etc/docker cat /etc/docker/daemon.json { \"log-opts\": { \"max-size\": \"5m\", \"max-file\":\"3\" }, \"userland-proxy\": false, \"live-restore\": true, \"default-ulimits\": { \"nofile\": { \"Hard\": 64000, \"Name\": \"nofile\", \"Soft\": 64000 } }, \"default-address-pools\": [ { \"base\": \"172.80.0.0/16\", \"size\": 24 }, { \"base\": \"172.90.0.0/16\", \"size\": 24 } ], \"no-new-privileges\": false, \"default-gateway\": \"\", \"default-gateway-v6\": \"\", \"default-runtime\": \"runc\", \"default-shm-size\": \"64M\", \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl daemon-reload systemctl restart docker 文件权限调整 shell script chmod 755 /etc/docker chown root:root /etc/docker chmod 660 /var/run/docker.sock chown root:docker /var/run/docker.sock systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 chmod 644 systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 chmod 644 systemctl show -p FragmentPath docker.socket|sed \"s/FragmentPath=//\"|xargs -n1 chown root:root systemctl show -p FragmentPath docker.service|sed \"s/FragmentPath=//\"|xargs -n1 chown root:root if [ -d /etc/docker/certs.d/ ];then chmod 444 /etc/docker/certs.d/*; fi if [ -d /etc/docker/certs.d/ ];then chown root:root /etc/docker/certs.d/*; fi if [ -f /etc/docker/daemon.json ];then chown root:root /etc/docker/daemon.json; fi 参考文档 Docker容器最佳安全实践白皮书（V1.0） Docker官方文档 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/镜像构建/buildah.html":{"url":"2.容器/镜像构建/buildah.html","title":"buildah","keywords":"","body":"buildah 安装 编译安装 安装依赖 yum -y install \\ make \\ gcc \\ golang \\ bats \\ btrfs-progs-devel \\ device-mapper-devel \\ glib2-devel \\ gpgme-devel \\ libassuan-devel \\ libseccomp-devel \\ git \\ bzip2 \\ go-md2man \\ runc \\ skopeo-containers mkdir ~/buildah cd ~/buildah export GOPATH=`pwd` git clone https://github.com/containers/buildah ./src/github.com/containers/buildah cd ./src/github.com/containers/buildah make sudo make install buildah --help 安装binary 下载解压 buildah-release-1.21-linux-amd64.tar.gz tar zxvf buildah-release-1.21-linux-amd64.tar.gz cp buildah-release-1.21-linux-amd64/bin/buildah /usr/bin Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/镜像构建/podman.html":{"url":"2.容器/镜像构建/podman.html","title":"podman","keywords":"","body":"podman Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"2.容器/镜像构建/skopeo.html":{"url":"2.容器/镜像构建/skopeo.html","title":"skopeo","keywords":"","body":"skopeo 项目地址 安装 编译安装 安装 yum install -y btrfs-progs-devel gpgme-devel device-mapper-devel libassuan-devel git clone https://github.com/containers/skopeo.git cd skopeo make bin/skopeo cp bin/skopeo /usr/local/bin chmod +x /usr/local/bin/skopeo 预编译包 skopeo-1.3.1-linux-amd64.tar.gz 下载安装 wget https://github.com/weiliang-ms/skopeo/releases/download/v1.3.1/skopeo-1.3.1-linux-amd64.tar.gz tar zxvf skopeo-1.3.1-linux-amd64.tar.gz cp skopeo-1.3.1-linux-amd64/bin/skopeo /usr/local/bin chmod +x /usr/local/bin/skopeo dir格式批量导出/导入 文件层级 harbor-export ├── download.sh ├── image-list.txt └── upload.sh image-list.txt harbor.wl.com/kubernetes/csi-snapshotter:v3.0.2 harbor.wl.com/kubernetes/csi-attacher:v3.0.2 harbor.wl.com/kubernetes/csi-node-driver-registrar:v2.0.1 harbor.wl.com/kubernetes/csi-provisioner:v2.0.4 harbor.wl.com/kubernetes/csi-resizer:v1.0.1 harbor.wl.com/kubernetes/cephfs-provisioner:latest harbor.wl.com/kubernetes/cephcsi:v3.2.0 harbor.wl.com/kubernetes/cephcsi:v3.2.1 harbor.wl.com/kubernetes/node:v3.15.1 harbor.wl.com/kubernetes/cni:v3.15.1 harbor.wl.com/kubernetes/pod2daemon-flexvol:v3.15.1 harbor.wl.com/kubernetes/kube-controllers:v3.15.1 harbor.wl.com/kubernetes/coredns:1.6.9 导出脚本download.sh #!/bin/bash GREEN_COL=\"\\\\033[32;1m\" RED_COL=\"\\\\033[1;31m\" NORMAL_COL=\"\\\\033[0;39m\" SOURCE_REGISTRY=harbor.wl.com REGISTRY_USER=admin REGISTRY_PASS=Harbor-12345 TARGET_REGISTRY=\"\" PROJECT_NAME=$1 IMAGES_DIR=$PROJECT_NAME/images : ${IMAGES_DIR:=\"images\"} : ${IMAGES_LIST_FILE:=\"$PROJECT_NAME/image-list.txt\"} : ${TARGET_REGISTRY:=\"hub.k8s.li\"} : ${SOURCE_REGISTRY:=\"harbor.chs.neusoft.com\"} BLOBS_PATH=\"$PROJECT_NAME/docker/registry/v2/blobs/sha256\" REPO_PATH=\"$PROJECT_NAME/docker/registry/v2/repositories\" set -eo pipefail CURRENT_NUM=0 TOTAL_NUMS=$(cat \"$IMAGES_LIST_FILE\" | wc -l) skopeo_sync() { mkdir -p $2/$1 if skopeo sync --all --insecure-policy --src-tls-verify=false --dest-tls-verify=false \\ --override-arch amd64 --override-os linux --src docker --dest dir $1 $2 > /dev/null; then echo -e \"$GREEN_COL Progress: ${CURRENT_NUM}/${TOTAL_NUMS} sync $1 to $2 successful $NORMAL_COL\" else echo -e \"$RED_COL Progress: ${CURRENT_NUM}/${TOTAL_NUMS} sync $1 to $2 failed $NORMAL_COL\" exit 2 fi } if [ -d $IMAGES_DIR ];then rm -rf $IMAGES_DIR fi mkdir -p $IMAGES_DIR #while read line #do # let CURRENT_NUM=${CURRENT_NUM}+1 # skopeo_sync ${line} $IMAGES_DIR #done $image_name image-tag -> $image_tag image-repo -> $image_repo\" mkdir -p ${IMAGES_DIR}/${image_repo} skopeo_sync ${SOURCE_REGISTRY}/${image} ${IMAGES_DIR}/${image_repo} manifest=\"${IMAGES_DIR}/${image}/manifest.json\" manifest_sha256=$(sha256sum ${manifest} | awk '{print $1}') mkdir -p ${BLOBS_PATH}/${manifest_sha256:0:2}/${manifest_sha256} ln -f ${manifest} ${BLOBS_PATH}/${manifest_sha256:0:2}/${manifest_sha256}/data # make image repositories dir mkdir -p ${REPO_PATH}/${image_name}/{_uploads,_layers,_manifests} mkdir -p ${REPO_PATH}/${image_name}/_manifests/revisions/sha256/${manifest_sha256} mkdir -p ${REPO_PATH}/${image_name}/_manifests/tags/${image_tag}/{current,index/sha256} mkdir -p ${REPO_PATH}/${image_name}/_manifests/tags/${image_tag}/index/sha256/${manifest_sha256} # create image tag manifest link file echo -n \"sha256:${manifest_sha256}\" > ${REPO_PATH}/${image_name}/_manifests/tags/${image_tag}/current/link echo -n \"sha256:${manifest_sha256}\" > ${REPO_PATH}/${image_name}/_manifests/revisions/sha256/${manifest_sha256}/link echo -n \"sha256:${manifest_sha256}\" > ${REPO_PATH}/${image_name}/_manifests/tags/${image_tag}/index/sha256/${manifest_sha256}/link # link image layers file to registry blobs dir for layer in $(sed '/v1Compatibility/d' ${manifest} | grep -Eo \"\\b[a-f0-9]{64}\\b\"); do mkdir -p ${BLOBS_PATH}/${layer:0:2}/${layer} mkdir -p ${REPO_PATH}/${image_name}/_layers/sha256/${layer} echo -n \"sha256:${layer}\" > ${REPO_PATH}/${image_name}/_layers/sha256/${layer}/link ln -f ${IMAGES_DIR}/${image}/${layer} ${BLOBS_PATH}/${layer:0:2}/${layer}/data done done 使用方式 sh download.sh library 导入脚本upload.sh #!/bin/bash REGISTRY_DOMAIN=\"harbor.wl.com\" # 切换到 registry 存储主目录下 gen_skopeo_dir() { # 定义 registry 存储的 blob 目录 和 repositories 目录，方便后面使用 BLOB_DIR=\"docker/registry/v2/blobs/sha256\" REPO_DIR=\"docker/registry/v2/repositories\" # 定义生成 skopeo 目录 SKOPEO_DIR=\"docker/skopeo\" # 通过 find 出 current 文件夹可以得到所有带 tag 的镜像，因为一个 tag 对应一个 current 目录 for image in $(find ${REPO_DIR} -type d -name \"current\"); do # 根据镜像的 tag 提取镜像的名字 name=$(echo ${image} | awk -F '/' '{print $5\"/\"$6\":\"$9}') link=$(cat ${image}/link | sed 's/sha256://') mfs=\"${BLOB_DIR}/${link:0:2}/${link}/data\" # 创建镜像的硬链接需要的目录 mkdir -p \"${SKOPEO_DIR}/${name}\" # 硬链接镜像的 manifests 文件到目录的 manifest 文件 ln ${mfs} ${SKOPEO_DIR}/${name}/manifest.json # 使用正则匹配出所有的 sha256 值，然后排序去重 layers=$(grep -Eo \"\\b[a-f0-9]{64}\\b\" ${mfs} | sort -n | uniq) for layer in ${layers}; do # 硬链接 registry 存储目录里的镜像 layer 和 images config 到镜像的 dir 目录 ln ${BLOB_DIR}/${layer:0:2}/${layer}/data ${SKOPEO_DIR}/${name}/${layer} done done } sync_image() { # 使用 skopeo sync 将 dir 格式的镜像同步到 harbor for project in $(ls ${SKOPEO_DIR}); do skopeo sync --insecure-policy --src-tls-verify=false --dest-tls-verify=false \\ --src dir --dest docker ${SKOPEO_DIR}/${project} ${REGISTRY_DOMAIN}/${project} done } gen_skopeo_dir sync_image 登录 skopeo login harbor.wl.com --tls-verify=false 执行导入 sh upload.sh Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/":{"url":"3.集成部署/","title":"3.集成部署","keywords":"","body":"!/bin/bash # Bitnami Redis Cluster library shellcheck disable=SC1091 shellcheck disable=SC2178 shellcheck disable=SC2128 shellcheck disable=SC1090 Load Generic Libraries . /opt/bitnami/scripts/libfile.sh . /opt/bitnami/scripts/libfs.sh . /opt/bitnami/scripts/liblog.sh . /opt/bitnami/scripts/libnet.sh . /opt/bitnami/scripts/libos.sh . /opt/bitnami/scripts/libservice.sh . /opt/bitnami/scripts/libvalidations.sh . /opt/bitnami/scripts/libredis.sh Functions # Load global variables used on Redis configuration. Globals: REDIS_* Arguments: None Returns: Series of exports to be used as 'eval' arguments # redis_cluster_env() { cat # Validate settings in REDIS_* env vars. Globals: REDIS_* Arguments: None Returns: None # rediscluster_validate() { debug \"Validating settings in REDIS* env vars..\" local error_code=0 # Auxiliary functions print_validation_error() { error \"$1\" error_code=1 } empty_password_enabled_warn() { warn \"You set the environment variable ALLOW_EMPTY_PASSWORD=${ALLOW_EMPTY_PASSWORD}. For safety reasons, do not use this flag in a production environment.\" } empty_password_error() { print_validation_error \"The $1 environment variable is empty or not set. Set the environment variable ALLOW_EMPTY_PASSWORD=yes to allow the container to be started with blank passwords. This is recommended only for development.\" } if is_boolean_yes \"$ALLOW_EMPTY_PASSWORD\"; then empty_password_enabled_warn else if ! is_boolean_yes \"$REDIS_CLUSTER_CREATOR\"; then [[ -z \"$REDIS_PASSWORD\" ]] && empty_password_error REDIS_PASSWORD fi fi if ! is_boolean_yes \"$REDIS_CLUSTER_DYNAMIC_IPS\"; then if ! is_boolean_yes \"$REDIS_CLUSTER_CREATOR\"; then [[ -z \"$REDIS_CLUSTER_ANNOUNCE_IP\" ]] && print_validation_error \"To provide external access you need to provide the REDIS_CLUSTER_ANNOUNCE_IP env var\" fi fi [[ -z \"$REDIS_NODES\" ]] && print_validation_error \"REDIS_NODES is required\" if [[ -z \"$REDIS_PORT\" ]]; then print_validation_error \"REDIS_PORT cannot be empty\" fi if is_boolean_yes \"$REDIS_CLUSTER_CREATOR\"; then [[ -z \"$REDIS_CLUSTER_REPLICAS\" ]] && print_validation_error \"To create the cluster you need to provide the number of replicas\" fi [[ \"$error_code\" -eq 0 ]] || exit \"$error_code\" } # Redis specific configuration to override the default one Globals: REDIS_* Arguments: None Returns: None # redis_cluster_override_conf() { Redis configuration to override redis_conf_set daemonize no redis_conf_set cluster-enabled yes redis_conf_set cluster-config-file \"${REDIS_VOLUME}/data/nodes.conf\" if ! (is_boolean_yes \"$REDIS_CLUSTER_DYNAMIC_IPS\" || is_boolean_yes \"$REDIS_CLUSTER_CREATOR\"); then redis_conf_set cluster-announce-ip \"$REDIS_CLUSTER_ANNOUNCE_IP\" fi } # Ensure Redis is initialized Globals: REDIS_* Arguments: None Returns: None # redis_cluster_initialize() { redis_configure_default redis_cluster_override_conf } # Creates the Redis cluster Globals: REDIS_* Arguments: - $@ Array with the hostnames Returns: None # redis_cluster_create() { local nodes=(\"$@\") local ips=() for node in \"${nodes[@]}\"; do while [[ $(redis-cli -h \"$node\" -p \"$REDIS_PORT\" ping) != 'PONG' ]]; do echo \"Node $node not ready, waiting for all the nodes to be ready...\" sleep 1 done ips=($(dns_lookup \"$node\") \"${ips[@]}\") done redis-cli --cluster create \"${ips[@]/%/:${REDIS_PORT}}\" --cluster-replicas \"$REDIS_CLUSTER_REPLICAS\" --cluster-yes || true if redis_cluster_check \"${ips[0]}\"; then echo \"Cluster correctly created\" else echo \"The cluster was already created, the nodes should have recovered it\" fi } # Checks if the cluster state is correct. Params: - $1: node where to check the cluster state # redis_cluster_check() { local -r check=$(redis-cli --cluster check \"$1\":\"$REDIS_PORT\") if [[ $check =~ \"All 16384 slots covered\" ]]; then true else false fi } # Recovers the cluster when using dynamic IPs by changing them in the nodes.conf Globals: REDIS_* Arguments: None Returns: None # redis_cluster_update_ips() { IFS=' ' read -ra nodes declare -A host_2_ip_array # Array to map hosts and IPs # Update the IPs when a number of nodes > quorum change their IPs if [[ ! -f \"${REDIS_VOLUME}/data/nodes.sh\" ]]; then # It is the first initialization so store the nodes for node in \"${nodes[@]}\"; do ip=$(wait_for_dns_lookup \"$node\" \"$REDIS_DNS_RETRIES\" 5) host_2_ip_array[\"$node\"]=\"$ip\" done echo \"Storing map with hostnames and IPs\" declare -p host_2_ip_array > \"${REDIS_VOLUME}/data/nodes.sh\" else # The cluster was already started . \"${REDIS_VOLUME}/data/nodes.sh\" # Update the IPs in the nodes.conf for node in \"${nodes[@]}\"; do newIP=$(wait_for_dns_lookup \"$node\" \"$REDIS_DNS_RETRIES\" 5) # The node can be new if we are updating the cluster, so catch the unbound variable error if [[ ${host_2_ip_array[$node]+true} ]]; then echo \"Changing old IP ${host_2_ip_array[$node]} by the new one ${newIP}\" nodesFile=$(sed \"s/${host_2_ip_array[$node]}/$newIP/g\" \"${REDIS_VOLUME}/data/nodes.conf\") echo \"$nodesFile\" > \"${REDIS_VOLUME}/data/nodes.conf\" fi host_2_ip_array[\"$node\"]=\"$newIP\" done declare -p host_2_ip_array > \"${REDIS_VOLUME}/data/nodes.sh\" fi } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/chrony/chrony.html":{"url":"3.集成部署/chrony/chrony.html","title":"chrony","keywords":"","body":"CentOS7时钟同步问题 最近项目中遇到个比较棘手的问题，虽然配置了时钟同步，但有些主机（虚拟机）时钟偏移量有些离谱（10+秒） 最终通过如下步骤，暂时解决。暂未发现问题。 清理主机上的时钟同步定时任务 由于主机由虚拟机模板创建，默认带了一个时钟同步命令，与现有的地址不一致，故先删除该任务。 确认定时任务 crontab -l 删除定时任务(会清空当前用户下的定时任务，执行前确保无其他定时任务策略) crontab -r 注： 这个场景还是有几率遇到 安装配置chrony yum install -y chrony 配置 10.10.10.10注意替换为你的实际时钟服务器服务端地址 echo \"server 10.10.10.10 iburst\" >> /etc/chrony.conf 关闭默认地址（内网解析不到） sed -i \"s;server 0.centos.pool.ntp.org iburst;#server 0.centos.pool.ntp.org iburst;g\" /etc/chrony.conf sed -i \"s;server 1.centos.pool.ntp.org iburst;#server 1.centos.pool.ntp.org iburst;g\" /etc/chrony.conf sed -i \"s;server 2.centos.pool.ntp.org iburst;#server 2.centos.pool.ntp.org iburst;g\" /etc/chrony.conf sed -i \"s;server 3.centos.pool.ntp.org iburst;#server 3.centos.pool.ntp.org iburst;g\" /etc/chrony.conf 启动 systemctl enable chronyd --now 关闭虚拟机主动同步主机时间 以vCenter为例 任选主机观察 查看时钟同步情况（定时刷新） watch chronyc sourcestats 输出格式如下 Every 2.0s: chronyc sourcestats Mon Sep 27 20:23:08 2021 210 Number of sources = 1 Name/IP Address NP NR Span Frequency Freq Skew Offset Std Dev ============================================================================== dns.sdly.hsip.gov.cn.165> 24 11 30m -0.003 0.032 -255ns 21us 观察Offset那列即可 强制同步 chronyc -a makestep timedatectl set-ntp yes 查看同步状态 timedatectl Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/es/es.html":{"url":"3.集成部署/es/es.html","title":"es","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/ftp/":{"url":"3.集成部署/ftp/","title":"ftp","keywords":"","body":"ftp 安装ftp yum install -y vsftpd 创建用户 useradd -s /sbin/nologin ftpuser passwd ftpuser 目录赋权 chown -R ftpuser:ftpuser /data 配置vsftp服务 sed -i \"s/anonymous_enable=YES/anonymous_enable=NO/g\" /etc/vsftpd/vsftpd.conf cat >> /etc/vsftpd/vsftpd.conf 添加用户 cat > /etc/vsftpd/user_list 启动 systemctl enable vsftpd --now Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/git/":{"url":"3.集成部署/git/","title":"git","keywords":"","body":"gitlab安装 添加源 $ cat> /etc/yum.repos.d/gitlab-ce.repo 安装 $ yum install curl policycoreutils-python openssh-server gitlab-ce -y 调整配置 精简配置 调整/etc/gitlab/gitlab.rb external_url(若80被占用建议使用8888) 数据目录 git_data_dirs({ \"default\" => { \"path\" => \"/data/gitlab/data\" } }) 重载配置 $ gitlab-ctl reconfigure 启动 $ gitlab-ctl start 初始化root用户 建立连接，需要大约半分钟左右 $ gitlab-rails console 初始化 u=User.where(id:1).first u.password='Gitlab@321' u.password_confirmation='Gitlab@321' u.save! quit git-cli安装 编译安装 $ curl -L https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.9.5.tar.xz -o ./git-2.9.5.tar.xz -k $ yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker -y $ tar xvf git-2.9.5.tar.xz $ cd git-2.9.5 $ ./configure --prefix=/usr/local/git $ make && make install $ cat >> ~/.bash_profile 重写大的历史提交 使用以下命令可以查看占用空间最多的五个文件： weiliang@DESKTOP-O8QG6I5:/mnt/d/github/easyctl$ git rev-list --objects --all | grep \"$(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | tail -5 | awk '{print$1}') \" 8d403ce945dc4254dfd9be92febe85ae17fb7276 _output/easyctl f3872705299ba5846ecd4007669a2d41510d8f4e _output/easyctl b9d2a6fdd7ad22462245d7f2b030aea145789ac5 easyctl f9a38dd0d17fb44d6da3661d3eaf5c74ff8b92dd easyctl b497ec95ee10e51f32af488e5caf47f19b564f29 easyctl _output/easyctl、easyctl为二进制文件，可以删除 $ git stash $ git filter-branch --force --index-filter 'git rm -rf --cached --ignore-unmatch _output/easyctl' --prune-empty --tag-name-filter cat -- --all 推送 $ git push origin master --force 清理回收空间 $ rm -rf .git/refs/original/ $ git reflog expire --expire=now --all $ git gc --prune=now Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/hadoop/":{"url":"3.集成部署/hadoop/","title":"hadoop","keywords":"","body":"hadoop 准备离线资源 mysql-connector-java-5.1.48.tar.gz linux节点配置 假设节点IP为： 主：192.168.1.12 从：192.168.1.13 从：192.168.1.14 设置hostname 节点一执行 cat >> /etc/sysconfig/network /proc/sys/kernel/hostname 节点二执行 cat >> /etc/sysconfig/network /proc/sys/kernel/hostname 节点三执行 cat >> /etc/sysconfig/network /proc/sys/kernel/hostname 配置host解析（三个节点均执行，注意IP替换为实际IP） cat >> /etc/hosts 关闭防火墙、selinux systemctl stop firewalld --now setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 节点主机互信 master节点执行 ssh-keygen -t rsa -n '' -f ~/.ssh/id_rsa # 根据提示输入对应节点root口令 ssh-copy-id hadoop1 ssh-copy-id hadoop2 ssh-copy-id hadoop3 安装oracle jdk（1.8）并配置软链接(oracle jdk安装至/opt/java下) mkdir -p /usr/java ln -s /opt/java /usr/java/jdk1.8 调整文件句柄数 echo \"* soft nofile 655350\" >> /etc/security/limits.conf echo \"* hard nofile 655350\" >> /etc/security/limits.conf echo \"* soft nproc 65535\" >> /etc/security/limits.conf echo \"* hard nproc 65535\" >> /etc/security/limits.conf ulimit -n 655350 主节点安装Mysql yum安装Marbidb yum install mariadb mariadb-server -y 启动 systemctl enable mariadb --now 初始化用户、数据库 mysql -u root 上传mysql-connector-java-5.1.48.tar.gz 至/tmp目录下，执行以下命令 mkdir -p /usr/share/java tar zxvf mysql-connector-java-5.1.48.tar.gz cp mysql-connector-java-5.1.48/mysql-connector-java-5.1.48.jar /usr/share/java/mysql-connector-java.jar 初始化CM Server数据库 /usr/share/cmf/schema/scm_prepare_database.sh mysql scm_db scm_server scm_server -h 127.0.0.1 创建cloudera-manager本地镜像源（主节点） 安装repo工具 yum install yum-utils createrepo yum-plugin-priorities -y 创建/cm目录，上传安装介质,结构如下 /cm ├── cloudera-manager-agent-5.7.0-1.cm570.p0.76.el7.x86_64.rpm ├── cloudera-manager-daemons-5.7.0-1.cm570.p0.76.el7.x86_64.rpm ├── cloudera-manager-server-5.7.0-1.cm570.p0.76.el7.x86_64.rpm ├── cloudera-manager-server-db-2-5.7.0-1.cm570.p0.76.el7.x86_64.rpm ├── enterprise-debuginfo-5.7.0-1.cm570.p0.76.el7.x86_64.rpm ├── jdk-6u31-linux-amd64.rpm ├── oracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm └── RPM-GPG-KEY-cloudera 创建repo源文件 cd /cm && createrepo ./ 配置本地cloudera-manager源 cat > /etc/yum.repos.d/cm.repo 主节点上传CDH文件 创建目录(server节点、agent节点均需执行) mkdir -p /opt/cloudera/parcel-repo 上传以下文件至主节点/opt/cloudera/parcel-repo下 CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha1 manifest.json 生成CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.torrent.sha cd /opt/cloudera/parcel-repo sha1sum CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.torrent | awk '{print $1}'> CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.torrent.sha 修改CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha1名 cd /opt/cloudera/parcel-repo mv CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha1 CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha 分发至agent节点 scp /opt/cloudera/parcel-repo/* hadoop2:/opt/cloudera/parcel-repo/ scp /opt/cloudera/parcel-repo/* hadoop3:/opt/cloudera/parcel-repo/ 安装Cloudera Manager Server端 yum安装cloudera-manager yum install cloudera-manager-daemons cloudera-manager-server -y 安装Cloudera Manager Agent端（所有agent节点） 拷贝资源文件 主节点拷贝以下内容至agent节点 scp -r /cm hadoop2:/ scp -r /cm hadoop3:/ scp /etc/yum.repos.d/cm.repo hadoop2:/etc/yum.repos.d/ scp /etc/yum.repos.d/cm.repo hadoop3:/etc/yum.repos.d/ 安装agent（agent节点运行） yum install cloudera-manager-agent -y 修改agent配置文件 修改文件/etc/cloudera-scm-agent/config.ini以下内容 server_host=localhost # listening_ip # listening_hostname= 启动agent systemctl start cloudera-scm-agent 启动CM Server端 启动 systemctl start cloudera-scm-server 访问WEB UI （主节点7180端口） 登录账号：admin/admin 接受协议 部署免费版本 确认部署应用，点击继续 添加部署节点，点击搜索 选取节点，继续 确认cdh版本，继续 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/mail/搭建内网邮件服务器.html":{"url":"3.集成部署/mail/搭建内网邮件服务器.html","title":"搭建内网邮件服务器","keywords":"","body":"搭建配置postfix 配置 $ sudo tee /etc/postfix/main.cf 启动 $ systemctl enable postfix --now 安装cyrus-sasl sasl主要用于邮箱用户名密码认证 安装sasl $ yum -y install cyrus-sasl 配置sasl $ sudo tee /etc/sysconfig/saslauthd 启动sasl [root@localhost ~]# service saslauthd start Redirecting to /bin/systemctl start saslauthd.service [root@localhost ~]# chkconfig saslauthd on Note: Forwarding request to 'systemctl enable saslauthd.service'. Created symlink from /etc/systemd/system/multi-user.target.wants/saslauthd.service to /usr/lib/systemd/system/saslauthd.service. 测试sasl [root@localhost ~]# useradd ilanni &&echo 'ilannimail'| passwd --stdin ilanni Changing password for user ilanni. passwd: all authentication tokens updated successfully. [root@localhost ~]# su - ilanni [ilanni@localhost ~]$ mkdir -p ~/mail/.imap/INBOX [ilanni@localhost ~]$ testsaslauthd -u ilanni -p 'ilannimail' 0: OK \"Success.\" [ilanni@localhost ~]$ exit [root@localhost ~]# userdel -r ilanni 测试postfix $ yum install -y telnet 安装dovecot 安装 $ yum -y install dovecot dovecot-devel dovecot-mysql pam-devel 配置 $ sudo tee /etc/dovecot/dovecot.conf 启动 $ systemctl enable dovecot --now 重启postfix $ systemctl restart postfix 查看端口监听 [root@localhost ~]# ss -aln|grep 25 tcp LISTEN 0 100 *:25 *:* tcp LISTEN 0 100 [::]:25 [::]:* [root@localhost ~]# ss -aln|grep 110 tcp LISTEN 0 100 *:110 *:* tcp LISTEN 0 100 [::]:110 [::]:* [root@localhost ~]# ss -aln|grep 143 tcp LISTEN 0 100 *:143 *:* tcp LISTEN 0 100 [::]:143 [::]:* 创建邮件用户 [root@localhost ~]# useradd user1 &&echo '123456'| passwd --stdin user1 Changing password for user user1. passwd: all authentication tokens updated successfully. [root@localhost ~]# useradd user2 &&echo '123456'| passwd --stdin user2 Changing password for user user2. passwd: all authentication tokens updated successfully. 激活用户 [root@localhost home]# telnet 192.168.1.1 pop3 Trying 192.168.1.1... Connected to 192.168.1.1. Escape character is '^]'. +OK Dovecot ready. user user1 +OK pass 123456 +OK Logged in. quit +OK Logging out. Connection closed by foreign host. [root@localhost home]# ls /home/user1/ .bash_logout .bash_profile .bashrc Maildir/ [root@localhost home]# ls /home/user1/Maildir/ cur dovecot.index.log dovecot-uidlist dovecot-uidvalidity dovecot-uidvalidity.61c43406 new tmp [root@localhost home]# telnet 192.168.1.1 pop3 Trying 192.168.1.1... Connected to 192.168.1.1. Escape character is '^]'. +OK Dovecot ready. user user2 +OK pass 123456 +OK Logged in. quit +OK Logging out. Connection closed by foreign host. [root@localhost home]# ls /home/user2/ .bash_logout .bash_profile .bashrc Maildir/ [root@localhost home]# ls /home/user2/Maildir/ cur dovecot.index.log dovecot-uidlist dovecot-uidvalidity dovecot-uidvalidity.61c43406 new tmp 配置foxmail 配置host解析 C:\\Windows\\System32\\drivers\\etc\\hosts 添加解析内容（192.168.1.1为邮件服务器IP地址） 192.168.1.1 mail.wl.com 配置foxmail 配置信息如下 测试 user1与user2互发邮件 架构说明 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/mysql/":{"url":"3.集成部署/mysql/","title":"mysql","keywords":"","body":" 集成部署 单机集成部署 添加slave节点 cli命令 查看连接 配置优化 连接数 暂存连接数 缓冲区变量 防止暴力破解 限制数据包大小 使用教程 慢查询 查看变量 查看锁性能 查看连接数 查看回滚数量 查询运行时间 查询缓存状态 查看连接信息 查询表使用状态 查看增删改数量 修改密码 binlog 集成部署 较全的教程 单机集成部署 适用于CentOS Red Hat 官网 5.5 下载 官网 5.6 下载 官网 5.7 下载 版本信息 5.7.27社区版 配置yum源 配置基础yum源即可无需epel源 配置阿里云源（保证网络可达） Centos-5.repo Centos-6.repo Centos-7.repo 安装卸载依赖 yum install net-tools -y yum remove mysql* -y centos7需要卸载mariadb yum remove -y mariadb-libs 上传安装包至/tmp下，进行安装 cd /tmp rpm -ivh mysql-community-common-*.el7.x86_64.rpm rpm -ivh mysql-community-libs-*.el7.x86_64.rpm rpm -ivh mysql-community-client-*.el7.x86_64.rpm rpm -ivh mysql-community-server-*.el7.x86_64.rpm 配置 echo \"default-storage-engine=INNODB\" >>/etc/my.cnf echo \"character-set-server=utf8\" >>/etc/my.cnf echo \"collation-server=utf8_general_ci\" >>/etc/my.cnf echo \"lower_case_table_names=1\" >>/etc/my.cnf 启动 centos7 systemctl daemon-reload systemctl enable mysqld --now centos6 service mysqld start chkconfig mysqld on 修改防火墙、SElinux策略 firewall-cmd --permanent --zone=public --add-port=3306/tcp firewall-cmd --reload setenforce 0 修改root密码 password=`grep 'temporary password' /var/log/mysqld.log|awk '{print $NF}'|awk 'END {print}'` mysql -uroot -p$password --connect-expired-password 添加slave节点 1、确认主节点版本 2、从节点安装相同版本mysql 3、更换默认存储目录（可选） systemctl stop mysqld.service mkdir -p /data/mysql chown -R mysql.mysql /data/mysql cp -a /var/lib/mysql/* /data/mysql/ sed -i \"s#/var/lib/mysql#/data/mysql#g\" /etc/my.cnf cat >> /etc/my.cnf 4、初始化密码 password=`grep 'temporary password' /var/log/mysqld.log|awk '{print $NF}'|awk 'END {print}'` mysql -uroot -p$password --connect-expired-password 5、调整主库参数 原有主库配置参数如下： server-id = 1 #id要唯一 log-bin = mysql-bin #开启binlog日志 auto-increment-increment = 1 #在Ubuntu系统中MySQL5.5以后已经默认是1 auto-increment-offset = 1 slave-skip-errors = all #跳过主从复制出现的错误 主库创建同步账号 grant all on *.* to 'sync'@'192.168.%.%' identified by 'sync'; 6、从库配置MySQL server-id = 3 #这个设置3 log-bin = mysql-bin #开启binlog日志 auto-increment-increment = 1 #这两个参数在Ubuntu系统中MySQL5.5以后都已经默认是1 auto-increment-offset = 1 slave-skip-errors = all #跳过主从复制出现的错误 update mysql.user set authentication_string=password('1qaz#EDC') where user='root'; mysqldump -h 192.168.174.30 -p3306 -uroot -p1qaz#EDC --all-databases > /root/all_db.sql cli命令 查看连接 查看当前连接数 show status like 'Threads%'; 查看最大连接数 show variables like '%max_connections%'; 查看显示连接状态 SHOW STATUS LIKE '%connect%'; 查看当前所有连接 show full processlist; 配置优化 参考地址 连接数 查看最大连接数，默认151 show VARIABLES like 'max_connections'; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | max_connections | 151 | +-----------------+-------+ 1 row in set (0.00 sec) 查看当前连接数 SHOW STATUS LIKE 'max_used_connections'; # 理想值约等于85% max_used_connections/max_connections*100% 配置方式 #客户端命令行 set GLOBAL max_connections=2000; set GLOBAL max_user_connections=1500; #配置文件 [mysqld] max_connections = 1000 max_user_connections=1500 暂存连接数 MySQL能够暂存的连接数量。 当主要MySQL线程在一个很短时间内得到非常多的连接请求，他就会起作用。 如果MySQL的连接数据达到max_connections时，新的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈数量即back_log，如果等待连接的数量超过back_log，将不被接受连接资源。 show VARIABLES like 'back_log'; mysql> show VARIABLES like 'back_log'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | back_log | 80 | +---------------+-------+ 1 row in set (0.00 sec) back_log值不能超过TCP/IP连接的侦听队列的大小。若超过则无效，查看当前系统的TCP/IP连接的侦听队列的大小命令（默认128） cat /proc/sys/net/ipv4/tcp_max_syn_backlog 配置方式 echo \"net.ipv4.tcp_max_syn_backlog = 8192\" >> /etc/sysctl.conf sysctl -p #配置文件 [mysqld] back_log=128 缓冲区变量 1、key_buffer_size 默认的配置数时8388608（8M），主机有4G内存可以调优值为268435456（256M） 通过检查状态值 key_read_requests和key_reads，可以知道key_buffer_size设置是否合理。 比例key_reads/key_read_requests应该尽可能的低，至少是1：100，1：1000更好（上述状态值可以使用show status like ‘key_read%'获得） mysql> show variables like 'key_buffer_size'; +-----------------+---------+ | Variable_name | Value | +-----------------+---------+ | key_buffer_size | 8388608 | +-----------------+---------+ 1 row in set (0.00 sec) mysql> show status like 'key_read%'; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Key_read_requests | 33 | | Key_reads | 8 | +-------------------+-------+ 2 rows in set (0.00 sec) set global key_buffer_size = 256*1024*1024; 2、query_cache_size 使用查询缓存，MySQL将查询结果存放在缓冲区中，今后对同样的select语句（区分大小写），将直接从缓冲区中读取结果。 一个SQL查询如果以select开头，那么MySQL服务器将尝试对其使用查询缓存。 注：两个SQL语句，只要相差哪怕是一个字符（例如 大小写不一样：多一个空格等），那么两个SQL将使用不同的cache 通过 show status like 'Qcache%'; 可以知道query_cache_size的设置是否合理 mysql> show status like 'Qcache%'; +-------------------------+---------+ | Variable_name | Value | +-------------------------+---------+ | Qcache_free_blocks | 1 | | Qcache_free_memory | 1031832 | | Qcache_hits | 0 | | Qcache_inserts | 0 | | Qcache_lowmem_prunes | 0 | | Qcache_not_cached | 9 | | Qcache_queries_in_cache | 0 | | Qcache_total_blocks | 1 | +-------------------------+---------+ 8 rows in set (0.00 sec) 3、sort_buffer_size 每个需要排序的线程分配该大小的一个缓冲区。增加这值加速ORDER BY 或 GROUP BY操作 sort_buffer_size是一个connection级的参数，在每个connection（session）第一次需要使用这个buffer的时候，一次性分配设置的内存。 sort_buffer_size并不是越大越好，由于是connection级的参数，过大的设置+高并发可能会耗尽系统的内存资源。例如：500个连接将会消耗500*sort_buffer_size(2M)=1G 默认0.25M set global sort_buffer_size = 1 *1024 * 1024; join_buffer_size 用于表示关联缓存的大小，和sort_buffer_size一样，该参数对应的分配内存也是每个连接独享。 set global join_buffer_size = 1 *1024 * 1024; 4、thread_cache_size 服务器线程缓存，这个值表示可以重新利用保存在缓存中的线程数量， 当断开连接时，那么客户端的线程将被放到缓存中以响应下一个客户而不是销毁（前提时缓存数未达上限），如果线程重新被请求，那么请求将从缓存中读取，如果缓存中是空的或者是新的请求，这个线程将被重新请求，那么这个线程将被重新创建，如果有很多新的线程，增加这个值可以改善系统性能，通过比较Connections和Threads_created状态的变量，可以看到这个变量的作用。 默认9 set global thread_cache_size = 100; 可以通过如下几个MySQL状态值来适当调整线程池的大小 Threads_cached : 当前线程池中缓存有多少空闲线程 Threads_connected : 当前的连接数 ( 也就是线程数 ) Threads_created : 已经创建的线程总数 Threads_running : 当前激活的线程数 ( Threads_connected 中的线程有些可能处于休眠状态 ) 可以通过 show global status like 'Threads_%'; 命令查看以上4个状态值 防止暴力破解 max_connect_errors 是一个MySQL中与安全有关的计数器值，他负责阻止过多尝试失败的客户端以防止暴力破解密码的情况， 当超过指定次数，MySQL服务器将禁止host的连接请求，直到mysql服务器重启或通过flush hosts命令清空此host的相关信息。 set global max_connect_errors = 20; 限制数据包大小 限制server接受的数据包大小，默认4M。 mysql> show VARIABLES like 'max_allowed_packet'; +--------------------+---------+ | Variable_name | Value | +--------------------+---------+ | max_allowed_packet | 4194304 | +--------------------+---------+ 1 row in set (0.00 sec) set global max_allowed_packet = 32*1024*1024; 使用教程 使用教程1 使用教程2 使用教程3 使用教程4 使用教程5 使用教程6 慢查询 查看查询慢sql配置 show variables like 'slow%'; 开启慢sql set global slow_query_log='ON' 查询慢 SQL 秒数值 show variables like 'long%'; 查看变量 #该语句输出较多 SHOW VARIABLES; SHOW VARIABLES like 'version'; 查看锁性能 锁性能状态： SHOW STATUS LIKE 'innodb_row_lock_%'; mysql> SHOW STATUS LIKE 'innodb_row_lock_%'; +-------------------------------+--------+ | Variable_name | Value | +-------------------------------+--------+ | Innodb_row_lock_current_waits | 0 | | Innodb_row_lock_time | 497180 | | Innodb_row_lock_time_avg | 4075 | | Innodb_row_lock_time_max | 51006 | | Innodb_row_lock_waits | 122 | +-------------------------------+--------+ 5 rows in set (0.00 sec) Innodb_row_lock_current_waits：当前等待锁的数量 Innodb_row_lock_time：系统启动到现在、锁定的总时间长度 Innodb_row_lock_time_avg：每次平均锁定的时间 Innodb_row_lock_time_max：最长一次锁定时间 Innodb_row_lock_waits：系统启动到现在、总共锁定次数 查看连接数 mysql> SHOW STATUS LIKE 'max_used_connections'; +----------------------+-------+ | Variable_name | Value | +----------------------+-------+ | Max_used_connections | 86 | +----------------------+-------+ 1 row in set (0.02 sec) mysql> 查看回滚数量 如果 rollback 过多，说明程序肯定哪里存在问题 SHOW STATUS LIKE '%Com_rollback%'; 查询运行时间 显示MySQL服务启动运行了多少时间，如果MySQL服务重启，该时间重新计算，单位秒 SHOW STATUS LIKE 'uptime'; 查询缓存状态 显示查询缓存的状态情况 SHOW STATUS LIKE 'qcache%'; 查看连接信息 例子出处 SHOW FULL PROCESSLIST; #输出如下 mysql> show processlist; +----+------+----------------------+---------+---------+------+-------+------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+------+----------------------+---------+---------+------+-------+------------------+ | 1 | root | 192.168.20.160:53417 | firefly | Sleep | 50 | | NULL | | 2 | root | localhost | NULL | Query | 0 | init | show processlist | +----+------+----------------------+---------+---------+------+-------+------------------+ 2 rows in set (0.00 sec) mysql> show processlist; +----+------+----------------------+---------+---------+------+--------------+---------------------+ | Id | User | Host | db | Command | Time | State | Info | +----+------+----------------------+---------+---------+------+--------------+---------------------+ | 1 | root | 192.168.20.160:53417 | firefly | Query | 125 | Sending data | SELECT o.order_id, creator_id, '', city_name, order_address, city_id, order_type_description, | | 2 | root | localhost | NULL | Query | 0 | init | show processlist | +----+------+----------------------+---------+---------+------+--------------+-------------------+ 2 rows in set (0.00 sec) id：标识 user：当前用户，如果不是root，这个命令就只显示你权限范围内的sql语句 host：显示执行sql语句的ip地址和端口号，追踪出问题语句的用户 db：显示这个进程目前连接的是哪个数据库 command：显示当前连接的执行的命令，一般就是休眠（sleep），查询（query），连接（connect） time：状态持续的时间，单位是秒。 state，使用当前连接的sql语句的状态，很重要的列。 注意，state只是语句执行中的某一个状态，一个sql语句，已查询为例，可能需要经过copying to tmp table，Sorting result，Sending data等状态才可以完成 info：显示执行的sql语句，因为长度有限，所以长的sql语句就显示不全，但是，是一个判断问题语句的重要依据。 state列 这个命令中最关键的就是state列，mysql列出的状态主要有以下几种，所有状态参考下面官方手册： Checking table 正在检查数据表（这是自动的）。 Closing tables 正在将表中修改的数据刷新到磁盘中，同时正在关闭已经用完的表。这是一个很快的操作，如果不是这样的话，就应该确认磁盘空间是否已经满了或者磁盘是否正处于重负中。 Connect Out 复制从服务器正在连接主服务器。 Copying to tmp table on disk 由于临时结果集大于tmp_table_size，正在将临时表从内存存储转为磁盘存储以此节省内存。 Creating tmp table 正在创建临时表以存放部分查询结果。 deleting from main table 服务器正在执行多表删除中的第一部分，刚删除第一个表。 deleting from reference tables 服务器正在执行多表删除中的第二部分，正在删除其他表的记录。 Flushing tables 正在执行FLUSH TABLES，等待其他线程关闭数据表。 Killed 发送了一个kill请求给某线程，那么这个线程将会检查kill标志位，同时会放弃下一个kill请求。MySQL会在每次的主循环中检查kill标志位，不过有些情况下该线程可能会过一小段才能死掉。如果该线程程被其他线程锁住了，那么kill请求会在锁释放时马上生效。 Locked 被其他查询锁住了。 Sending data 正在处理Select查询的记录，同时正在把结果发送给客户端。 Sorting for group 正在为GROUP BY做排序。 Sorting for order 正在为ORDER BY做排序。 Opening tables 这个过程应该会很快，除非受到其他因素的干扰。例如，在执Alter TABLE或LOCK TABLE语句行完以前，数据表无法被其他线程打开。正尝试打开一个表。 Removing duplicates 正在执行一个Select DISTINCT方式的查询，但是MySQL无法在前一个阶段优化掉那些重复的记录。因此，MySQL需要再次去掉重复的记录，然后再把结果发送给客户端。 Reopen table 获得了对一个表的锁，但是必须在表结构修改之后才能获得这个锁。已经释放锁，关闭数据表，正尝试重新打开数据表。 Repair by sorting 修复指令正在排序以创建索引。 Repair with keycache 修复指令正在利用索引缓存一个一个地创建新索引。它会比Repair by sorting慢些。 Searching rows for update 正在讲符合条件的记录找出来以备更新。它必须在Update要修改相关的记录之前就完成了。 Sleeping 正在等待客户端发送新请求. System lock 正在等待取得一个外部的系统锁。如果当前没有运行多个mysqld服务器同时请求同一个表，那么可以通过增加--skip-external-locking参数来禁止外部系统锁。 Upgrading lock Insert DELAYED正在尝试取得一个锁表以插入新记录。 Updating 正在搜索匹配的记录，并且修改它们。 User Lock 正在等待GET_LOCK()。 Waiting for tables 该线程得到通知，数据表结构已经被修改了，需要重新打开数据表以取得新的结构。然后，为了能的重新打开数据表，必须等到所有其他线程关闭这个表。以下几种情况下会产生这个通知：FLUSH TABLES tbl_name, Alter TABLE, RENAME TABLE, REPAIR TABLE, ANALYZE TABLE,或OPTIMIZE TABLE。 waiting for handler insert Insert DELAYED已经处理完了所有待处理的插入操作，正在等待新的请求。 大部分状态对应很快的操作，只要有一个线程保持同一个状态好几秒钟，那么可能是有问题发生了，需要检查一下。 还有其他的状态没在上面中列出来，不过它们大部分只是在查看服务器是否有存在错误是才用得着 查询表使用状态 查询哪些表在被使用，是否有锁表： SHOW OPEN TABLES WHERE In_use > 0; 查看增删改数量 查询当前MySQL中查询、更新、删除执行多少条了，可以通过这个来判断系统是侧重于读还是侧重于写，如果是写要考虑使用读写分离。 SHOW STATUS LIKE '%Com_select%'; SHOW STATUS LIKE '%Com_insert%'; SHOW STATUS LIKE '%Com_update%'; SHOW STATUS LIKE '%Com_delete%'; 修改密码 use mysql update user set authentication_string=password('1qaz#EDC') where user='root'; flush privileges; binlog 查看binlog保存天数 默认值为0，即永久保存 show variables like 'expire_logs_days'; 配置binlog失效时间 set global expire_logs_days=7; 清理binlog flush logs; 清除指定时间的binlog purge binary logs before '2017-05-01 13:09:51'; Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/nginx/":{"url":"3.集成部署/nginx/","title":"nginx","keywords":"","body":" Table of Contents generated with DocToc 安装部署 使用源码编译安装 使用预编译包安装 配置调优 安全加固 相关文档 keepalive及444状态码 nginx location匹配顺序 nginx重定向 nginx http请求处理流程 配置为系统服务 生成文件 tee /usr/lib/systemd/system/nginx.service 启动 systemctl daemon-reload systemctl enable nginx --now 查看nginx最大连接数 grep 'open files' /proc/$( cat /var/run/nginx.pid )/limits nginx.conf最大连接数配置 worker_rlimit_nofile 65535; events { use epoll; worker_connections 65535; } 安装部署 使用源码编译安装 灵活性最高 官网下载地址 自用开源项目 使用预编译包安装 参考地址 RHEL7 or CentOS 7 From EPEL # Install epel repository: yum install epel-release # or alternative: # wget -c https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # yum install epel-release-latest-7.noarch.rpm # Install NGINX: yum install nginx From Software Collections # Install and enable scl: yum install centos-release-scl yum-config-manager --enable rhel-server-rhscl-7-rpms # Install NGINX (rh-nginx14, rh-nginx16, rh-nginx18): yum install rh-nginx16 # Enable NGINX from SCL: scl enable rh-nginx16 bash From Official Repository # Where: # - is: rhel or centos cat > /etc/yum.repos.d/nginx.repo /$releasever/$basearch/ gpgcheck=0 enabled=1 __EOF__ # Install NGINX: yum install nginx Debian or Ubuntu From Debian/Ubuntu Repository # Install NGINX: apt-get install nginx From Official Repository # Where: # - is: debian or ubuntu # - is: xenial, bionic, jessie, stretch or other cat > /etc/apt/sources.list.d/nginx.list / nginx deb-src http://nginx.org/packages// nginx __EOF__ # Update packages list: apt-get update # Download the public key (or from your GPG error): apt-key adv --keyserver keyserver.ubuntu.com --recv-keys # Install NGINX: apt-get update apt-get install nginx 配置调优 nginx管理员手册 参考地址 绑定cpu worker_processes 2; worker_cpu_affinity 01 10; #2核CPU,开启4个进程 worker_processes 4; worker_cpu_affinity 01 10 01 10; #4核CPU，开户4个进程 worker_processes 4; worker_cpu_affinity 0001 0010 0100 1000; #8核 worker_processes 8; worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000; 打开文件数 #与ulimit -n一致 worker_rlimit_nofile 655350; 每个进程允许的最多连接数 #ulimit -n / worker数量 worker_connections 102400; 请求头部的缓冲区大小 #与系统分页大小一致（getconf PAGESIZE获取分页大小） client_header_buffer_size 4k; 提高文件传输性能 #开启高效文件传输模式，sendfile 指令指定 Nginx 是否调用sendfile 函数来输出文件， #对于普通应用设为 on，如果用来进行下载等应用磁盘 IO 重负载应用，可设置为 off， #以平衡磁盘与网络 I/O 处理速度，降低系统的负载。 sendfile on; 小的数据包不等待直接传输 tcp_nodelay on; 开启gzip压缩 gzip on; gzip_min_length 1100; #对数据启用压缩的最少字节数,如:请求小于1K文件,不要压缩,压缩小数据会降低处理此请求的所有进程速度 gzip_buffers 4 16k; gzip_proxied any; #允许或者禁止压缩基于请求和响应的响应流,若设置为any,将会压缩所有请求 gzip_http_version 1.0; gzip_comp_level 9; #gzip压缩等级在0-9内,数值越大压缩率越高,CPU消耗也就越大 gzip_types text/plain text/css application/javascript application/x-javascript text/xml application/xml application/xml+rss text/javascript application/json image/jpeg image/gif image/png; #压缩类型 gzip_vary on; #varyheader支持,让前端的缓存服务器识别压缩后的文件,代理 open_file_cache max=65535 inactive=20s; #这个将为打开文件指定缓存,max 指定缓存数量.建议和打开文件数一致.inactive 是指经过多长时间文件没被请求后删除缓存 open_file_cache_valid 30s; #这个是指多长时间检查一次缓存的有效信息,例如我一直访问这个文件,30秒后检查是否更新,反之更新 open_file_cache_min_uses 2; #定义了open_file_cache中指令参数不活动时间期间里最小的文件数 open_file_cache_errors on; #NGINX可以缓存在文件访问期间发生的错误,这需要设置该值才能有效,如果启用错误缓存.则在访问资源（不查找资源）时.NGINX会报告相同的错误 关闭404日志记录 log_not_found off; 安全加固 隐藏版本号 server_tokens off; 修改server名 more_set_headers \"Server: web\"; 配置host白名单 #结合开源项目，定期更新IP黑名单 项目地址 #结合lua维护白名单 -- 校验Host合法性 function host_check(host) if tableFind(host,valid_hosts) == false then record_attack_log(\"BadHost\") ngx.exit(444) end end 配置refer黑白名单 #结合开源项目，定期更新refer黑名单 项目地址 #结合lua维护白名单 -- 校验Referer合法性 function referer_check(refer,host) if refer ~= nil and string.find(refer,host) == nil then if tableFind(refer,valid_referers) == false then record_attack_log(\"BadRefer\") ngx.exit(444) end end end 拦截sql注入 naxsi 结合naxsi模块 拦截XSS攻击 naxsi 结合naxsi模块 SSL/TLS ssl_protocols TLSv1.3 TLSv1.2; 使用tls时关闭gzip Some attacks are possible (e.g. the real BREACH attack is a complicated) because of gzip (HTTP compression not TLS compression) being enabled on SSL requests. In most cases, the best action is to simply disable gzip for SSL. gzip off; 降低XSS劫持配置 add_header Content-Security-Policy \"default-src 'none'; script-src 'self'; connect-src 'self'; img-src 'self'; style-src 'self';\" always; add_header X-XSS-Protection \"1; mode=block\" always; 配置Referrer-Policy refer介绍 https://scotthelme.co.uk/a-new-security-header-referrer-policy/ #http请求分为请求行，请求头以及请求体，而请求头又分为general，request headers，此字段设置与general中，用来约定request headers中的referer #任何情况下都不发送referer add_header Referrer-Policy \"origin\"; 可选值 \"no-referrer\", #任何情况下都不发送referer \"no-referrer-when-downgrade\", #在同等安全等级下（例如https页面请求https地址），发送referer，但当请求方低于发送方（例如https页面请求http地址），不发送referer \"same-origin\", #当双方origin相同时发送 \"origin\", #仅仅发送origin，即protocal+host \"strict-origin\", #当双方origin相同且安全等级相同时发送 \"origin-when-cross-origin\", #跨域时发送origin \"strict-origin-when-cross-origin\", \"unsafe-url\" #任何情况下都显示完整的referer 配置X-Frame-Option add_header X-Frame-Options \"SAMEORIGIN\" always; 配置Feature-Policy Feature Policy是一个新的http响应头属性，允许一个站点开启或者禁止一些浏览器属性和API，来更好的确保站点的安全性和隐私性。 可以严格的限制站点允许使用的属性是很愉快的，而可以对内嵌在站点中的iframe进行限制则更加增加了站点的安全性。 W3C标准 https://w3c.github.io/webappsec-feature-policy/ add_header Feature-Policy \"geolocation 'none'; midi 'none'; notifications 'none'; push 'none'; sync-xhr 'none'; microphone 'none'; camera 'none'; magnetometer 'none'; gyroscope 'none'; speaker 'none'; vibrate 'none'; fullscreen 'none'; payment 'none'; usb 'none';\"; 相关文档 keepalive及444状态码 keepalive 该配置官方文档给出的默认值为75s 官方文档地址 1、nginx keepalive配置方便起见配置为30s #配置于nginx.conf 中的 http{}内 keepalive_timeout 30s; 2、nginx server配置 server { listen 8089; location /123 { proxy_pass http://192.168.1.145:8080; } location / { index html/index.html; } } 3、开启wireshark监听虚拟网卡（nginx部署于本地vmware上的虚机，nat模式） 4、使用POSTMAN发送请求 5、wireshark过滤观察 keepalive与断开连接 444状态码 适用于屏蔽非安全请求或DDOS防御 1、nginx server配置 server { listen 8089; location /123 { proxy_pass http://192.168.1.145:8080; } location / { index html/index.html; } location /abc { return 444; } } 2、开启wireshark监听虚拟网卡（nginx部署于本地vmware上的虚机，nat模式） 3、发送请求 4、wireshark过滤观察 nginx http请求处理流程 参考文章 https://github.com/trimstray/nginx-admins-handbook#introduction https://blog.51cto.com/wenxi123/2296295?source=dra nginx处理一个请求共分为11个阶段 阶段一，NGX_HTTP_POST_READ_PHASE 获取请求头信息 #相关模块: ngx_http_realip_module 阶段二，NGX_HTTP_SERVER_REWRITE_PHASE 实现在server{}块中定义的重写指令: 使用PCRE正则表达式更改请求uri，返回重定向uri； #相关模块: ngx_http_rewrite_module 阶段三，NGX_HTTP_FIND_CONFIG_PHASE **仅nginx核心模块可以参与** 根据阶段二的uri匹配location 阶段四，NGX_HTTP_REWRITE_PHASE 由阶段三匹配到location，并在location{}块中再次进行uri转换 #相关模块: ngx_http_rewrite_module 阶段五，NGX_HTTP_POST_REWRITE_PHASE **仅nginx核心模块可以参与** 请求地址重写提交阶段，防止递归修改uri造成死循环，（一个请求执行10次就会被nginx认定为死循环） #相关模块: ngx_http_rewrite_module 阶段六，NGX_HTTP_PREACCESS_PHASE 访问控制阶段一： 验证预处理请求限制，访问频率、连接数限制（访问限制） #相关模块：ngx_http_limit_req_module, ngx_http_limit_conn_module, ngx_http_realip_module 阶段七，NGX_HTTP_ACCESS_PHASE 访问控制阶段二： 客户端验证(源IP是否合法，是否通过HTTP认证) #相关模块：ngx_http_access_module, ngx_http_auth_basic_module 阶段八，NGX_HTTP_POST_ACCESS_PHASE **仅nginx核心模块可以参与** 访问控制阶段三： 访问权限检查提交阶段；如果请求不被允许访问nginx服务器，该阶段负责向用户返回错误响应； #相关模块：ngx_http_access_module, ngx_http_auth_basic_module 阶段九，NGX_HTTP_PRECONTENT_PHASE **仅nginx核心模块可以参与** 如果http请求访问静态文件资源，try_files配置项可以使这个请求顺序地访问多个静态文件资源，直到某个静态文件资源符合选取条件 #相关模块：ngx_http_try_files_module 阶段十，NGX_HTTP_CONTENT_PHASE 内容产生阶段，大部分HTTP模块会介入该阶段，是所有请求处理阶段中最重要的阶段，因为这个阶段的指令通常是用来生成HTTP响应内容的； #相关模块：ngx_http_index_module, ngx_http_autoindex_module, ngx_http_gzip_module 阶段十一，NGX_HTTP_LOG_PHASE 记录日志阶段 #相关模块：ngx_http_log_module 示例图 安全配置 more_set_headers \"X-Content-Type-Options nosniff\";more_set_headers \"Content-Security-Policy frame-ancestors http://hs.192.168.174.71.nip.io:30933 http://tpd.10.9.20.42.nip.io:30750;default-src *;style-src 'self' http://192.168.174.69:30459 http://192.168.174.106:8088 'unsafe-inline';script-src 'self' http://192.168.174.69:30459 http://192.168.174.106:8088 'unsafe-inline' 'unsafe-eval';img-src * data:;worker-src * blob:;font-src 'self' http://192.168.174.69:30459 http://192.168.174.106:8088 http://at.alicdn.com data:;\"; more_set_headers \"Access-Control-Allow-Origin http://192.168.174.69:30459\";more_set_headers \"Access-Control-Allow-Methods GET, POST, PUT, DELETE, OPTIONS\"; Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/nginx/配置pdf预览.html":{"url":"3.集成部署/nginx/配置pdf预览.html","title":"配置pdf预览","keywords":"","body":"default_type application/pdf; location = /pdf_carpet { alias /var/www/html/pdf_carpet/file.pdf; default_type application/pdf; add_header Content-Disposition 'inline'; } 如果访问 PDF 文件的 URI 以斜杠结尾(或者它是一个特殊情况的根 URI),则上述配置将不起作用,因为 nginx 会将索引文件名附加到这样的 URI(使location = /path/ { ... }不匹配$uri内部 nginx 变量)。对于这种情况,可以使用另一种技术: location = / { root /var/www/html/pdf_carpet; rewrite ^ /file.pdf break; add_header Content-Disposition 'inline'; } 源：https://devpress.csdn.net/cloud/630546977e6682346619dd7e.html Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/nodejs/":{"url":"3.集成部署/nodejs/","title":"nodejs","keywords":"","body":"nodejs环境安装 下载 curl -O https://nodejs.org/dist/v12.14.0/node-v12.14.0-linux-x64.tar.xz 解压 sudo tar xvf node-v12.14.0-linux-x64.tar.xz -C /usr/local/ 加入PATH echo \"export PATH=\\$PATH:/usr/local/node-v12.14.0-linux-x64/bin/\" >> ~/.bashrc . ~/.bashrc 修改镜像源 npm config set registry https://registry.npm.taobao.org Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/ntp/":{"url":"3.集成部署/ntp/","title":"ntp","keywords":"","body":"ntp yum -y install ntpdate /usr/sbin/ntpdate ntp1.aliyun.com echo \"*/5 * * * * /usr/sbin/ntpdate ntp1.aliyun.com\" >> /etc/crontab ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/openjdk/":{"url":"3.集成部署/openjdk/","title":"openjdk","keywords":"","body":"openjdk yum install -y java-1.8.0-openjdk.x86_64 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/oracle/":{"url":"3.集成部署/oracle/","title":"oracle","keywords":"","body":" oracle 安装条件检测 安装oracle单机 docker启动oracle oracle 安装条件检测 以下内容仅为官网要求部分摘抄，详细环境要求如下 https://docs.oracle.com/en/database/oracle/oracle-database/12.2/ladbi/oracle-database-installation-checklist.html#GUID-E847221C-1406-4B6D-8666-479DB6BDB046 硬件要求 1、DVD光驱（如果采用DVD光盘安装） 2、linux系统运行级别为3或5 3、显卡分辨率最低1024 x 768（Oracle Universal Installer图形化安装需要） 4、oracle宿主机联网（拥有网络适配器） 5、最小1g内存，建议2g #Linux系统有7个运行级别(runlevel) 运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11控制台，登陆后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 #查看当前系统运行级别(即多用户级别) systemctl get-default 或 runlevel 运行级别的原理： 1。在目录/etc/rc.d/init.d下有许多服务器脚本程序，一般称为服务(service) 2。在/etc/rc.d下有7个名为rcN.d的目录，对应系统的7个运行级别 3。rcN.d目录下都是一些符号链接文件，这些链接文件都指向init.d目录下的service脚本文件，命名规则为K+nn+服务名或S+nn+服务名，其中nn为两位数字。 4。系统会根据指定的运行级别进入对应的rcN.d目录，并按照文件名顺序检索目录下的链接文件 对于以K开头的文件，系统将终止对应的服务 对于以S开头的文件，系统将启动对应的服务 5。查看运行级别用：runlevel 6。进入其它运行级别用：init N 7。另外init0为关机，init 6为重启系统 操作系统要求 1、安装OpenSSH服务 2、针对X86-64系统内核支持 Oracle Linux 7 with the Unbreakable Enterprise Kernel 3: 3.8.13-35.3.1.el7uek.x86_64 or later Oracle Linux 7.2 with the Unbreakable Enterprise Kernel 4: 4.1.12-32.2.3.el7uek.x86_64 or later Oracle Linux 7 with the Red Hat Compatible kernel: 3.10.0-123.el7.x86_64 or later Red Hat Enterprise Linux 7: 3.10.0-123.el7.x86_64 or later Oracle Linux 6.4 with the Unbreakable Enterprise Kernel 2: 2.6.39-400.211.1.el6uek.x86_64or later Oracle Linux 6.6 with the Unbreakable Enterprise Kernel 3: 3.8.13-44.1.1.el6uek.x86_64 or later Oracle Linux 6.8 with the Unbreakable Enterprise Kernel 4: 4.1.12-37.6.2.el6uek.x86_64 or later Oracle Linux 6.4 with the Red Hat Compatible kernel: 2.6.32-358.el6.x86_64 or later Red Hat Enterprise Linux 6.4: 2.6.32-358.el6.x86_64 or later SUSE Linux Enterprise Server 12 SP1: 3.12.49-11.1 or later SUSE Linux Enterprise Server 15: 4.12.14-25-default or later Review the system requirements section for a list of minimum package requirements. 3、若宿主机操作系统为Oracle Linux，建议使用oracle预编译rpm包进行oracle环境初始化 宿主机配置要求 1、/tmp下只要1GB可用存储空间 2、交换区内存大小应满足以下要求 当物理内存在1GB与2GB之间，swap内存应为物理内存的1.5倍 当物理内存在2GB与16GB之间，swap内存应等于物理内存 当物理内存高于16GB，swap内存固定16G 需要注意的是，如果为Linux服务器启用了HugePages，那么在计算交换空间之前，应该从可用RAM中减去分配给HugePages的内存 3、oracle安装目录必须为ASCII字符 4、清除以下变量（如果当前主机安装过oracle，会存在以下变量） $ORACLE_HOME,$ORA_NLS10, $TNS_ADMIN, $ORACLE_BASE, $ORACLE_SID 5、使用root用户或具有root权限的用户（sudo）进行安装 宿主机存储空间要求 针对Linux x86-64: 单节点最低8.6 GB 企业版最低7.5 GB 安装oracle单机 参考地址 下载安装介质（迅雷下载） https://updates.oracle.com/Orion/Services/download/p13390677_112040_Linux-x86-64_1of7.zip?aru=16716375&patch_file=p13390677_112040_Linux-x86-64_1of7.zip https://updates.oracle.com/Orion/Services/download/p13390677_112040_Linux-x86-64_2of7.zip?aru=16716375&patch_file=p13390677_112040_Linux-x86-64_2of7.zip 安装依赖 yum -y install binutils compat-libstdc++-33 elfutils-libelf gcc gcc-c++ \\ glibc glibc-common glibc-devel glibc-headers ksh libaio libaio-devel \\ libgomp libgcc libstdc++ libstdc++-devel make sysstat unixODBC \\ unixODBC-devel numactl-devel kernel-headers glibc-headers \\ glibc-devel elfutils-libelf-devel pdksh readline-dev* libXp-* unzip perl psmisc 修改系统参数 HOSTNAME=ora11g echo \"$HOSTNAME\">/etc/hostname echo \"$(grep -E '127|::1' /etc/hosts)\">/etc/hosts echo \"$(ip a|grep \"inet \"|grep -v 127|awk -F'[ /]' '{print $6}') $HOSTNAME\">>/etc/hosts rm -rf /etc/systemd/system/default.target ln -s /lib/systemd/system/multi-user.target /etc/systemd/system/default.target setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config systemctl disable firewalld --now systemctl disable NetworkManager --now systemctl disable NetworkManager-dispatcher --now systemctl disable postfix --now 创建用户用户组 groupadd oinstall groupadd dba # 服务器一定要配置成新建用户同时新建用户家目录！！！ useradd -g oinstall -G dba oracle echo oracle|passwd --stdin oracle echo 'fs.suid_dumpable = 1'>>/etc/sysctl.conf echo 'fs.aio-max-nr = 1048576'>>/etc/sysctl.conf echo 'fs.file-max = 6815744'>>/etc/sysctl.conf echo 'kernel.shmmni = 4096'>>/etc/sysctl.conf echo 'kernel.shmmax = 1075267584'>>/etc/sysctl.conf echo 'kernel.shmall = 2097152'>>/etc/sysctl.conf echo 'kernel.sem = 250 32000 100 128'>>/etc/sysctl.conf echo 'net.ipv4.ip_local_port_range = 9000 65500'>>/etc/sysctl.conf echo 'net.core.rmem_default = 1048576'>>/etc/sysctl.conf echo 'net.core.rmem_max = 4194304'>>/etc/sysctl.conf echo 'net.core.wmem_default = 262144'>>/etc/sysctl.conf echo 'net.core.wmem_max = 1048586'>>/etc/sysctl.conf sysctl -p echo 'oracle soft nproc 2047'>>/etc/security/limits.conf echo 'oracle hard nproc 16384'>>/etc/security/limits.conf echo 'oracle soft nofile 4096'>>/etc/security/limits.conf echo 'oracle hard nofile 65536'>>/etc/security/limits.conf echo 'oracle soft stack 10240'>>/etc/security/limits.conf echo 'session required pam_limits.so'>>/etc/pam.d/login mkdir -p /u01/app/oracle/product/11.2.0/db_1 chown -R oracle:oinstall /u01 chmod -R 775 /u01 配置环境变量 cat >> /home/oracle/.bash_profile 上传安装介质至/tmp下,root执行 chown oracle:oinstall /tmp/p13390677_112040_Linux-x86-64_* 切换用户解压 su - oracle cd /tmp # linux安装zip unzip: yum install -y unzip zip; unzip p13390677_112040_Linux-x86-64_1of7.zip unzip p13390677_112040_Linux-x86-64_2of7.zip 创建配置文件 # 复制粘贴执行即可#############开始###################### cd database/ cat >>/tmp/database/response/install_11g.rsp 静默安装 # 很重要的一个过程,出现警告正常，出现FATAL不可继续执行！！！ # 出现FATAL,请检查tmp及其子目录是否有多余的一些垃圾文件 # 这个执行过程是很长的，一定要等待这一步完全执行完成，方可进行下一步操作，可以通过：ps -ef | grep oracle ,查看此命令的执行进程！！！！！ ./runInstaller -force -silent -responseFile /tmp/database/response/install_11g.rsp oracle用户执行 /u01/app/oracle/oraInventory/orainstRoot.sh /u01/app/oracle/product/11.2.0/db_1/root.sh oracle用户创建监听 cat >/u01/app/oracle/product/11.2.0/db_1/network/admin/listener.ora root执行建库脚本 mkdir -p /oradata/orcl chown -R oracle: /oradata su - oracle ###建库脚本，执行并挂到后台 vi /oradata/orcl/dbca.sh # 以下是脚本内容：**************************************** #!/bin/bash cat >>/home/oracle/init.ora>/etc/oratab /u01/app/oracle/product/11.2.0/db_1/bin/sqlplus /nolog 启动oracle sqlplus / as sysdba 调整配置 ALTER PROFILE DEFAULT LIMIT PASSWORD_LIFE_TIME UNLIMITED; #将默认的密码生存周期由180天改为无限制 alter system set audit_trail=none scope=spfile; shutdown immediate; #关闭默认库级审计 startup alter system set deferred_segment_creation=false; #关闭段延迟分配 ################################################### host mkdir -p /oradata/arch/orcl alter system set log_archive_format='arch_%t_%s_%r.arc' scope=spfile; alter system set log_archive_dest_10='location=/oradata/arch/orcl/' scope=spfile; shutdown immediate; startup mount; alter database archivelog; alter database open; alter system archive log current; alter system set control_file_record_keep_time=30; docker启动oracle 1.安装docker 2.拉群镜像 docker pull registry.cn-hangzhou.aliyuncs.com/helowin/oracle_11g 3.启动 docker run -itd -p 11521:1521 --restart=always --name oracle11g registry.cn-hangzhou.aliyuncs.com/helowin/oracle_11g Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/pdns/":{"url":"3.集成部署/pdns/","title":"pdns","keywords":"","body":"pdns 安装epel源 yum install -y epel-release 安装mariadb yum -y install mariadb mariadb-server 启动mariadb systemctl enable mariadb --now 配置mariadb mysql_secure_installation 依次输入以下内容 Enter current password for root (enter for none): -- 回车 Set root password? [Y/n] -- Y New password: -- 输入root口令，这里演示用设置为root Re-enter new password: -- 输入上一步设置的root口令进行确认 Remove anonymous users? [Y/n] -- 回车 Disallow root login remotely? [Y/n] -- 回车 Remove test database and access to it? [Y/n] -- 回车 Reload privilege tables now? [Y/n] -- 回车 修改mariadb字符集 修改服务端 sed -i \"s/\\[mysqld\\]/&\\ \\ninit_connect='SET collation_connection = utf8_unicode_ci'\\ \\ninit_connect='SET NAMES utf8'\\ \\ncharacter-set-server=utf8\\ \\ncollation-server=utf8_unicode_ci\\ \\nskip-character-set-client-handshake/\" /etc/my.cnf 修改客户端 sed -i \"s/\\[client\\]/&\\ndefault-character-set=utf8/\" /etc/my.cnf.d/client.cnf sed -i \"s/\\[mysql\\]/&\\ndefault-character-set=utf8/\" /etc/my.cnf.d/mysql-clients.cnf 重启mariadb systemctl restart mariadb 查看字符集 mysql -uroot -proot 输出如下： Variable_name Value character_set_client utf8 character_set_connection utf8 character_set_database utf8 character_set_filesystem binary character_set_results utf8 character_set_server utf8 character_set_system utf8 character_sets_dir /usr/share/mysql/charsets/ Variable_name Value collation_connection utf8_unicode_ci collation_database utf8_unicode_ci collation_server utf8_unicode_ci 创建pdns_db mysql -uroot -proot 创建pdns用户 mysql -uroot -proot 初始化数据 mysql -u root -proot poweradmin 安装pdns yum install -y pdns.x86_64 pdns-backend-mysql 配置pdns sed -i \"s#launch=bind#launch=gmysql\\ \\ngmysql-host=localhost\\ \\ngmysql-user=powerdns\\ \\ngmysql-dbname=pdns_db\\ \\ngmysql-password=powerdns#\" /etc/pdns/pdns.conf Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/poweradmin/":{"url":"3.集成部署/poweradmin/","title":"poweradmin","keywords":"","body":"poweradmin 下载poweradmin curl -L https://sourceforge.net/projects/poweradmin/files/poweradmin-2.1.7.tgz/download -o ./poweradmin-2.1.7.tgz 安装lnap yum install httpd php php-common php-curl php-devel php-gd php-pear php-imap php-mcrypt php-mhash php-mysql php-xmlrpc gettext -y 拷贝项目文件 tar xvf poweradmin-2.1.7.tgz cp -r poweradmin-2.1.7/* /var/www/html/ 启动apache systemctl enable httpd --now 启动powerdns systemctl enable pdns --now 配置poweradmin 浏览器访问：http://宿主机互联网IP:80/install 下一步 配置数据库信息(全为：poweradmin) 配置如下 下一步 配置/var/www/html/inc/config.inc.php vim /var/www/html/inc/config.inc.php 内容为web页面红框内容: 删除/var/www/html/install/ rm -rf /var/www/html/install/ 访问宿主机80端口，进行登录(admin/poweradmin) Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/python/":{"url":"3.集成部署/python/","title":"python","keywords":"","body":"Python3.6 yum install -y python36 python36-pip python36-devel Python3.7 1.下载源码包：https://www.python.org/ftp/python/3.9.18/Python-3.9.18.tgz 安装编译依赖 编译安装 yum -y install make gcc zlib-devel gcc-c++ openssl-devel Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/tomcat/":{"url":"3.集成部署/tomcat/","title":"tomcat","keywords":"","body":"修改tomcat8.5默认jvm内存参数 1.查看java进程jvm参数 $ jps -v 1633 Bootstrap -Djava.util.logging.config.file=/opt/apache-tomcat-8.5.71/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -Dcatalina.base=/opt/apache-tomcat-8.5.71 -Dcatalina.home=/opt/apache-tomcat-8.5.71 -Djava.io.tmpdir=/opt/apache-tomcat-8.5.71/temp 结果发现默认启动时，并没有配置jvm参数 2.查看java进程实际运行时内存大小 发现MaxHeapSize为4006.0MB $ jmap -heap 12206 Attaching to process ID 12206, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.262-b10 using thread-local object allocation. Parallel GC with 4 thread(s) Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 4200595456 (4006.0MB) NewSize = 88080384 (84.0MB) MaxNewSize = 1399848960 (1335.0MB) OldSize = 176160768 (168.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB) Heap Usage: PS Young Generation Eden Space: capacity = 1360003072 (1297.0MB) used = 945066744 (901.2858810424805MB) free = 414936328 (395.71411895751953MB) 69.49004479895763% used From Space: capacity = 18350080 (17.5MB) used = 13697696 (13.063140869140625MB) free = 4652384 (4.436859130859375MB) 74.64651925223214% used To Space: capacity = 19922944 (19.0MB) used = 0 (0.0MB) free = 19922944 (19.0MB) 0.0% used PS Old Generation capacity = 305135616 (291.0MB) used = 255110640 (243.29246520996094MB) free = 50024976 (47.70753479003906MB) 83.6056581477529% used 46774 interned Strings occupying 5194776 bytes. 3.调整jvm参数 vim apache-tomcat-8.5.71/bin/catalina.sh 在JAVA_OPTS=\"$JAVA_OPTS $JSSE_OPTS\"上面添加JAVA_OPTS参数，JAVA_OPTS参考值如下: 宿主机8G内存，并只运行一个java应用 JAVA_OPTS=\"-Dfile.encoding=UTF-8 -server -Xms6144m -Xmx6144m -XX:NewSize=1024m -XX:MaxNewSize=2048m -XX:PermSize=512m -XX:MaxPermSize=512m -XX:MaxTenuringThreshold=10 -XX:NewRatio=2 -XX:+DisableExplicitGC\" 宿主机16G内存，并只运行一个java应用 JAVA_OPTS=\"-Dfile.encoding=UTF-8 -server -Xms13312m -Xmx13312m -XX:NewSize=3072m -XX:MaxNewSize=4096m -XX:PermSize=512m -XX:MaxPermSize=512m -XX:MaxTenuringThreshold=10 -XX:NewRatio=2 -XX:+DisableExplicitGC\" 宿主机32G内存，并只运行一个java应用 JAVA_OPTS=\"-Dfile.encoding=UTF-8 -server -Xms29696m -Xmx29696m -XX:NewSize=6144m -XX:MaxNewSize=9216m -XX:PermSize=1024m -XX:MaxPermSize=1024m -XX:MaxTenuringThreshold=10 -XX:NewRatio=2 -XX:+DisableExplicitGC\" 参数说明： -Dfile.encoding：默认文件编码 -server：表示这是应用于服务器的配置，JVM 内部会有特殊处理的 -Xmx1024m：设置JVM最大可用内存为1024MB -Xms1024m：设置JVM最小内存为1024m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。 -XX:NewSize：设置年轻代大小 -XX:MaxNewSize：设置最大的年轻代大小 -XX:PermSize：设置永久代大小 -XX:MaxPermSize：设置最大永久代大小 -XX:NewRatio=4：设置年轻代（包括 Eden 和两个 Survivor 区）与终身代的比值（除去永久代）。设置为 4，则年轻代与终身代所占比值为 1：4，年轻代占整个堆栈的 1/5 -XX:MaxTenuringThreshold=10：设置垃圾最大年龄，默认为：15。如果设置为 0 的话，则年轻代对象不经过 Survivor 区，直接进入年老代。对于年老代比较多的应用，可以提高效率。 如果将此值设置为一个较大值，则年轻代对象会在 Survivor 区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。 -XX:+DisableExplicitGC：这个将会忽略手动调用 GC 的代码使得 System.gc() 的调用就会变成一个空调用，完全不会触发任何 GC 4.优雅下线应用 $ kill -15 24188 5.启动应用 $ sh bin/startup.sh 6.确认jvm参数 jps -v 8751 Bootstrap -Djava.util.logging.config.file=/opt/apache-tomcat-8.5.71/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Dfile.encoding=UTF-8 -Xms13312m -Xmx13312m -XX:NewSize=3072m -XX:MaxNewSize=4096m -XX:PermSize=512m -XX:MaxPermSize=512m -XX:MaxTenuringThreshold=10 -XX:NewRatio=2 -XX:+DisableExplicitGC -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -Dcatalina.base=/opt/apache-tomcat-8.5.71 -Dcatalina.home=/opt/apache-tomcat-8.5.71 -Djava.io.tmpdir=/opt/apache-tomcat-8.5.71/temp 7.查看应用运行时内存 $ jmap -heap 8751 Attaching to process ID 8751, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.262-b10 using thread-local object allocation. Parallel GC with 4 thread(s) Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 13958643712 (13312.0MB) NewSize = 4294967296 (4096.0MB) MaxNewSize = 4294967296 (4096.0MB) OldSize = 9663676416 (9216.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB) Heap Usage: PS Young Generation Eden Space: capacity = 3784835072 (3609.5MB) used = 2417960320 (2305.9466552734375MB) free = 1366874752 (1303.5533447265625MB) 63.8854870556431% used From Space: capacity = 261619712 (249.5MB) used = 11125784 (10.610374450683594MB) free = 250493928 (238.8896255493164MB) 4.252655090454346% used To Space: capacity = 248512512 (237.0MB) used = 0 (0.0MB) free = 248512512 (237.0MB) 0.0% used PS Old Generation capacity = 9663676416 (9216.0MB) used = 37241992 (35.51673126220703MB) free = 9626434424 (9180.483268737793MB) 0.38538119859165615% used 39540 interned Strings occupying 4308880 bytes. 修改tomcat默认停止脚本 有这样的一个现象：一个tomcat程序shutdown.sh后，并未完全退出 $ ps -ef|grep java root 1633 1 0 Sep23 ? 00:09:18 /usr/bin/java -Djava.util.logging.config.file=/opt/apache-tomcat-8.5.71/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /opt/apache-tomcat-8.5.71/bin/bootstrap.jar:/opt/apache-tomcat-8.5.71/bin/tomcat-juli.jar -Dcatalina.base=/opt/apache-tomcat-8.5.71 -Dcatalina.home=/opt/apache-tomcat-8.5.71 -Djava.io.tmpdir=/opt/apache-tomcat-8.5.71/temp org.apache.catalina.startup.Bootstrap start root 3059 1 0 Sep24 ? 00:15:43 /usr/bin/java -Djava.util.logging.config.file=/opt/apache-tomcat-8.5.71/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /opt/apache-tomcat-8.5.71/bin/bootstrap.jar:/opt/apache-tomcat-8.5.71/bin/tomcat-juli.jar -Dcatalina.base=/opt/apache-tomcat-8.5.71 -Dcatalina.home=/opt/apache-tomcat-8.5.71 -Djava.io.tmpdir=/opt/apache-tomcat-8.5.71/temp org.apache.catalina.startup.Bootstrap start root 12206 1 0 Sep27 ? 00:14:44 /usr/bin/java -Djava.util.logging.config.file=/opt/apache-tomcat-8.5.71/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /opt/apache-tomcat-8.5.71/bin/bootstrap.jar:/opt/apache-tomcat-8.5.71/bin/tomcat-juli.jar -Dcatalina.base=/opt/apache-tomcat-8.5.71 -Dcatalina.home=/opt/apache-tomcat-8.5.71 -Djava.io.tmpdir=/opt/apache-tomcat-8.5.71/temp org.apache.catalina.startup.Bootstrap start root 27186 24283 0 09:47 pts/0 00:00:00 grep --color=auto java 原因 一般关闭不了的情况，是由于程序在tomcat中开启了新的线程，而且未设置成daemon，造成的主线程不能退出 解决方式 vim bin/shutdown.sh，修改最后一行exec \"$PRGDIR\"/\"$EXECUTABLE\" stop \"$@\" 改为： exec \"$PRGDIR\"/\"$EXECUTABLE\" stop -force \"$@\" 参考 文档 tomcat关不掉的原因 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"3.集成部署/trafodion/":{"url":"3.集成部署/trafodion/","title":"trafodion","keywords":"","body":"trafodion CDH集群部署 主机列表 192.168.1.11 192.168.1.12 192.168.1.13 操作系统（必须） CentOS6 环境初始化 1.设置hostname，并配置host解析 192.168.1.11主机执行： cat >> /etc/sysconfig/network /proc/sys/kernel/hostname cat >> /etc/hosts 192.168.1.12主机执行： cat >> /etc/sysconfig/network /proc/sys/kernel/hostname cat >> /etc/hosts 192.168.1.13主机执行： cat >> /etc/sysconfig/network /proc/sys/kernel/hostname cat >> /etc/hosts 2.安装必要软件 yum install -y openssh-clients yum-utils createrepo yum-plugin-priorities 3.实现主机互信（hadoop1节点执行） hadoop1节点执行以下命令，生成ssh密钥 ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa 分发密钥完成互信（依次输入各节点密码） ssh-copy-id hadoop1 ssh-copy-id hadoop2 ssh-copy-id hadoop3 4.调整系统参数（三个节点均执行） # 调整文件句柄数 echo \"* soft nofile 655350\" >> /etc/security/limits.conf echo \"* hard nofile 655350\" >> /etc/security/limits.conf echo \"* soft nproc 65535\" >> /etc/security/limits.conf echo \"* hard nproc 65535\" >> /etc/security/limits.conf ulimit -n 655350 # 关闭防火墙 service iptables stop chkconfig iptables off # 关闭selinux setenforce 0 sed -i \"s#SELINUX=enforcing#SELINUX=disabled#g\" /etc/selinux/config 5.安装oracle jdk1.8（所有节点） 安装oracle jdk1.8,并配置软链接 mkdir -p /usr/java ln -s /opt/java /usr/java/jdk1.8 6.安装mysql，初始化用户 mysql部署文档 创建scm_server用户 mysql -uroot -proot 7.安装配置ntp hadoop1节点执行： yum install -y ntp service ntpd start chkconfig ntpd on sed -i \"s;restrict default kod nomodify notrap nopeer noquery;#restrict default kod nomodify notrap nopeer noquery;g\" /etc/ntp.conf sed -i \"s;restrict -6 default kod nomodify notrap nopeer noquery;#restrict -6 default kod nomodify notrap nopeer noquery;g\" /etc/ntp.conf sed -i \"s#restrict -6 ::1#restrict ::1#g\" /etc/ntp.conf sed -i \"s;server 0.centos.pool.ntp.org iburst;#server 0.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 1.centos.pool.ntp.org iburst;#server 1.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 2.centos.pool.ntp.org iburst;#server 2.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 3.centos.pool.ntp.org iburst;#server 3.centos.pool.ntp.org iburst;g\" /etc/ntp.conf echo \"server 127.127.1.0\" >> /etc/ntp.conf echo \"fudge 127.127.1.0 stratum 10\" >> /etc/ntp.conf echo \"disable monitor\" >> /etc/ntp.conf echo \"restrict default nomodify\" >> /etc/ntp.conf hadoop2、hadoop3节点执行： yum install -y ntp sed -i \"s;restrict default kod nomodify notrap nopeer noquery;#restrict default kod nomodify notrap nopeer noquery;g\" /etc/ntp.conf sed -i \"s;restrict -6 default kod nomodify notrap nopeer noquery;#restrict -6 default kod nomodify notrap nopeer noquery;g\" /etc/ntp.conf sed -i \"s#restrict -6 ::1#restrict ::1#g\" /etc/ntp.conf sed -i \"s;server 0.centos.pool.ntp.org iburst;#server 0.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 1.centos.pool.ntp.org iburst;#server 1.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 2.centos.pool.ntp.org iburst;#server 2.centos.pool.ntp.org iburst;g\" /etc/ntp.conf sed -i \"s;server 3.centos.pool.ntp.org iburst;#server 3.centos.pool.ntp.org iburst;g\" /etc/ntp.conf echo \"server hadoop1\" >> /etc/ntp.conf echo \"disable monitor\" >> /etc/ntp.conf echo \"restrict default nomodify\" >> /etc/ntp.conf ntpdate hadoop1 8.下载mysql驱动（hadoop1节点） 下载mysql-connector-java-5.1.48.tar.gz上传至hadoop1:/tmp下 执行以下命令，调整名称 mkdir -p /usr/share/java tar zxvf mysql-connector-java-5.1.48.tar.gz cp mysql-connector-java-5.1.48/mysql-connector-java-5.1.48.jar /usr/share/java/mysql-connector-java.jar 安装CDH5.4 1.创建离线镜像源目录（所有节点） mkdir -p /cm 下载cloudera-manager下的所有文件，并上传至/cm下 2.配置cm源（所有节点） 创建repo源文件 cd /cm && createrepo ./ cat > /etc/yum.repos.d/cm.repo /cm目录最终结构如下： /cm ├── cloudera-manager-agent-5.4.0-1.cm540.p0.165.el6.x86_64.rpm ├── cloudera-manager-daemons-5.4.0-1.cm540.p0.165.el6.x86_64.rpm ├── cloudera-manager-server-5.4.0-1.cm540.p0.165.el6.x86_64.rpm ├── cloudera-manager-server-db-2-5.4.0-1.cm540.p0.165.el6.x86_64.rpm ├── enterprise-debuginfo-5.4.0-1.cm540.p0.165.el6.x86_64.rpm ├── jdk-6u31-linux-amd64.rpm ├── oracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm ├── repodata │ ├── 3a8b6a8a03c3846eadd0f0d8df2ef1142e6e32d21ce7e4e58a304ad3bef8b5b7-primary.sqlite.bz2 │ ├── 853ca50d5d1f076b5f53cd06ed4d74c62ee729af1a86e3caa1bd39aaf6e68cf7-other.sqlite.bz2 │ ├── a6b67b1228bbb6791eb66fd52cfc2044a681a9444e1a1aa044111029b6f4760c-filelists.xml.gz │ ├── b05946fbbf3fec9e107640249a183e7109d0e336ef23fcbe199b3dd1743f84f3-other.xml.gz │ ├── c2c48ea8c58913116c14e8ec853d2fd2731bee779edafc81dec0d60771709f17-filelists.sqlite.bz2 │ ├── fd1f07dacbe9d5e3be1e7f7930fbb6eac4d29a75c172c2a31e6e18baa56b5fee-primary.xml.gz │ └── repomd.xml └── RPM-GPG-KEY-cloudera 3.安装cloudera-manager-server(hadoop1节点) yum install cloudera-manager-daemons cloudera-manager-server -y 4.安装cloudera-manager-agent(hadoop1、hadoop2、hadoop3节点) yum install cloudera-manager-agent -y 5.修改cloudera-manager-agent配置文件(hadoop1、hadoop2、hadoop3节点) hadoop1、hadoop2、hadoop3节点执行： sed -i \"s#server_host=localhost#server_host=hadoop1#g\" /etc/cloudera-scm-agent/config.ini echo \"listening_ip=`hostname`\" >> /etc/cloudera-scm-agent/config.ini echo \"listening_hostname=`hostname`\" >> /etc/cloudera-scm-agent/config.ini 6.创建CDH离线源仓储 创建目录(hadoop1节点) mkdir -p /opt/cloudera/parcel-repo 上传以下文件至hadoop1节点/opt/cloudera/parcel-repo下 manifest.json CDH-5.4.0-1.cdh5.4.0.p0.27-el6.parcel CDH-5.4.0-1.cdh5.4.0.p0.27-el6.parcel.sha1 调整CDH-5.4.0-1.cdh5.4.0.p0.27-el6.parcel.sha1名称 cd /opt/cloudera/parcel-repo mv CDH-5.4.0-1.cdh5.4.0.p0.27-el6.parcel.sha1 CDH-5.4.0-1.cdh5.4.0.p0.27-el6.parcel.sha 7.初始化CM Server数据库(hadoop1节点) /usr/share/cmf/schema/scm_prepare_database.sh mysql scm_server_db scm_server scm_server -h 127.0.0.1 8.启动cloudera-manager-agent（hadoop1、hadoop2、hadoop3节点） service cloudera-scm-agent start chkconfig cloudera-scm-agent on 9.启动cloudera-manager-server(hadoop1节点) service cloudera-scm-server start chkconfig cloudera-scm-server on 查看日志 tail -200f /var/log/cloudera-scm-server/cloudera-scm-server.log 10.访问控制台初始化 主要hadoop1替换为实际IP地址 访问http://hadoop1:7180，账号密码: admin/admin 部署免费版本 确认部署应用，点击继续 添加部署节点，点击继续 确认cdh版本，继续 确认安装完成，点击继续 确认检测结果，点击完成 选取hbase内核hadoop安装 默认角色分配，继续 输入用户名密码数据库实例名，测试连接后点击继续 hive/hive hive oozie/oozie oozie 确认审核设置，点击继续 安装完成 安装trafodion 主机列表 192.168.1.11 192.168.1.12 192.168.1.13 1.下载安装介质及脚本 互联网下载地址: installer trafodion_server 2.创建/trafodion目录，上传安装介质及脚本至该目录下 目录结构如下： /trafodion/ ├── apache-trafodion_installer-2.1.0-incubating.tar.gz └── apache-trafodion_server-2.1.0-RH6-x86_64-incubating.tar.gz 3.解压运行安装脚本 cd /trafodion tar zxvf apache-trafodion_pyinstaller-2.1.0-incubating.tar.gz cd python-installer ./db_install.py 4.按提示输入相关信息 Enter HDP/CDH web manager URL:port, (full URL, if no http/https prefix, default prefix is http://): -- 输入 http://192.168.1.11:7180 Enter HDP/CDH web manager user name [admin]: -- 回车默认 Enter HDP/CDH web manager user password: -- 输入 admin Confirm Enter HDP/CDH web manager user password: -- 输入 admin Enter full path to Trafodion tar file: -- 输入 /trafodion/apache-trafodion_server-2.1.0-RH6-x86_64-incubating.tar.gz Enter directory name to install trafodion to [apache-trafodion-2.1.0]: -- 回车默认 Enter trafodion user password: -- 输入 trafodion Enter number of DCS client connections per node [4]: -- 回车默认 Enter trafodion scratch file folder location(should be a large disk), if more than one folder, use comma seperated [$TRAF_HOME/tmp]: -- 回车默认 Start instance after installation (Y/N) [Y]: -- 回车默认 Enable LDAP security (Y/N) [N]: -- 回车默认 Enable DCS High Avalability (Y/N) [N]: -- 回车默认 Enter Hadoop admin password, default is [admin]: -- 回车默认 Confirm result (Y/N) [N]: -- 输入 Y 安装过程 安装完毕 5.查看trafodion状态 登录hadoop2，切换到trafodion用户执行以下语句： sqcheck 返回如下，说明成功 *** Checking Trafodion Environment *** Checking if processes are up. Checking attempt: 1; user specified max: 2. Execution time in seconds: 0. The Trafodion environment is up! Process Configured Actual Down ------- ---------- ------ ---- DTM 2 2 RMS 4 4 DcsMaster 1 1 DcsServer 2 2 mxosrvr 8 8 RestServer 1 1 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/argo/":{"url":"4.持续集成&交付/argo/","title":"argo","keywords":"","body":"Argo CD 简介 Argo CD是什么？ Argo CD是一个基于Kubernetes声明性的GitOps持续交付工具 为什么使用Argo CD 声明式定义应用程序、配置和环境，并且是版本控制的 应用程序部署和生命周期管理是自动化的、可审计的和易于理解的 工作原理 Argo CD遵循GitOps模式，使用Git存储库作为定义应用程序期望状态的数据源。 Kubernetes应用清单可以通过以下几种方式指定: kustomize应用 helm应用 ksonnet应用 jsonnet 带有yaml|json清单的目录 任意自定义配置管理工具|插件 Argo CD可以在指定的目标环境中自动部署、维护期望的应用程序状态，该期望状态由清单文件定义。 应用程序清单版本可以基于Git提交时跟踪对分支、tag或固定到特定版本的Git commit。 Argo CD基于kubernetes控制器实现，它持续监控运行中的应用程序， 并将当前的活动状态与期望的目标状态(如Git repo中所指定的)进行比较。 如果已部署的应用程序的活动状态偏离目标状态，则将被视为OutOfSync。 Argo CD可视化展现程序状态差异，同时提供自动或手动同步工具。 特性 将应用程序自动部署到指定的目标环境 支持多种应用配置管理工具/模板（Kustomize, Helm, Ksonnet, Jsonnet, plain-YAML） 能够管理和部署到多个k8s集群 单点登录（OIDC, OAuth2, LDAP, SAML 2.0, GitHub, GitLab, Microsoft, LinkedIn） 用于授权的多租户和RBAC策略 回滚至Git仓库中指定的commit 应用程序资源的运行状况分析 自动配置漂移检测和可视化 自动/手动同步应用至期望状态 提供应用程序活动的实时视图的Web UI 用于自动化和CI集成的CLI Webhook集成(GitHub, BitBucket, GitLab) PreSync, Sync, PostSync钩子来支持复杂应用(例如蓝/绿和金丝雀的升级) 应用程序事件审计和追踪API调用 Prometheus指标 覆盖Git中ksonnet/helm的参数 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/argo/01-核心概念.html":{"url":"4.持续集成&交付/argo/01-核心概念.html","title":"01-核心概念","keywords":"","body":"核心概念 argocd架构示意图 以下argocd概念需要具有Git、Docker、Kubernetes、Continuous Delivery和GitOps相关背景 Application(应用): 基于Kubernetes CRD定义的一组Kubernetes资源清单 应用数据源类型: 构建应用的工具类型(helm等) 目标状态: 描述应用的期望状态（如副本数、配额、调度等），由git仓库内的应用清单文件描述 活动状态: 描述应用的活动状态（如副本数、配额、调度、探针状态等） 同步状态: 描述应用活动状态与目标状态同步情况（是否一致） 同步: 一个动作，使应用程序（集群内）与目标状态（git仓库清单文件描述）达成一致 同步操作执行的状态: 描述同步动作是否成功 刷新: 对比git仓库内的应用目标状态与活动状态，指出不同之处 健康状态: 描述应用程序是否运行正常，可以对外提供服务 工具: 创建应用程序清单描述文件的工具（如helm、Kustomize） Argo CD中项目是什么？ 项目提供了应用程序的逻辑分组，这在Argo CD被多个团队使用时非常有用。项目提供以下特性: 限制部署的内容(如可Git源代码库) 限制应用部署的位置(目标k8s集群和命名空间) 限制可部署或不可部署的对象类型(例如RBAC、CRDs、daemonset、NetworkPolicy等) 定义项目角色以提供应用程序RBAC(绑定到OIDC组和/或JWT令牌) 关于默认项目 每个应用程序都属于一个项目。如果未指定，应用程序属于默认项目，该项目是自动创建的， 默认情况下允许从任何源repo部署到任何集群，以及所有资源类型。 默认业务群组只能被修改，不能被删除。最初创建时，它的规范声明如下: spec: sourceRepos: - '*' destinations: - namespace: '*' server: '*' clusterResourceWhitelist: - group: '*' kind: '*' Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/argo/02-部署argocd.html":{"url":"4.持续集成&交付/argo/02-部署argocd.html","title":"02-部署argocd","keywords":"","body":"部署argocd 1.下载声明文件 install.yaml 2.发布 修改文件内镜像引用tag [root@node1 ~]# grep \"image:\" install.sh image: ghcr.io/dexidp/dex:v2.27.0 image: quay.io/argoproj/argocd:v2.0.4 image: redis:6.2.4-alpine image: quay.io/argoproj/argocd:v2.0.4 image: quay.io/argoproj/argocd:v2.0.4 image: quay.io/argoproj/argocd:v2.0.4 发布创建 kubectl create namespace argocd kubectl apply -n argocd -f install.yaml 查看部署状态 [root@node1 ~]# kubectl get pod -n argocd -w NAME READY STATUS RESTARTS AGE argocd-application-controller-0 1/1 Running 0 113s argocd-dex-server-764699868-28tmj 1/1 Running 0 113s argocd-redis-675b9bbd9d-dtbzh 1/1 Running 0 113s argocd-repo-server-59ffd86d98-2w7k4 1/1 Running 0 113s argocd-server-6d66686c5c-nqfpf 1/1 Running 0 113s 3.调整服务类型为NodePort kubectl -n argocd expose deployments/argocd-server --type=\"NodePort\" --port=8080 --name=argocd-server-nodeport 获取NodePort [root@node1 ~]# kubectl get service/argocd-server-nodeport -n argocd NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE argocd-server-nodeport NodePort 10.233.34.101 8080:31398/TCP 87s 4.查看登录口令 kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d|xargs -n1 echo 5.登录 登录地址： http://NodeIP:31418 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/argo/03-使用argocd.html":{"url":"4.持续集成&交付/argo/03-使用argocd.html","title":"03-使用argocd","keywords":"","body":"实践 通过一个样例来说明，argocd是如何结合gitlab与k8s实现应用的cicd流程 关于argocd其他部分内容（用户管理、安全审计、自定义hook）等内容，这里不做过多讨论 相关技术&工具： gitlab: 存放源代码与应用清单 docker: 构建镜像&容器运行时 harbor: 镜像制品库，管理镜像 jenkins: ci流水线工具 k8s: 容器编排工具 argocd: 基于k8s的cd工具 流程解析 代码库变更 开发人员提交代码，触发jenkins ci pipeline ci pipeline执行构建 包含以下步骤： 打包构建应用程序 构建应用镜像 根据commit id创建镜像tag 推送至镜像库 变更配置库配置 变更配置库配置 ci pipeline最后一个流程，执行以下内容： checkout配置库 利用yq工具变更配置库内yaml清单文件内容（主要为镜像tag） 提交变更至配置库 cd流程 argocd拉取配置库清单文件，比对内容。 文件发生变更 -> 执行变更 文件未发生变更 -> 继续观测配置库变更 源码库关键文件 源码工程 demo: 基于spring boot工程 Jenkinsfile内容 pipeline { agent any environment { DEMO_IMAGE_TAG=\"harbor.wl.com/library/demo\" DOCKER_REGISTRY_DOMAIN=\"harbor.wl.com\" DOCKER_CREDENTIAL_ID = 'harbor-secret' GIT_CREDENTIAL_ID='d145edf3-929c-4efa-aa46-48ea0cf4336e' GIT_CONFIG_REPO_URL=\"192.168.1.1:80/demo-group/demo.git\" } stages { stage ('checkout scm') { steps { checkout(scm) } } // 获取git提交的commit id stage('get commit id...') { steps { script { env.GIT_COMMIT_ID = sh (script: 'git rev-parse --short HEAD', returnStdout: true).trim() } } } // 基于Dockerfile内容构建demo应用镜像，生成两个版本tag：latest && commit id stage ('build demo image...') { steps { sh ''' sudo docker build -t $DEMO_IMAGE_TAG -f Dockerfile . sudo docker tag $DEMO_IMAGE_TAG $DEMO_IMAGE_TAG:$GIT_COMMIT_ID ''' } } // 推送镜像至本地Harbor库，票据有jenkins管理 stage ('publish image with portal...') { steps { withCredentials([usernamePassword(passwordVariable : 'DOCKER_PASSWORD' ,usernameVariable : 'DOCKER_USERNAME' ,credentialsId : \"$DOCKER_CREDENTIAL_ID\" ,)]) { sh 'sudo echo \"$DOCKER_PASSWORD\" | sudo docker login $DOCKER_REGISTRY_DOMAIN -u \"$DOCKER_USERNAME\" --password-stdin' sh ''' sudo docker push \"$DEMO_IMAGE_TAG\" sudo docker push \"$DEMO_IMAGE_TAG:$GIT_COMMIT_ID\" ''' } } } // checkout 配置库 stage ('checkout config repo ...') { steps { checkout([$class: 'GitSCM', branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: \"$GIT_CREDENTIAL_ID\", url: \"http://${GIT_CONFIG_REPO_URL}\"]]]) } } // 更改demo-config库下demo/demo.yaml文件内镜像tag // 提交更改至demo-config库 stage ('commit config repo changes ...') { steps { withCredentials([usernamePassword(credentialsId: \"$GIT_CREDENTIAL_ID\", passwordVariable: 'GIT_PASSWORD', usernameVariable: 'GIT_USERNAME')]) { sh ''' echo \"#$GIT_COMMIT_ID#\" tag=$DEMO_IMAGE_TAG:$GIT_COMMIT_ID tag=$tag yq eval \".spec.template.spec.containers[0].image = strenv(tag)\" -i demo/demo.yaml git add demo/demo.yaml git commit -m \"modify version\" git config --global push.default simple git push http://$GIT_USERNAME:$GIT_PASSWORD@${GIT_CONFIG_REPO_URL} HEAD:master ''' } } } } } Dockerfile内容 FROM harbor.wl.com/library/maven:3.8.1 AS builder WORKDIR /usr/local ADD . . RUN mvn clean package FROM harbor.wl.com/library/openjdk-1.8:alpine COPY --from=builder /usr/local/demo/target/demo-0.0.1-SNAPSHOT.jar /opt/app.jar EXPOSE 8080 配置库关键文件 demo-config配置库层级及清单内容 层级 demo-config └── demo ├── demo-svc.yaml └── demo.yaml demo.yaml内容： apiVersion: apps/v1 kind: Deployment metadata: name: demo-app labels: app: demo-app spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: demo-app template: metadata: labels: app: demo-app spec: containers: - name: demo-app image: harbor.wl.com/library/demo:da28fcb imagePullPolicy: Always args: - java - '-Xms2048m' - '-Xmx2048m' - '-jar' - /opt/app.jar - '--server.port=8080' - '--spring.profiles.active=dev' livenessProbe: failureThreshold: 10 httpGet: path: /actuator/health port: 7002 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 readinessProbe: failureThreshold: 10 httpGet: path: /actuator/health port: 7002 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 ports: - containerPort: 8080 name: http-8080 protocol: TCP dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 demo-svc.yaml内容 --- apiVersion: v1 kind: Service metadata: name: demo-svc labels: app: demo-svc spec: ports: - name: http-8080 port: 80 protocol: TCP targetPort: 8080 selector: app: demo-app sessionAffinity: None type: ClusterIP harbor库配置信息 配置镜像清理策略，以免垃圾镜像过多 argocd配置信息 配置仓库 web控制台进入仓库配置界面 点击CONNECT REPO USING HTTPS添加仓库 配置相关信息点击CONNECT 查看项目下仓库状态 配置集群 点击设置->集群 编辑集群信息，namespace值为空（保存后会自动填充为All namespaces） 创建项目（逻辑分组） 点击设置->项目 创建demo项目 配置项目关联的git仓库与k8s集群信息 创建应用 新建应用 配置应用 至此流程配置完毕 样例应用 以下展示实际开发项目的cd应用 应用关联的资源对象 应用同步信息 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/argo/04-最佳实践.html":{"url":"4.持续集成&交付/argo/04-最佳实践.html","title":"04-最佳实践","keywords":"","body":"最佳实践 分离配置库和源代码库 使用单独的Git存储库来保存kubernetes清单，将配置与应用程序源代码分开，强烈推荐使用，原因如下: 清晰分离了应用程序代码与应用程序配置。有时您希望只修改清单，而不触发整个CI构建。 例如，如果您只是希望增加部署规范中的副本数量，那么您可能不希望触发构建（由于构建周期可能较长） 更清洁的审计日志。出于审计目的，只保存配置库历史更改记录，而不是掺有日常开发提交的日志记录。 微服务场景下，应用程序可能由多个Git库构建的服务组成，但是作为单个单元部署（比如同一pod内）。 通常，微服务应用由不同版本和不同发布周期的服务组成(如ELK, Kafka + Zookeeper)。 将配置清单存储在单个组件的一个源代码库中可能没有意义 访问的分离。开发应用程序的开发人员不一定是能够/应该推送到生产环境的同一个人，无论是有意的还是无意的。 通过使用单独的库，可以将提交访问权限授予源代码库，而不是应用程序配置库 自动化CI Pipeline场景下，将清单更改推送到同一个Git存储库可能会触发构建作业和Git提交触发器的无限循环。 使用一个单独的repo来推送配置更改，可以防止这种情况发生。 确保在Git版本中的清单是真正不可变的 当使用像helm或kustomize这样的模板工具时，清单的内容可能会随着时间的推移而改变。 这通常是由对上游helm库或kustomize库的更改引起的。 以下面kustomization.yaml为例 bases: - github.com/argoproj/argo-cd//manifests/cluster-install 由于这不是一个稳定的目标，因此这个自定义应用程序的清单可能会突然改变内容，甚至不需要对自己的Git存储库进行任何更改。（比如git master分支） 更好的选择是使用Git标记或提交SHA的版本。例如: bases: - github.com/argoproj/argo-cd//manifests/cluster-install?ref=v0.11.1 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/gitlab-ci/git-runner.html":{"url":"4.持续集成&交付/gitlab-ci/git-runner.html","title":"git-runner","keywords":"","body":"离线安装 下载地址 安装 yum localinstall gitlab-runner-13.11.0-1.x86_64.rpm -y 启动 systemctl enable gitlab-runner --now 编译安装git-cli curl -L https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.9.5.tar.xz -o ./git-2.9.5.tar.xz -k yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker -y tar xvf git-2.9.5.tar.xz cd git-2.9.5 ./configure --prefix=/usr/local/git make && make install cat >> ~/.bash_profile 安装docker groupadd docker gpasswd -a gitlab-runner docker newgrp docker 重启docker 配置 注册 gitlab-runner register # 键入gitlab地址 配置 1.集成k8s集群 1.获取api-server地址 kubectl cluster-info | grep -E 'Kubernetes master|Kubernetes control plane' | awk '/http/ {print $NF}' 2.获取ca证书 caTokenName=`kubectl get secrets|grep default-token|awk '{print $1}'` kubectl get secret $caTokenName -o jsonpath=\"{['data']['ca\\.crt']}\" | base64 --decode 3.获取用户token 创建用户 cat 获取token kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep gitlab | awk '{print $1}') Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/gitlab-ci/k8s-runner.html":{"url":"4.持续集成&交付/gitlab-ci/k8s-runner.html","title":"k8s-runner","keywords":"","body":"k8s运行runner 创建cephfs 用于挂载.m2、node缓存数据 创建cephfs $ ceph osd pool create devops_cephfs_data 8 $ ceph osd pool create devops_cephfs_metadata 8 $ ceph fs new k8s-cephfs devops_cephfs_metadata devops_cephfs_data $ ceph osd pool application enable devops_cephfs_data cephfs $ ceph osd pool set-quota devops_cephfs_data max_bytes 4T 获取Gitlab组 获取Gitlab组的gitlab-runner token，用于后续注册 Wf-qTAsNjBBDZPDzsXwT base64 $ echo \"Wf-qTAsNjBBDZPDzsXwT\" | base64 -w0 V2YtcVRBc05qQkJEWlBEenNYd1QK 创建gitlab runner 新建命名空间，命名空间需唯一。原则上以devops-起始（例devops-linyi） $ kubectl create ns devops-demo 创建密钥 $ cat 创建管理脚本 $ cat &1 | tail -n1 | awk '{print $4}' | cut -d'=' -f2)\" -n ${RUNNER_NAME} exit $? } trap 'unregister' EXIT HUP INT QUIT PIPE TERM echo \"Registering runner ${RUNNER_NAME} ...\" /usr/bin/gitlab-ci-multi-runner register -r ${GITLAB_CI_TOKEN} sed -i 's/^concurrent.*/concurrent = '\"${RUNNER_REQUEST_CONCURRENCY}\"'/' /home/gitlab-runner/.gitlab-runner/config.toml echo \"Starting runner ${RUNNER_NAME} ...\" /usr/bin/gitlab-ci-multi-runner run -n ${RUNNER_NAME} & wait EOF 创建配置文件 $ cat 创建rbac $ cat 创建runner $ cat Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/gitlab-ci/yq.html":{"url":"4.持续集成&交付/gitlab-ci/yq.html","title":"yq","keywords":"","body":"Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/gitlab-ci/变量定义与引用.html":{"url":"4.持续集成&交付/gitlab-ci/变量定义与引用.html","title":"变量定义与引用","keywords":"","body":" 按分支定义不同变量 workflow: rules: - if: $CI_COMMIT_BRANCH == \"java8-runtime-base\" variables: CI_IMAGE_TAG: \"base\" - if: $CI_COMMIT_BRANCH == \"java8-runtime-advance\" variables: CI_IMAGE_TAG: \"advance\" - if: $CI_COMMIT_BRANCH == \"java8-runtime-base\" variables: CI_IMAGE_TAG: \"base\" - if: $CI_COMMIT_BRANCH == \"java8-runtime-base\" variables: CI_IMAGE_TAG: \"base\" - if: $CI_COMMIT_BRANCH == \"java8-runtime-base\" variables: CI_IMAGE_TAG: \"base\" Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/jenkins/01-jenkins安装.html":{"url":"4.持续集成&交付/jenkins/01-jenkins安装.html","title":"01-jenkins安装","keywords":"","body":"jenkins安装 方便起见我们假设jenkins宿主机IP为 192.168.1.2 1.环境依赖 系统要求 最低推荐配置: 256MB可用内存,1GB可用磁盘空间(作为一个Docker容器运行jenkins的话推荐10GB) 为小团队推荐的硬件配置: 1GB+可用内存,50GB+可用磁盘空间 软件配置: Java8—无论是Java运行时环境（JRE）还是Java开发工具包（JDK）都可以。 2.安装jdk1.8 yum install -y java-1.8.0-openjdk.x86_64 java-1.8.0-openjdk-devel.x86_64 3.安装jenkins wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key yum install jenkins -y 4.调整默认配置 sed -i \"s#JENKINS_PORT=\\\"8080\\\"#JENKINS_PORT=\\\"8081\\\"#g\" /etc/sysconfig/jenkins sed -i \"s#JENKINS_ARGS=\\\"\\\"#s#JENKINS_ARGS=\\\"--prefix=/jenkins\\\"#g\" /etc/sysconfig/jenkins 5.启动并开机自启动 systemctl enable jenkins --now 6.初始化jenkins账号 浏览器访问http://192.168.1.2:8081/jenkins 根据提示获取管理员初始密码 cat /var/lib/jenkins/secrets/initialAdminPassword 根据提示安装默认插件（默认即可，后续更改插件源地址，按需下载） 根据提示创建新用户 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/jenkins/02-插件管理.html":{"url":"4.持续集成&交付/jenkins/02-插件管理.html","title":"02-插件管理","keywords":"","body":"插件管理 浏览器访问http://192.168.1.2:8081/jenkins/pluginManager/advanced 按需配置代理 更改插件源为国内源 更改为清华源 https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 安装以下插件 Credentials Binding Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/jenkins/03-github配置webhook.html":{"url":"4.持续集成&交付/jenkins/03-github配置webhook.html","title":"03-github配置webhook","keywords":"","body":"github配置webhook 登录GitHub，进入要本次构建用到的工程 在工程主页面点击右上角的Settings，再点击左侧Webhooks，然后点击Add webhook，如下图： 如下图，在Payload URL位置填入webhook地址，再点击底部的Add webhook按钮，这样就完成webhook配置了， 今后当前工程有代码提交，GitHub就会向此webhook地址发请求，通知`Jenkins``构建： http://192.168.1.2:8081/jenkins/github-webhook push触发效果 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/jenkins/04-github生成Tocken.html":{"url":"4.持续集成&交付/jenkins/04-github生成Tocken.html","title":"04-github生成Tocken","keywords":"","body":"github生成Tocken 图片内地址链接 选择Personal access tokens 选择Generate new tocken 勾选repo与admin:repo_hook 点击最下方绿色按钮Generate tocken，复制该tocken jenkins配置github Tocken 访问jenkins系统设置页面，找到配置Github 服务器 生成凭据 配置凭据 测试连通性 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/jenkins/111-trouble-shooting.html":{"url":"4.持续集成&交付/jenkins/111-trouble-shooting.html","title":"111-trouble-shooting","keywords":"","body":"常见问题 安装插件时提示：No valid crumb was included in the request 更改配置地址 http://192.168.1.2:8081/jenkins/configureSecurity/ 解决方案： 在jenkins 的Configure Global Security下 , 取消“防止跨站点请求伪造（`Prevent Cross Site Request Forgery exploits）”的勾选。 GitHub webhook触发时You are authenticated as: anonymous 403 解决方案：99%是tocken过期了，重新生成并配置 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"4.持续集成&交付/jenkins/jenkinsfile.html":{"url":"4.持续集成&交付/jenkins/jenkinsfile.html","title":"jenkinsfile","keywords":"","body":" 使用预定义密钥构建 stage('build java8-centos-office...') { steps { withCredentials([usernamePassword(passwordVariable : 'DOCKER_PASSWORD' ,usernameVariable : 'DOCKER_USERNAME' ,credentialsId : \"$DOCKER_CREDENTIAL_ID\" ,)]) { sh ''' make all ''' } } } Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"5.cncf/cncf.html":{"url":"5.cncf/cncf.html","title":"cncf","keywords":"","body":" CNCF基金会 CNCF生态 CNCF介绍 加入CNCF好处 基金会 CNCF宪章 CNCF使命 CNCF角色 CNCF价值观 CNCF会员制 CNCF理事会 TOC技术监督委员会 终端用户社区 终端用户技术咨询委员会(End User TAB) CNCF项目 营销委员会 知识产权策略 反垄断指南 行为准则 关联公司 CNCF项目 CNCF项目提交流程 沙盒阶段 项目孵化流程 相关术语 CLA TODO CNCF毕业项目落地使用 CNCF基金会 CNCF生态 全景图 CNCF介绍 CNCF为开源项目提供了强大的服务支柱，围绕着维持大多数项目需求的目标而建立，而不仅仅是代码管理和技术决策。 我们通过专业人员提供一组增强的服务，这些专业人员培养本地云开源项目的成熟度和更多的采用。 我们采用数据驱动的方法与我们的项目和维护人员社区一起工作; 我们积极进行调查，以改善我们的服务和社区对我们的满意程度，并为社区提供优质服务。 加入CNCF好处 基金会 1.开源项目的中立组织增加了来自企业软件公司、初创公司和独立开发人员协作、贡献和成为提交者的意愿 2.CNCF的技术监督委员会是技术管理机构，由有文件记录的负责人指导，并承认和监督所有项目 3.被CNCF TOC(CNCF技术监督委员会)接纳的项目，证明你的项目质量达标。 4.加入CNCF基金会的项目，项目所有者不会变，CNCF基金会为项目提供一个文档化良好的、中立的管理过程。 5. CNCF宪章 github Linux基金会 2015年11月6日生效/ 2018年12月10日更新 CNCF使命 CNCF的使命是让云原生计算无处不在，CNCF云原生v1.0定义内容: 云原生技术有利于各组织在现代化动态环境（公有云、私有云和混合云等）中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。 这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。 云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。 CNCF角色 CNCF将在开源社区中扮演一个负责以下内容的角色: 云原生项目管理 确保技术对社区开放，不受党派影响 确保技术的品牌(商标和标识)得到社区成员的关注和适当使用，特别强调统一的用户体验和高水平的应用程序兼容性 促进云原生生态系统的发展 评估应该添加哪些额外的技术来满足云原生应用程序的愿景，努力鼓励社区提供这些服务，予以整合并推进进度 提供促进各领域通用技术标准落地方式 云原生基础技术的推广，以及云原生应用程序定义和管理的方法，包括:活动和会议、营销(SEM，直接营销)、培训课程和开发者认证 通过使云原生技术可及和可靠的方式服务于社区 CNCF基金会致力于提供云原生标准架构各领域的组件 CNCF价值观 CNCF将努力坚持以下原则: 快 推进项目高速发展，以支持用户能够积极的使用 开放 基金会是开放的、可访问的，并且独立于特定的党派利益运作。基金会根据贡献者的贡献的价值接受所有贡献者，并且基金会的技术必须根据开放源码的价值对所有人开放。技术社区及其决定应该是透明的 公平 基金会将避免不当影响、不良行为或“付费”决策 雄厚的技术身份 基金会将实现并维持它自己的高度的技术识别，并且跨项目共享 清晰的界限 基金会应该建立明确的目标，在某些情况下，确定什么不是基金会的目标，并帮助生态系统了解新的创新应该集中在哪里 可伸缩的 能够支持所有规模的部署，从小型的以开发人员为中心的环境到企业和服务提供商的规模。这意味着在某些部署中可能不部署某些可选组件，但总体设计和体系结构仍然适用。 跨平台 所开发的规范不会是特定于平台的，这样它们就可以在各种架构和操作系统上实现。 CNCF会员制 CNCF由白金会员、黄金会员、白银会员、终端用户、学术和非盈利会员组成，不同级别的会员在理事会中的投票权不同。 所有的成员申请将由Linux基金会审查,基金会将决定是否将该申请人归类为CNCF会员、终端用户、学术/非营利或CNCF会员供应商 白金会员权益 任命一名代表加入CNCF理事会 在任何小组委员会或理事会活动中指定一名代表作为有表决权的成员 享受最突出的位置在会员展示，包括在网站上 如果该成员也是经过批准的终端用户，则享有终端用户成员的所有特权 黄金会员权益 每五名黄金会员指派一名黄金会员代表加入CNCF理事会，最多有三名黄金会员代表 如果该成员也是经过批准的终端用户，则享有终端用户成员的所有特权 白银会员权益 每十名白银会员指派一名白银会员代表加入CNCF理事会，最多有三名白银会员代表 如果该成员也是经过批准的终端用户，则享有终端用户成员的所有特权 CNCF理事会 CNCF理事会负责CNCF的营销和其他业务监督以及预算决策。 理事会不为CNCF做技术决定，只是和TOC(技术监督委员会)一起协作，为CNCF设定云原生定义中所述的技术决策 理事会处理以下商业事务： 在与CNCF技术委员会协商后，确定CNCF的整体范围 定义及执行有关使用基金会商标及版权的政策 指导市场营销，包括布道，活动和生态系统参与 按需建立品牌合规管理制度 监督运营，业务发展 筹资和财务治理 理事会组织结构：理事会投票成员由成员代表和技术社区代表组成 会员代表包括：每名白金会员委任一名代表、每五名黄金会员指派一名黄金会员代表、每十名白银会员指派一名白银会员代表 技术社区代表包括：技术委员会主席、从CNCF项目中选出两名经理事会批准的技术委员会成员 对于被理事会视为战略技术贡献者、营收低于5000万美元的初创公司，理事会可以按银牌会员比例逐年延长其白金会员资格，最长可延长5年。 同类型企业中最多有两个代表，且一人作为会员代表，一人作为技术社区代表 理事会职责 资金管理：指导使用筹集的资金，用于推云原生技术、营销或社区投资 选举理事会主席来主持会议，审批资金支出并管理日常业务 对理事会的决定或事项进行表决 指定执行有关基金会知识产权(版权、专利或商标)的政策 通过活动、新闻、网络、社交和其他营销活动进行直接营销和宣传 监督运营，业务发展 建立和监督为推动CNCF使命而创建的委员会 基于CNCF需求，制定执行品牌合规计划，包括认证测试，使用TOC建立的品牌标志 制定商标的使用方针或政策 提供全面的财务治理 资金收入用于下列用途: CNCF项目推广 关键基础设施的建设与运营 推广云原生概述的基于容器的计算原理，并通过CNCF的项目实施 TOC技术监督委员会 权力职责：TOC期望通过以下方式推动达成中立共识: 定义和维护CNCF的技术远景 批准董事会确定的CNCF范围内的新项目，并为这些项目建立概念架构 调整、删除或归档项目 接受来自终端用户委员会的反馈并映射到项目 将接口与管理下的组件对齐（标准化前的代码参考实现） 定义在CNCF项目中实施的通用实践 组织结构： 目前11名成员 当选的TOC成员涵盖关键技术领域：容器技术、操作系统、技术操作、分布式系统、用户级应用程序设计等 投票权：理事会6票，终端用户2票，非沙盒项目维护者1票，TOC2票。 如果有超过2(2)个TOC成员来自同一组关联公司，无论是在选举时，还是在后来的工作变动中，由TOC成员共同决定谁下台，如果没有达成一致，则随机抽签 运营模式： 选举出TOC主席，制定议程，召开理事会会议 定期举行面对面的会议，讨论关键问题 TOC会在需要时召开会议，讨论新出现的问题。问题可由以下人员提出，供TOC审查： TOC成员 董事会成员 CNCF顶级项目的负责人 CNCF执行董事 终端用户技术咨询委员会多数票通过 透明度:TOC定期召开公开的TOC会议，所有与项目有关的决定应在这些会议、公开邮件列表或公开问题上作出 简单的TOC问题可以通过简短的讨论和多数投票来解决。可以通过电子邮件或在TOC的会议上进行讨论 审核意见后，确定方案，寻求共识，并在必要时进行表决 其目的是让TOC找到一条在TOC和社区内达成共识的途径。在符合法定人数之会议上，理事会的决定，要求应以超过50%的委员投票通过 TOC会议的法定人数应为TOC全体成员的三分之二，以进行表决或作出任何决定。如果TOC会议未能达到法定人数要求，可进行讨论，但不得进行表决或作出决定 TOC的决定可以在不召开会议的情况下通过电子方式作出，但要通过投票，要求达到会议法定人数所需的票数。在电子投票过程中，如果任何两(2)TOC成员要求召开会议讨论该决定，电子投票过程将无效终止，会议结束后可以发起新的投票讨论该决定 提名标准。TOC的提名人应: 承诺有足够的时间投入CNCF TOC 在CNCF领域内具有高级工程师的专业经验 在讨论中保持中立，将CNCF的目标和成功与公司目标或CNCF的任何特定项目相平衡 TOC成员提名及选举流程： 提名：每个选择组（理事会选择组、终端用户选择组、非沙盒项目维护者选择组）中的每个人最多可提名两人，其中最多一人可来自同一组关联公司。每一位被提名人在被加入提名名单之前必须同意参加 提名要求最多一(1)页的提名陈述，其中应包括被提名人的姓名、联系信息和证明被提名人在CNCF领域的经验的支持性陈述 理事会应决定TOC成员提名、资格和选举的流程和时间表 评估期应至少为14个工作日，理事会成员和TOC成员可在该期间联系TOC的提名人 资格确认：评估期结束后，理事会和TOC成员应分别对每一位被提名者进行投票，以确认被提名者符合资格标准。有效的投票要求至少有50%的参与率。得票率超过50%的候选人为合格候选人 选举：如果合格的候选人数目等于或少于TOC可供选举的席位数目，合格的候选人将在提名期结束后获得批准。如果被提名者多于TOC席位可供选举数量，则继续从提名者中投票选出TOC成员。 新一轮提名：如合格的提名人数少于选举小组可供选举的TOC席位，则选举小组进行新一轮提名 TOC推选的TOC成员可以提名并有资格参加选举，但在选举时不能投票 成员约束： TOC成员任期两年，交错任期 TOC成员可由其他TOC成员的三分之二投票罢免，受影响的个人无资格参加投票 任何未能连续三（3）次参加会议的TOC成员，在连续参加两次会议之前，自动取消其投票资格。 TOC议程将由TOC确定。TOC讨论和决定包含: 评估CNCF中包括的技术 制定新技术纳入CNCF的验收标准 定义将贡献的技术批准为标准API的过程 终端用户社区 CNCF的终端用户成员应有权协调和驱动CNCF用户作为CNCF设计的消费者的重要活动。 任何作为终端用户的会员或非会员，均为“终端用户参与者”，均应被邀请参加。 终端用户参与者将帮助就与用户相关的主题向技术咨询委员会和CNCF社区提供投入 终端用户社区成员应选举一个终端用户技术咨询委员会 终端用户社区成员将由CNCF执行董事批准，如果不存在，则由Linux基金会执行董事批准 终端用户技术咨询委员会(End User TAB) 人员组成： 终端用户技术咨询委员会由来自终端用户参与者的7名代表和TOC的1名成员组成，以便于从终端用户技术咨询委员会输入到TOC 选举： 为了鼓励终端用户参与CNCF，前七（7）名终端用户成员可指定一（1）名代表参加终端用户咨询委员会，剩余席位由CNCF董事分配给终端用户参与者。 在最初的一年之后，所有终端用户参与者可提名一（1）名代表，终端用户社区应通过当时的终端用户咨询委员会批准的流程投票选择终端用户咨询委员会成员 终端用户咨询委员会可以以三分之二的票数更改终端用户的数量，前提是至少有七（7）名终端用户技术咨询委员会代表 终端用户代表应在业务和技术上具有敏锐的洞察力。被提名者应在建设和运营基础设施和应用方面具有重要的实践经验，这些经验体现了CNCF的原则 终端用户技术咨询委员会将讨论和推进各项议题，重点是找出差距，并为TOC和CNCF社区开发人员提出优先事项 终端用户技术咨询委员会还可以侧重于主动提出终端用户关注的话题、促进CNCF的市场采用、为终端用户主持会议或向理事会提供建议 如果终端用户技术咨询委员会需要，它可以批准Special Interest Groups（SIGs）来处理行业或专门主题 TOC的终端用户技术委员会输入应与其他输入和反馈一起进行，以便TOC做出决策和计划。建议仅为咨询性建议，在任何情况下， 终端用户技术咨询小组的建议均不得用于命令或指示任何TOC或项目参与方采取任何行动或取得任何结果。 CNCF项目 成员公司和开放源码社区成员将把项目资产提交TOC讨论，并纳入CNCF所有此类捐款均应符合技术选择委员会制定并经理事会批准的一套标准。 我们的目标是使越来越多的项目与已经被CNCF接受的项目结合在一起。 项目可通过以下三种方式与CNCF进行关联: 纳入CNCF，在一个中立的合作之家 项目的所有方面都由CNCF管理 该项目被CNCF称为CNCF项目 该项目应该是CNCF解决方案的核心功能部分。（例如Kubernetes、Mesos、etcd等） 通过API或规范与CNCF相关联 包括CNCF可能提供或启用多个选项的组件 该项目被称为与CNCF集成的组件，而不是由CNCF托管的项目 被CNCF使用 在osi批准的开源许可下完全获得许可的项目或组件，并且在CNCF中作为组件得到良好的管理和使用 CNCF未积极推广的项目 上游社区积极开发完成的项目或组件 现有的开源项目应该继续通过它们现有的技术治理结构来保持凝聚力和速度。 经技术监督委员会批准列入CNCF的项目将“轻微”接受技术监督委员会的监督 应根据个人的贡献水平和持续时间，在各个项目中建立一个标准协议， 以实现提交者的地位。维护者状态是通过一段时间内对给定项目的贡献和对等提交者的验证来实现的 在CNCF发起的新开源项目应完成TOC采用的项目建议书模板，并由TOC批准纳入CNCF。TOC成员应被给予足够的时间来讨论和审查新的项目建议。 新的项目提案应包括项目中角色的详细信息，为项目提出的治理方案，并确定与CNCF角色和价值观的一致性 营销委员会 组成:营销委员会将开放给所有成员参加。应选举一名营销委员会主席，以制定会议议程，引导讨论，并帮助委员会实现其目标。 营销委员会应在可能的情况下寻求共识。任何不能在市场委员会中达成大致共识的问题应提交给理事会 职责：营销委员会应代表董事会负责开展、执行营销工作 如果市场营销委员会规模过大，无法有效运作，市场营销委员会可以选择选举经销管理局，并将决策权下放给经销管理局 知识产权策略 a.任何加入到CNCF的项目必须将其商标和标识资产的所有权转移给Linux基金会 b.每个项目应确定是否需要使用经批准的CNCF CLA，对于选择使用CLA的项目，所有代码贡献者都将承担Apache贡献者许可协议中规定的义务，只有在必要时才会修改，以确定CNCF是贡献的接受者，该协议应得到治理委员会的批准。 c.所有提交贡献CNCF项目的新入站代码应: 附带开发者源签字证书 源代码基于Apache License, Version 2.0 许可, 该许可是对(b)中规定的贡献许可协议项下所承担的义务的补充，且不应取代 d.所有出站代码都将在 Apache许可2.0 版本下提供 e.所有被评估纳入CNCF的项目都应该完全按照osi批准的开源许可证进行许可。如果CNCF中包含的项目许可证不是 Apache许可2.0 版，则需要得到理事会的批准。 f.CNCF将根据Creative Commons Attribution 4.0 International License(知识共享署名4.0国际许可协议)接收并提供所有文档。 反垄断指南 所有成员都应遵守Linux基金会反垄断政策中对Linux基金会的要求，详见 所有成员应鼓励任何能够满足成员资格要求的组织公开参与，而不考虑竞争利益。 换言之，理事会不应试图以任何标准、要求或理由排除所有成员所使用的标准、要求或理由以外的任何标准、要求或理由 行为准则 所有参与者同意遵守Linux基金会的行为准则，详见。TOC可以投票通过自己的CNCF社区行为准则 关联公司 关联公司的定义： “子公司”是指成员直接或间接拥有该实体百分之五十以上表决权证券或成员权益的任何实体 关联公司是指任何实体控制或由一个成员或控制,与成员一起,共同控制的第三方,在每种情况下,这样的控制结果所有权,直接或间接,超过百分之五十的投票证券或会员利益的实体问题 关联公司”是指各为成员关联公司的实体 只有签署了参与协议的法人实体及其附属公司才有权享受该成员资格的权利和特权;但是，该成员及其附属公司应被视为一个单一成员 一组关联公司中只有一名成员有权一次任命或提名一名董事会代表进行成员类别选举 如果成员本身是基金会、财团、开源项目、会员组织、用户组或其他具有成员或发起人的实体， 则授予该成员的权利和特权仅限于该成员的雇员代表，而不限于其成员或发起人，除非在特定情况下理事会另有批准。 成员资格应不可转让、不可出售和不可转让，除非任何成员可以通过合并、 出售或其他方式将其现有的成员资格利益和义务转让给其几乎所有业务和/或资产的继承人； 前提是受让人同意受本章程和Linux基金会成员要求的规章制度和政策的约束 CNCF项目 CNCF项目提交流程 v1.3 主要分为以下阶段： 沙盒 孵化 毕业 沙盒阶段 沙盒阶段目标： 通过确保所有项目遵守CNCF的法律约束、行为准则和知识产权政策要求，消除可能的法律和治理障碍 Promote an environment that encourages visibility of experiments to the TOC and SIGs and early work that can add value to the CNCF mission and collaboration across the CNCF community 如果（且仅当）需要，促进与现有项目的一致性 通过CNCFService Desk请求培育项目 （注意:沙盒项目不提供营销协助） 沙盒提交流程: 项目通过提交一个 表单 来申请加入沙盒，该表单在公共可见的电子表格中填充一行。表格待定，但将包括以下内容： 必需项：项目的GitHub链接地址或其他公共源代码存储库上的项目链接地址 可选项：项目网站链接地址 必需项：复选框，确认如果项目被CNCF接受，需要遵循CNCF IP政策 必需项：复选框，确认将给予CNCF项目的商标和帐户，如果CNCF接受该项目 必需项：对云原生的简短描述 必需项：与同类项目对比 必需项：链接到公共路线图（可以是任何格式-文档、GitHub issues、项目板等。这个要求很简单，只是要求提供有一些关于项目未来方向和目标的公共文档） 必需项：链接到贡献指南 必需项：链接到行为准则 可选项: 与现有CNCF项目保持一致的声明 可选项：链接到预先存在的演示文稿（胶片/视频） CNCF工作人员检查提交表单内容是否满足沙盒提交标准，如果目前不符合这些标准，则建议项目维护人员提供上述必须项 TOC每季度参考以下标准审查一次提交的沙盒项目： TOC评估该项目是否适合CNCF 项目的路线图是否与CNCF的目标一致? 该项目是否处于良好的治理,且厂商中立 在提交审查期间，技术咨询委员会对每一份提交的文件进行表决， 如果被拒绝，我们将指出TOC认为该项目不符合的标准。欢迎项目提出反馈意见，并在未来再次提交。除非另有通知，否则项目在六个月内不得再次提交。 沙盒项目仍然受 年度审查程序 的约束 项目孵化流程 项目计划书： 通过创建 GitHub issue 提出项目孵化 SIG评估1-2个月 项目提交给SIG SIG预 尽职调查 确认项目满足孵化要求 SIG向TOC提出建议：沙箱/孵化/拒绝 注意，这个步骤仍然是一个轻量级的、dd(尽职调查)之前的过程 TOC成员提出项目孵化情况 如果TOC成员作为项目孵化发起人，则开始进行尽职调查 2-3个月时间进行尽职调查 TOC孵化发起人推动尽职调查(参见 模板 和 指南 ) TOC孵化发起人可将尽职调查工作委托给CNCF SIG 或 其他TOC成员 TOC孵化发起人可以要求项目维护者首先完成尽职调查模板 CNCF员工做项目管理与合法性的尽职调查 在尽职调查期间，一些谈话可能是私下进行的（例如，用户希望保持匿名的用户访谈），并使用酌情权进行记录。 TOC孵化发起人确定尽职调查何时完成。尽职调查文档应该在GitHub上，公开给公众评论 2-6周时间进行审查尽职调查结果 尽职调查文件可供公众审查和评论（GitHub、TOC邮件列表、TOC公开会议） TOC孵化发起人决定何时召集TOC投票，在召集投票前至少留出两周时间征求公众意见 6周时间进行TOC投票 TOC成员评估项目是否符合孵化标准 如果2/3的TOC成员投赞成票，项目将被接受孵化。 如果投票在6周后仍没有结论，TOC主席可以延长投票，或得出沉默=弃权的结论 TODO 相关术语 CLA Contributor License Agreement的缩写，大致包含以下两个内容 Grant of Copyright License：贡献者拥有代码的版权（copyright），同时授权组织者和项目的所有使用者使用这个版权。 Grant of Patent License：贡献者的贡献如果申请了专利（patent）保护，那么贡献者持有专利，同时授权组织者和项目使用者使用这个专利。 CLA与开源许可（Apache 2.0、MIT、BSD License等）区别 CLA 定义代码贡献者和项目组织者的法律权责 开源许可定义的是贡献者和使用者之间的法律权责 TODO CNCF毕业项目落地使用 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"5.cncf/kubevirt.html":{"url":"5.cncf/kubevirt.html","title":"kubevirt","keywords":"","body":"集成部署 依赖检测 内核：5.10.2-1.el7.elrepo.x86_64 操作系统：CentOS Linux release 7.9.2009 (Core) docker：20.10.1 kubernetes：v1.18.6 KubeVirt：v0.35.0 检测宿主是否满足虚拟化条件 安装libvirt-client yum install -y libvirt-client 检测 [root@node3 ~]# virt-host-validate qemu QEMU: Checking for hardware virtualization : PASS QEMU: Checking if device /dev/kvm exists : PASS QEMU: Checking if device /dev/kvm is accessible : PASS QEMU: Checking if device /dev/vhost-net exists : PASS QEMU: Checking if device /dev/net/tun exists : PASS QEMU: Checking for cgroup 'memory' controller support : PASS QEMU: Checking for cgroup 'memory' controller mount-point : PASS QEMU: Checking for cgroup 'cpu' controller support : PASS QEMU: Checking for cgroup 'cpu' controller mount-point : PASS QEMU: Checking for cgroup 'cpuacct' controller support : PASS QEMU: Checking for cgroup 'cpuacct' controller mount-point : PASS QEMU: Checking for cgroup 'cpuset' controller support : PASS QEMU: Checking for cgroup 'cpuset' controller mount-point : PASS QEMU: Checking for cgroup 'devices' controller support : PASS QEMU: Checking for cgroup 'devices' controller mount-point : PASS QEMU: Checking for cgroup 'blkio' controller support : PASS QEMU: Checking for cgroup 'blkio' controller mount-point : PASS QEMU: Checking for device assignment IOMMU support : PASS QEMU: Checking if IOMMU is enabled by kernel : WARN (IOMMU appears to be disabled in kernel. Add intel_iommu=on to kernel cmdline arguments) 处理IOMMU告警 方法一 修改GRUB_CMDLINE_LINUX添加intel_iommu=on 修改前 [root@node3 ~]# cat /etc/default/grub GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=\"$(sed 's, release .*$,,g' /etc/system-release)\" GRUB_DEFAULT=saved GRUB_DISABLE_SUBMENU=true GRUB_TERMINAL_OUTPUT=\"console\" GRUB_CMDLINE_LINUX=\"crashkernel=auto rd.lvm.lv=centos00/root rhgb quiet\" GRUB_DISABLE_RECOVERY=\"true\" 修改后 GRUB_TIMEOUT=5 GRUB_DISTRIBUTOR=\"$(sed 's, release .*$,,g' /etc/system-release)\" GRUB_DEFAULT=saved GRUB_DISABLE_SUBMENU=true GRUB_TERMINAL_OUTPUT=\"console\" GRUB_CMDLINE_LINUX=\"crashkernel=auto rd.lvm.lv=centos00/root rhgb quiet intel_iommu=on\" GRUB_DISABLE_RECOVERY=\"true\" 重建内核引导文件 grub2-mkconfig -o /boot/grub2/grub.cfg dracut --regenerate-all --force 重启 reboot 验证 cat /proc/cmdline |grep intel_iommu=on 如返回为空，尝试方法二 方法二 查询引导文件 [root@node3 ~]# find / -name \"grub.cfg\" /boot/efi/EFI/centos/grub.cfg /boot/grub2/grub.cfg 修改/boot/efi/EFI/centos/grub.cfg文件内容 修改前 linuxefi /vmlinuz-5.10.2-1.el7.elrepo.x86_64 root=/dev/mapper/centos00-root ro crashkernel=auto rd.lvm.lv=centos00/root rhgb quiet LANG=en_US.UTF-8 修改后 linuxefi /vmlinuz-5.10.2-1.el7.elrepo.x86_64 root=/dev/mapper/centos00-root ro crashkernel=auto rd.lvm.lv=centos00/root rhgb quiet intel_iommu=on LANG=en_US.UTF-8 重建内核引导文件 grub2-mkconfig -o /boot/grub2/grub.cfg dracut --regenerate-all --force 重启 reboot 验证 cat /proc/cmdline |grep intel_iommu=on 部署KubeVirt operator 下载kubevirt-operator.yaml v0.35.0版本下载链接 下载kubevirt operator所需镜像 镜像列表 kubevirt/virt-operator:v0.35.0 kubevirt/virt-api:v0.35.0 kubevirt/virt-controller:v0.35.0 kubevirt/virt-handler:v0.35.0 修改镜像tag，修改后如下 harbor.neusoft.com/kubevirt/virt-operator:v0.35.0 harbor.neusoft.com/kubevirt/virt-api:v0.35.0 harbor.neusoft.com/kubevirt/virt-controller:v0.35.0 harbor.neusoft.com/kubevirt/virt-handler:v0.35.0 推送至私有仓库 docker push harbor.neusoft.com/kubevirt/virt-operator:v0.35.0 docker push harbor.neusoft.com/kubevirt/virt-api:v0.35.0 docker push harbor.neusoft.com/kubevirt/virt-controller:v0.35.0 docker push harbor.neusoft.com/kubevirt/virt-handler:v0.35.0 上传kubevirt-operator.yaml并调整镜像tag ... containers: - command: - virt-operator - --port - \"8443\" - -v - \"2\" env: - name: OPERATOR_IMAGE value: harbor.neusoft.com/kubevirt/virt-operator:v0.35.0 - name: WATCH_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.annotations['olm.targetNamespaces'] - name: KUBEVIRT_VERSION value: v0.35.0 - name: VIRT_API_SHASUM value: sha256:bf38c1997f3c60a71d53b956f235973834d37c0c604b5711084b2a7ef8cd3c7b - name: VIRT_CONTROLLER_SHASUM value: sha256:7b81c59034df51c1a1f54d3180e0df678469790b6a8ac4fcc5fcaa615b1ca84c - name: VIRT_HANDLER_SHASUM value: sha256:14b4bd6d62b585ef2f4dbacafc75a66a6c575c64ed630835cf4ef6c0f77d40d1 - name: VIRT_LAUNCHER_SHASUM value: sha256:a6d9f1dada1d33a218ba9ed0494d2e2cd09f5596eff5eb5b8d70bfe1fd4f8812 image: harbor.neusoft.com/kubevirt/virt-operator:v0.35.0 ... 上传k8s节点发布 kubectl apply -f kubevirt-operator.yaml 部署KubeVirt CR 下载kubevirt-cr.yaml v0.35.0版本下载链接 上传k8s节点发布 kubectl apply -f kubevirt-cr.yaml 等待组件启动 kubectl -n kubevirt wait kv kubevirt --for condition=Available Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"6.编程/golang/01-安装配置/01-golang安装配置.html":{"url":"6.编程/golang/01-安装配置/01-golang安装配置.html","title":"01-golang安装配置","keywords":"","body":"linux 下载安装 wget https://studygolang.com/dl/golang/go1.13.4.linux-amd64.tar.gz sudo tar zxvf go1.13.4.linux-amd64.tar.gz -C /usr/local 配置 sudo mkdir -p $HOME/{src,bin,pkg} cat >> ~/.bash_profile Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"6.编程/golang/01-安装配置/02-配置代理.html":{"url":"6.编程/golang/01-安装配置/02-配置代理.html","title":"02-配置代理","keywords":"","body":"配置代理 类unix配置goproxy cat >> ~/.bash_profile windwos配置goproxy 打开powershell执行: # Enable the go modules feature $env:GO111MODULE=\"on\" # Set the GOPROXY environment variable $env:GOPROXY=\"https://goproxy.cn\" Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"6.编程/golang/111-开源库/01-logrus.html":{"url":"6.编程/golang/111-开源库/01-logrus.html","title":"01-logrus","keywords":"","body":"日志管理 logrus项目地址 1.添加或直接import(利用gomod自动下载) go get github.com/sirupsen/logrus 2.默认输出格式 样例如下入参为 /opt/nginx/sbin/nginx package process import ( \"bytes\" log \"github.com/sirupsen/logrus\" \"os/exec\" ) var( exitCode = 0 out bytes.Buffer ) func Shell(shell string) (stdout string,exitCode int){ cmd := exec.Command(\"/bin/bash\", \"-c\", shell + \" 2>&1\") log.Info(\"执行命令为\" + shell) cmd.Stdout = &out if err := cmd.Start();err != nil { println(err.Error()) } if err := cmd.Wait();err != nil { exitCode = 1 } return out.String(),exitCode } 输出如下 3.引入格式包 go get github.com/antonfisher/nested-logrus-formatter 4.格式化日志输出 package utils import ( nested \"github.com/antonfisher/nested-logrus-formatter\" \"github.com/sirupsen/logrus\" \"time\" ) type Formatter struct { FieldsOrder []string // by default fields are sorted alphabetically TimestampFormat string // by default time.StampMilli = \"Jan _2 15:04:05.000\" is used HideKeys bool // to show only [fieldValue] instead of [fieldKey:fieldValue] NoColors bool // to disable all colors NoFieldsColors bool // to disable colors only on fields and keep levels colored ShowFullLevel bool // to show full level (e.g. [WARNING] instead of [WARN]) TrimMessages bool // to trim whitespace on messages } func Logger(format string, args ...interface{}) { logrus.SetFormatter(&logrus.TextFormatter{ForceColors: true}) logrus.SetLevel(logrus.InfoLevel) logrus.SetFormatter(&nested.Formatter{ HideKeys: true, NoColors: false, TrimMessages:false, TimestampFormat: time.RFC3339, FieldsOrder: []string{\"component\", \"category\"}, }) } 输出信息如下 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"6.编程/letcode/letcode.html":{"url":"6.编程/letcode/letcode.html","title":"letcode","keywords":"","body":" 两数之和 二分查找 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"6.编程/数据结构/链表/链表.html":{"url":"6.编程/数据结构/链表/链表.html","title":"链表","keywords":"","body":" Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"7.FAQ/sre.html":{"url":"7.FAQ/sre.html","title":"sre","keywords":"","body":" 1.linux启动顺序 启动第一步－－加载 BIOS 当你打开计算机电源，计算机会首先加载 BIOS 信息，BIOS 信息是如此的重要，以至于计 算机必须在最开始就找到它。这是因为 BIOS 中包含了 CPU 的相关信息、设备启动顺序信 息、硬盘信息、内存信息、时钟信息、PnP 特性等等。在此之后，计算机心里就有谱了， 知道应该去读取哪个硬件设备了。 启动第二步－－读取 MBR 众所周知，硬盘上第 0 磁道第一个扇区被称为 MBR，也就是 Master Boot Record，即主 引导记录，它的大小是 512 字节，别看地方不大，可里面却存放了预启动信息、分区表信 息。 系统找到 BIOS 所指定的硬盘的 MBR 后，就会将其复制到 0x7c00 地址所在的物理内存中。 其实被复制到物理内存的内容就是 Boot Loader，而具体到你的电脑，那就是 lilo 或者 grub 了。 启动第三步－－Boot Loader Boot Loader 就是在操作系统内核运行之前运行的一段小程序。通过这段小程序，我们可 以初始化硬件设备、建立内存空间的映射图，从而将系统的软硬件环境带到一个合适的状态， 以便为最终调用操作系统内核做好一切准备。 Boot Loader 有若干种，其中 Grub、Lilo 和 spfdisk 是常见的 Loader。 我们以 Grub 为例来讲解吧，毕竟用 lilo 和 spfdisk 的人并不多。 系统读取内存中的 grub 配置信息（一般为 menu.lst 或 grub.lst），并依照此配置信息来 启动不同的操作系统。 启动第四步－－加载内核 根据 grub 设定的内核映像所在路径，系统读取内核映像，并进行解压缩操作。此时，屏幕 一般会输出“Uncompressing Linux”的提示。当解压缩内核完成后，屏幕输出“OK, booting the kernel”。 系统将解压后的内核放置在内存之中，并调用 start_kernel()函数来启动一系列的初始化函 数并初始化各种设备，完成 Linux 核心环境的建立。至此，Linux 内核已经建立起来了，基 于 Linux 的程序应该可以正常运行了。 启动第五步－－用户层 init 依据 inittab 文件来设定运行等级 内核被加载后，第一个运行的程序便是/sbin/init，该文件会读取/etc/inittab 文件，并依据 此文件来进行初始化工作。 其实/etc/inittab 文件最主要的作用就是设定 Linux 的运行等级，其设定形式是“： id:5:initdefault:”，这就表明 Linux 需要运行在等级 5 上。Linux 的运行等级设定如下： 0：关机 1：单用户模式 2：无网络支持的多用户模式 3：有网络支持的多用户模式 4：保留，未使用 5：有网络支持有 X-Window 支持的多用户模式 6：重新引导系统，即重启 启动第六步－－init 进程执行 rc.sysinit 在设定了运行等级后，Linux 系统执行的第一个用户层文件就是/etc/rc.d/rc.sysinit 脚本程 序，它做的工作非常多，包括设定 PATH、设定网络配置（/etc/sysconfig/network）、启 动 swap 分区、设定/proc 等等。如果你有兴趣，可以到/etc/rc.d 中查看一下 rc.sysinit 文 件。 启动第七步－－启动内核模块 具体是依据/etc/modules.conf 文件或/etc/modules.d 目录下的文件来装载内核模块。 启动第八步－－执行不同运行级别的脚本程序 根据运行级别的不同，系统会运行 rc0.d 到 rc6.d 中的相应的脚本程序，来完成相应的初始 化工作和启动相应的服务。 启动第九步－－执行/etc/rc.d/rc.local 你如果打开了此文件，里面有一句话，读过之后，你就会对此命令的作用一目了然 rc.local 就是在一切初始化工作后，Linux 留给用户进行个性化的地方。你可以把你想设置 和启动的东西放到这里。 启动第十步－－执行/bin/login 程序，进入登录状态 Linux 常见的系统日志文件 /var/log/messages: 内核及公共消息日志 /var/log/cron: 计划任务日志 /var/log/dmesg: 系统引导日志 /var/log/maillog: 邮件系统日志 /var/log/secure: 记录与访问限制相关日志 3.讲一下 Keepalived 的工作原理 在一个虚拟路由器中，只有作为 MASTER 的 VRRP 路由器会一直发送 VRRP 通告信息， BACKUP 不会抢占 MASTER，除非它的优先级更高。 当 MASTER 不可用时(BACKUP收不到通告信息)多台 BACKUP 中优先级最高的这台会被抢占为 MASTER。这种抢占是非常快速的( 4.OSI协议 物理层：EIA/TIA-232, EIA/TIA-499, V.35, V.24, RJ45, Ethernet, 802.3, 802.5, FDDI, NRZI, NRZ, B8ZS 数据链路层：Frame Relay, HDLC, PPP, IEEE 802.3/802.2, FDDI, ATM, IEEE 802.5/802.2 网络层：IP，IPX，AppleTalk DDP 传输层：TCP，UDP，SPX 会话层：RPC,SQL,NFS,NetBIOS,names,AppleTalk,ASP,DECnet,SCP 表示层:TIFF,GIF,JPEG,PICT,ASCII,EBCDIC,encryption,MPEG,MIDI,HTML 应用层：FTP,WWW,Telnet,NFS,SMTP,Gateway,SNMP 5.文件系统只读及恢复 6.raid 主要性能排序 冗余从好到坏：raid 1 raid 10 raid 5 raid 0 性能从好到坏：raid 0 raid 10 raid 5 raid 1 成本从低到高：raid 0 raid 5 raid 1 raid 10 Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"arm/harbor.html":{"url":"arm/harbor.html","title":"harbor","keywords":"","body":"# $ docker-compose ps NAME COMMAND SERVICE STATUS PORTS harbor-core \"/harbor/entrypoint.…\" core running (healthy) harbor-db \"/docker-entrypoint.…\" postgresql running (healthy) harbor-jobservice \"/harbor/entrypoint.…\" jobservice running (healthy) harbor-log \"/bin/sh -c /usr/loc…\" log running (healthy) 127.0.0.1:1514->10514/tcp harbor-portal \"nginx -g 'daemon of…\" portal running (unhealthy) nginx \"nginx -g 'daemon of…\" proxy restarting redis \"redis-server /etc/r…\" redis running (unhealthy) registry \"/home/harbor/entryp…\" registry restarting registryctl \"/home/harbor/start.…\" registryctl restarting 异常一: registry 服务 configuration error: open /etc/registry/config.yml: permission denied $ docker-compose logs -f registry registry | Appending internal tls trust CA to ca-bundle ... registry | find: '/etc/harbor/ssl': No such file or directory registry | Internal tls trust CA appending is Done. registry | ls: /harbor_cust_cert: Permission denied registry | configuration error: open /etc/registry/config.yml: permission denied Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"arm/kubesphere.html":{"url":"arm/kubesphere.html","title":"kubesphere","keywords":"","body":" v3.3.1 kubesphere-system ks-console 启动异常 异常信息： $ kubectl logs -f ks-console-695ccff5f9-v9jsm -n kubesphere-system # # Fatal process OOM in insufficient memory to create an Isolate # 解决方式 -> 本地编译运行 流程如下： 下载源码 $ git clone --branch v3.3.1 https://github.com/kubesphere/console.git 修改 build/Dockerfile $ cd console $ vim build/Dockerfile 调整内容：构建基础镜像、设置 yarn 源、调整目录权限等 编译 $ docker buildx build --platform linux/arm64 -t kubesphere/ks-console:v3.3.1 -f build/Dockerfile . 重启服务 $ kubectl rollout restart deploy ks-console -n kubesphere-system kubesphere-logging-system logsidecar-injector-deploy 启动异常 更换镜像版本 $ kubectl set image deployment/logsidecar-injector-deploy \\ logsidecar-injector=kubesphere/log-sidecar-injector:v1.2.0 -n kubesphere-logging-system elasticsearch-logging-discovery、 启动异常 更换镜像版本 $ kubectl set image sts/elasticsearch-logging-discovery \\ chown=kubesphere/elasticsearch-oss:6.7.0-1-arm64 -n kubesphere-logging-system $ kubectl set image sts/elasticsearch-logging-discovery \\ elasticsearch=kubesphere/elasticsearch-oss:6.7.0-1-arm64 -n kubesphere-logging-system $ kubectl set image sts/elasticsearch-logging-data \\ chown=kubesphere/elasticsearch-oss:6.7.0-1-arm64 -n kubesphere-logging-system $ kubectl set image sts/elasticsearch-logging-data \\ elasticsearch=kubesphere/elasticsearch-oss:6.7.0-1-arm64 -n kubesphere-logging-system 强制重启 $ kubectl delete pod elasticsearch-logging-discovery-0 -n kubesphere-logging-system --force $ kubectl delete pod elasticsearch-logging-data-0 -n kubesphere-logging-system --force istio-system $ kubectl set image deploy/kiali-operator \\ operator=kubesphere/kiali-operator:v1.50.1 -n istio-system $ kubectl set image deploy/istiod-1-11-2 \\ discovery=istio/pilot:1.15.6 -n istio-system Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "},"vocabulary.html":{"url":"vocabulary.html","title":"vocabulary","keywords":"","body":"DevOps 持续交付 Continuous Delivery [kənˈtɪnjuəs] [dɪˈlɪvəri] Copyright © weiliang 2021 all right reserved，powered by Gitbook本书发布时间： 2024-04-22 16:03:42 "}}